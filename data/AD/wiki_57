<doc id="19283265" url="http://en.wikipedia.org/wiki?curid=19283265" title="Nelson Rockefeller">
Nelson Rockefeller

Nelson Aldrich Rockefeller (July 8, 1908 — January 26, 1979) was an American businessman, philanthropist, public servant, and politician. He served as the 41st Vice President of the United States (1974–1977) under President Gerald Ford, and as the 49th Governor of New York (1959–1973). He also served in the administrations of Presidents Franklin Roosevelt, Harry Truman and Dwight Eisenhower in a variety of positions. A member of the wealthy Rockefeller family, he was also a noted art collector.
Rockefeller, a Republican, was politically liberal, progressive, or moderate. In his time, liberals in the Republican Party were called "Rockefeller Republicans." As Governor of New York from 1959 to 1973 his achievements included the expansion of the State University of New York, efforts to protect the environment, the building of the Governor Nelson A. Rockefeller Empire State Plaza in Albany, increased facilities and personnel for medical care, and creation of the New York State Council on the Arts.
After unsuccessfully seeking the Republican presidential nomination in 1960, 1964, and 1968, he served as United States Vice President from 1974 to 1977 under President Gerald R. Ford. Ford ascended to the presidency following the August 1974 resignation of Richard Nixon, over the Watergate Scandal, and Ford selected Rockefeller as his own replacement, under the provisions of the 25th Amendment. But Rockefeller did not join the 1976 Republican national ticket with President Ford, marking his retirement from politics.
As a businessman he was President and later Chairman of Rockefeller Center, Inc., and he formed the International Basic Economy Corporation in 1947. Rockefeller assembled a significant art collection and promoted public access to the arts. He served as trustee, treasurer, and president, of the Museum of Modern Art, and founded the Museum of Primitive Art in 1954. In the area of philanthropy, he established the American International Association for Economic and Social Development in 1946, and with his four brothers he founded the Rockefeller Brothers Fund in 1940 and helped guide it.
Early life and education.
Rockefeller was born in Bar Harbor, Maine. He was the second son of financier and philanthropist John Davison Rockefeller, Jr. and philanthropist and socialite Abigail Greene "Abby" Aldrich. He had a sister, Abby (1903–1976); and four brothers: John III (1906–1978), Laurance (1910–2004), Winthrop (1912–1973), and David (born 1915). John Jr. was the only son of Standard Oil co-founder John Davison Rockefeller, Sr. and schoolteacher Laura Celestia "Cettie" Spelman. Abby was a daughter of Senator Nelson Wilmarth Aldrich and Abigail Pearce Truman "Abby" Chapman. He received his elementary and middle school education at the Lincoln School, an experimental school administered by Teachers College of Columbia University. He also attended the prestigious Phillips Exeter Academy in New Hampshire. In 1930, he graduated "cum laude" with an A.B. in economics from Dartmouth College, where he was a member of Casque and Gauntlet (a senior society), Phi Beta Kappa, and the Zeta chapter of the Psi Upsilon.
Early business career.
Following his graduation, he worked in a number of family-related businesses, including Chase National bank
(later Chase Manhattan), 1931; Rockefeller Center, Inc., joining the board of directors in 1931, serving as president, 1938–1945 and 1948–1951, and as chairman, 1945–1953 and 1956–1958; and Creole Petroleum, the Venezuelan subsidiary of Standard Oil of New Jersey, 1935–1940. From 1932 to 1979 he served as a trustee of the Museum of Modern Art, where he also served as treasurer, 1935–1939, and president, 1939–1941 and 1946–1953. He and his four brothers established the Rockefeller Brothers Fund, a philanthropy, in 1940, where he served as trustee, 1940–1975 and 1977–1979, and as president in 1956.
Early public career.
Rockefeller served as a member of the Westchester County (NY) Board of Health, 1933–1953. His service with Creole Petroleum led to his deep, lifelong interest in Latin America. He became fluent in the Spanish language. In 1940, after he expressed his concern to President Franklin D. Roosevelt over Nazi influence in Latin America, the President appointed him to the new position of Coordinator of Inter-American Affairs (CIAA) in the Office of Inter-American Affairs (OIAA). Rockefeller was charged with overseeing a program of U.S. cooperation with the nations of Latin America to help raise the standard of living, to achieve better relations among the nations of the western hemisphere, and to counter rising Nazi influence in the region. His efforts included spreading anti-Axis propaganda to head off Nazi fifth column activity, which was subsequently laughed at and booed by the Latin American population, resulting in pro-Axis riots. The movie "Down Argentine Way" had to be refilmed because it was actually considered offensive, while "The Great Dictator" was banned in several countries.
In 1944 President Roosevelt appointed Rockefeller Assistant Secretary of State for American Republic Affairs. As Assistant Secretary of State, he initiated the Inter-American Conference on Problems of War and Peace in 1945. The conference produced the Act of Chapultepec, which provided the framework for economic, social and defense cooperation among the nations of the Americas, and set the principle that an attack on one of these nations would be regarded as an attack on all and jointly resisted. Rockefeller signed the Act on behalf of the United States.
Rockefeller was a member of the U.S. delegation at the United Nations Conference on International Organization at San Francisco in 1945; this gathering marked the UN's founding. At the Conference there was considerable opposition to the idea of permitting, within the UN charter, the formation of regional pacts such as the Act of Chapultepec. Rockefeller, who believed that the inclusion was essential, especially to U.S. policy in Latin America, successfully urged the need for regional pacts within the framework of the UN. Rockefeller was also instrumental in persuading the UN to establish its headquarters in New York City.
Returns to private life.
After resigning as Assistant Secretary of State, Rockefeller returned to private life later in 1945. He served as Chairman of Rockefeller Center, Inc., (1945–1953 and 1956–1958) and began a program of physical expansion. He established the American International Association for Economic and Social Development (AIA), in 1946, and the International Basic Economy Corporation (IBEC), in 1947 to jointly continue the work he had begun as Coordinator of Inter-American Affairs. He intermittently served as president of both through 1958. AIA was a philanthropy for the dissemination of technical and managerial expertise and equipment to underdeveloped countries to support grass-roots efforts in overcoming illiteracy, disease and poverty. IBEC was a for-profit business that established companies that would stimulate underdeveloped economies of certain countries. It was hoped the success of these companies would encourage investors in those countries to set up competing or supporting businesses and further stimulate the local economy. Using AIA and IBEC Rockefeller established model farms in Venezuela, Ecuador, and Brazil. He maintained a home at Monte Sacro, the farm in Venezuela.
Returns to public service.
Rockefeller returned to public service in 1950 when President Harry S. Truman appointed him Chairman of the International Development Advisory Board. The Board was charged with developing a plan for implementing the President's Point IV program of providing foreign technical assistance. In 1952 President-Elect Dwight D. Eisenhower asked Rockefeller to Chair the President's Advisory Committee on Government Organization to recommend ways of improving efficiency and effectiveness of the executive branch of the federal government. Rockefeller recommended thirteen reorganization plans, all of which were implemented. The plans implemented organizational changes in the Department of Defense, the Office of Defense Mobilization and the Department of Agriculture. His recommendations also led to the creation of the Department of Health, Education and Welfare. Rockefeller was appointed Under-Secretary of this new department in 1953. Rockefeller was active in HEW's legislative program and implemented measures that added ten million people under the Social Security program.
In 1954 he was appointed Special Assistant to the President for Foreign Affairs (sometimes referred to as Special Assistant to the President for Psychological Warfare). He was tasked with providing the President with advice and assistance in developing programs by which the various departments of the government could counter Soviet foreign policy challenges. As part of this responsibility he was named as the President's representative on the Operations Coordinating Board, a committee of the National Security Council. The other members were the Undersecretary of State, the Deputy Secretary of Defense, the director of the Foreign Operations Administration, and the Central Intelligence Agency director. The OCB's purpose was to oversee coordinated execution of security policy and plans, including clandestine operations.
Rockefeller broadly interpreted his directive and became an advocate for foreign economic aid as indispensable to national security. Most of Rockefeller's initiatives were blocked by Secretary of State John Foster Dulles and his Under Secretary, Herbert Hoover, Jr., both traditionalists who resented what they perceived as outside interference from Rockefeller, and by Treasury Secretary George M. Humphrey for financial reasons. However, in June 1955 Rockefeller convened a week-long meeting of experts from various disciplines to assess the U.S. position in the psychological aspects of the Cold War and develop proposals that could give the U.S. the initiative at the upcoming Summit Conference in Geneva. The meeting was held at the Marine Corps school at Quantico, Virginia, and became known as the Quantico Study. The Quantico panel developed a proposal called "open skies" wherein the U.S. and the Soviet Union would exchange blueprints of military installations and agree to mutual aerial reconnaissance. Thus military buildups would be revealed and the danger of surprise attacks minimized. It was a counter proposal to the Soviet proposal of universal disarmament. The feeling was that the Soviets could not refuse the proposal if they were serious about disarmament.
In March 1955 Rockefeller proposed the creation of the Planning Coordination Group, a small high level group that would plan and develop national security operations, both overt and covert. The group consisted of the Undersecretary of State, the Deputy Secretary of Defense, the director the CIA, and Special Assistant Rockefeller as chairman. The group's purpose was to oversee CIA operation and other anti-communist actions. However, State Department officials and CIA Director Allen Dulles refused to cooperate with the group and its initiatives were stymied or ignored. In September Rockefeller recommended the abolishment of the PCG, and in December he resigned as Special Assistant to the President.
In 1956, he created the Special Studies Project, a major seven-panel planning group directed by Henry Kissinger and funded by the Rockefeller Brothers Fund, of which he was then president. It was an ambitious study created to define the central problems and opportunities facing the U.S. in the future, and to clarify national purposes and objectives. The reports were published individually as they were released and were republished together in 1961 as "Prospect for America: The Rockefeller Panel Reports".
The Special Studies Project came into national prominence with the early release of its military subpanel's report, whose principal recommendation was a massive military buildup to counter a then-perceived military superiority threat posed by the USSR. The report was released two months after the October 1957 launch of Sputnik, and its recommendations were fully endorsed by Eisenhower in his January 1958 State of the Union address.
This initial contact with Kissinger was to develop into a lifelong relationship; Kissinger was later to be described as his closest intellectual associate. From this period Rockefeller employed Kissinger as a personally funded part-time consultant, principally on foreign policy issues, until the appointment to his staff became full-time in late 1968. In 1969, when Kissinger entered Richard Nixon's administration, Rockefeller paid him $50,000 as a severance payment.
Governor of New York, 1959–1973.
Rockefeller resigned from the Federal government in 1956 to focus on New York State and on national politics. From September 1956 to April 1958 he chaired the Temporary State Commission on the Constitutional Convention. That was followed by his chairmanship of the Special Legislative Committee on the Revision and Simplification of the Constitution. In the state election of 1958, he was elected governor of New York by over 600,000 votes, defeating the incumbent, multi-millionaire W. Averell Harriman, even though 1958 was a banner year for Democrats elsewhere in the nation. Rockefeller was ultimately elected to four, four-year terms as governor. Re-elected in elections of 1962, of 1966 and of 1970, Rockefeller vastly increased the state's role in education, environmental protection, transportation, housing, welfare, medical aid, civil rights, and the arts. He resigned three years into his fourth term to work at the Commission on Critical Choices for Americans.
Education.
Rockefeller was the driving force in turning the State University of New York into the largest system of public higher education in the United States. Under his governorship it grew from 29 campuses and 38,000 full-time students to 72 campuses and 232,000 full-time students. Other accomplishments included more than quadrupling state aid to primary and secondary schools; providing the first state financial support for educational television; and requiring special education for children with disabilities in public schools.
Conservation.
Consistent with his personal interest in design and planning, Rockefeller began expansion of the New York State Parks system and improvement of park facilities. He persuaded voters to approve three major bond acts to raise more than $300 million for acquisition of park and forest preserve land and he built or started 55 new state parks. Rockefeller initiated studies of environmental issues, such as loss of agricultural land through development—an issue now characterized as "sprawl." In September 1968, Rockefeller appointed the Temporary Study Commission on the Future of the Adirondacks. This led to his introduction to the Legislature in 1971 of a bill to create the controversial Adirondack Park Agency, which was designed to protect the Adirondack State Park from encroaching development. Also, he launched the Pure Waters Program, the first state bond issue to end water pollution; created the Department of Environmental Conservation; banned DDT and other pesticides; and established the Office of Parks and Recreation.
Transportation.
In 1967 Rockefeller won approval of the largest state bond issue at the time ($2.5 billion) for the coordinated development of mass transportation, highways and airports. He initiated the creation or expansion of over of highway including the Long Island Expressway, the Southern Tier Expressway, the Adirondack Northway, and Interstate 81 which vastly improved road transportation in the state of New York. Rockefeller introduced the state's first support for mass transportation. He reformed the governance of New York City's transportation system, creating the New York Metropolitan Transportation Authority (MTA) in 1965. The MTA merged the New York City subway system with the publicly owned Triborough Bridge and Tunnel Authority, the Long Island Rail Road, Staten Island Rapid Transit, and later the Metro North Railroad, which were purchased by the state from private owners in a massive public bailout of bankrupt railroads. He also created the State Department of Transportation.
In taking over control of the Triborough Bridge and Tunnel Authority, Rockefeller shifted power away from Robert Moses, and in doing so became the first politician to win such a battle with the master builder Moses in decades. Under the New York MTA, toll revenue collected from the bridges and tunnels, which had previously been used to build more bridges, tunnels, and highways, now went to support mass transportation operations, thus shifting costs from general state funds to the motorist. In one controversial move, Rockefeller abandoned one of Moses's most desired projects, a Long Island Sound bridge from Rye to Oyster Bay in 1973 due to environmental opposition.
Housing.
To create more low-income housing, Rockefeller created the New York State Urban Development Corporation (UDC), with unprecedented powers to override local zoning, condemn property, and create financing schemes to carry out desired development. The financing involved the creation of a new sort of bond—what came to be called "moral obligation" bonds. They were not backed by the full faith and credit of the State, but the quasi-public arrangements were meant to, and did, convey the impression that the State would not let them fail. Rockefeller is criticized in some quarters for having contributed to the "Too Big To Fail" phenomenon in U.S. finance in general. (UDC is now called the Empire State Development Corporation.) By 1973, the Rockefeller administration had completed or started over 88,000 units of housing for limited income families and the aging.
Welfare and Medicaid.
In the area of public assistance the Rockefeller administration carried out the largest state medical care program for the needy in the United States under Medicaid; achieved the first major decline in New York State's welfare rolls since World War II; required employable welfare recipients to take available jobs or job training; began the state breakfast program for children in low income areas; and established the first state loan fund for nonprofit groups to start day-care centers.
Civil Rights.
Rockefeller achieved virtual total prohibition of discrimination in housing and places of public accommodation. He outlawed job discrimination based on gender or age; increased by nearly 50% the number of African Americans and Hispanics holding state jobs; appointed women to head the largest number of state agencies in state history; prohibited discrimination against women in education, employment, housing and credit applications; admitted the first women to the State Police; initiated affirmative action programs for women in state government; and backed New York's ratification of the Equal Rights Amendment to the U.S. Constitution. He outlawed "block-busting" as a means of artificially depressing housing values and banned discrimination in the sale of all forms of insurance.
The Arts.
Rockefeller created the first State Council on the Arts in the country, which became a model for the National Endowment for the Arts. He also oversaw the construction of the Saratoga Performing Arts Center in Saratoga Spa State Park.
Crime.
During his fifteen years as governor Rockefeller doubled the size of the state police, established the New York State Police Academy, adopted the "stop and frisk" and "no-knock" laws to strengthen police powers, and authorized 228 additional state judgeships to reduce court congestion.
New York was the last state to have a mandatory death penalty for premeditated first degree murder. In 1963 Rockefeller signed legislation abandoning that and establishing a two stage trial for murder cases with punishment determined in the second stage. Rockefeller was a supporter of capital punishment and oversaw 14 executions by electrocution as Governor. The last execution, of Eddie Mays in 1963, remains to date the last execution in New York and was the last execution before "Furman v. Georgia" in the Northeast. However, despite his personal support for capital punishment, Rockefeller signed a bill in 1965 to abolish the death penalty except in cases involving the murder of police officers.
Rockefeller was also a supporter of the "law and order" platform.
Tough laws on drug users.
What became known as the "Rockefeller drug laws" were a product of Rockefeller's attempt to deal with the rapid increase in narcotics addiction and related crime. In 1962, he proposed a program of voluntary rehabilitation for addicted convicts rather than prison time. This was approved by the legislature, but by 1966 it was evident that this program was not working, as most addicts chose short prison terms rather than three years of treatment. Rockefeller then turned to a program of compulsory treatment, rehabilitation, and aftercare for three years. While this program saw success in rehabilitating addicts, it did little to reduce the narcotics trade and associated crime. Rockefeller was also frustrated believing that the federal government was not doing anything significant to address the problem. Feeling that existing laws and the way they were being implemented did not solve the problem of the "drug pusher", and pressured by voters angry about the drug problem, Rockefeller proposed a hard-line approach. As approved by the legislature in 1973, the new drug laws included mandatory life sentences without the possibility of plea-bargaining or parole for all drug users, dealers, and those convicted of drug-related violent crimes; a $1,000 reward for information leading to the conviction of drug pushers; and deleting less harsh penalties for youthful offenders. Public support for the measures was mixed, as were the results. They did not lead more addicts to seek rehabilitation as hoped, and ultimately did not solve the problem of drug trafficking. These were among the toughest drug laws in the United States when they were enacted and are still on the books, albeit in moderated form. To carry out the rehabilitation program Rockefeller created the State Narcotics Addiction Control Commission (later the State Drug Abuse Control Commission.) New York also provided the financial support for research in methadone maintenance and the administration of the largest methadone maintenance program in the US.
Attica prison riot.
On September 9, 1971, prisoners at the state penitentiary at Attica, NY, took control of a cell block and seized thirty-nine guards as hostages. After four days of negotiations, Department of Correctional Services Commissioner Russell Oswald agreed to most of the inmates' demands for various reforms but refused to grant complete amnesty to the rioters, with passage out of the country and removal of the prison's superintendent. When negotiations stalled and the hostages appeared to be in imminent danger, Rockefeller ordered New York State Police and national guard troops to restore order and take back the prison on September 13. Thirty nine people died in the assault, including ten of the hostages. An additional eighty people were wounded in what was called "a turkey shoot" by state prosecutor Malcolm Bell.
A later investigation showed all but three of the deaths were caused by the gunfire of the national guard and police. The other three were inmates killed by other inmates at the beginning of the riot. Opponents blamed Rockefeller for these deaths in part because of his refusal to go to the prison and talk with the inmates, while his supporters, including many conservatives who had often vocally differed with him in the past, defended his actions as being necessary to the preservation of law and order. "I was trying to do the best I could to save the hostages, save the prisoners, restore order, and preserve our system without undertaking actions which could set a precedent which would go across this country like wildfire," Rockefeller later said.
In a telephone call with President Nixon, Rockefeller explained the deaths by saying "that's life."
Buildings and public works programs.
Rockefeller engaged in massive building projects that left a profound mark on the state of New York. (Some of his detractors claimed that he had an "Edifice Complex.") He was personally interested in the planning, design, and construction of the many projects initiated during his administration, consistent with his interest in architecture. In addition, Rockefeller's construction programs included the US$2 billion South Mall in Albany, later renamed the Nelson A. Rockefeller Empire State Plaza by Gov. Hugh Carey in 1978. It is a campus of skyscrapers housing state offices and public plazas punctuated by an egg-shaped arts center. While in office he supported the construction of the World Trade Center.
Other programs.
Rockefeller worked with the legislature and unions to create generous pension programs for many public workers, such as teachers, professors, firefighters, police officers, and prison guards. He proposed the first statewide minimum wage law in the U.S. which was increased five times during his administration. Additional accomplishments of Rockefeller's fifteen years as governor of New York include initiating the state lottery and off-track betting; adopting modern treatment techniques in state mental hospitals to reduce the number of mentally ill patients by over 50%; creating the State Office of the Aging and constructing nearly 12,000 units of housing for the aging; the first mandatory seatbelt law in the US; and creating the State Consumer Protection Board.
Abortion.
Rockefeller supported reform of New York's abortion laws beginning around 1968. The proposals supported by his administration would not have repealed the long-standing prohibition, but would have expanded the exceptions allowed for the protection of the mother's health, or in circumstances of fetal abnormality.
The reform bills did not pass. However, when an outright repeal of the prohibition managed to pass in 1970, Rockefeller signed it. In 1972, he vetoed another bill that would have restored the abortion ban. He said in his 1972 veto message, "I do not believe it right for one group to impose its vision of morality on an entire society."
The "Roe v. Wade" abortion decision came on January 22, 1973; it was based partly on New York's law.
Moderate Republican.
Reflecting his interdisciplinary approach to problem solving Rockefeller took a pragmatic approach to governing. In their book "Rockefeller of New York: Executive Power in the State House", Robert Connery and Gerald Benjamin state, "Rockefeller was not committed to any ideology. Rather, he considered himself a practical problem solver, much more interested in defining problems and finding solutions around which he could unite support sufficient to ensure their enactment in legislation than in following either a strictly liberal or strictly conservative course. Rockefeller's programs did not consistently follow either liberal or conservative ideology." Early fiscal policies were conservative while later ones were not so. In the later years of his administration "conservative decisions on social programs were paralleled by liberal ones on environmental issues." Rockefeller was opposed by conservatives in the GOP such as Barry Goldwater and Ronald Reagan because of his liberal political views. As governor, Rockefeller spent more than his predecessors. Rockefeller expanded the state's infrastructure, increased spending on education including a massive expansion of the State University of New York, and increased the state's involvement in environmental issues. Rockefeller had good relations with unions, especially the construction trades, which benefited from his extensive building programs.
In foreign affairs, Rockefeller supported U.S. involvement in the United Nations as well as U.S. foreign aid. He also supported the U.S.'s fight against communism and its membership in NATO. As a result of Rockefeller's policies, some conservatives sought to gain leverage by creating the Conservative Party of New York. The small party acted as a minor counterweight to the Liberal Party of New York. The most common criticism of Rockefeller's governorship of New York is that he tried to do too much too fast, vastly increasing the level of state debt which later contributed to New York's fiscal crisis in 1975. Rockefeller created some 230 public-benefit authorities like the Urban Development Corporation. They were often used to issue bonds in order to avoid the requirement of a vote of the people for the issuance of a bond; such authority-issued bonds bore higher interest than if they had been issued directly by the state. The state budget went from $2.04 billion in 1959–60 to $8.8 billion in his last year, 1973–74. "Rockefeller sought and obtained eight tax increases during his fifteen years in office." "During his administration, the tax burden rose to a higher level than in any other state, and the incidence of taxation shifted, with a greater share being borne by the individual taxpayer."
National politics.
Rockefeller sought the Republican presidential nomination in 1960, 1964, and 1968. His bid in 1960 was ended early when then-Vice President Richard Nixon surged ahead in the polls. After quitting the campaign, Rockefeller backed Nixon, and concentrated his efforts on introducing more moderate planks into Nixon's platform.
Rockefeller, favored by moderate and liberal Republicans, was considered the front-runner for the 1964 campaign against conservative Senator Barry Goldwater of Arizona, who led the right wing of the Republican Party. In 1963, a year after Rockefeller's divorce from his first wife, he married Margaretta "Happy" Murphy, a divorcee with four children. This turned many in the party off, especially women. The divorce hurt Rockefeller's standing among voters and was widely condemned by politicians, including U.S. Senator Prescott S. Bush of Connecticut (father and grandfather of future Presidents George H. W. Bush and George W. Bush), who spoke out condemning Rockefeller for his infidelity, divorce, and remarriage. Rockefeller finished third in the New Hampshire primary in March, behind write-in Henry Cabot Lodge II (from neighboring Massachusetts) and Goldwater. He then endured poor showings in several primaries, before winning an upset in the Oregon primary in May. The birth of Rockefeller's child during the California campaign put the divorce and remarriage issue back in the headlines. After a furious contest, Rockefeller narrowly lost the California primary in early June and dropped out of the race. However, at the Republican National Convention in San Francisco in July, Rockefeller was given five minutes to speak before the convention in defense of five amendments to the party platform put forth by the moderate wing of the Republican Party to counter the Goldwater plank. Right wing delegates booed and heckled Rockefeller for 16 minutes while he stood firmly at the podium insisting on his right to speak. Rockefeller refused to support Goldwater in the general election. This conflict between Rockefeller and Goldwater would have lasting effects as Goldwater would subsequently vote against Rockefeller's confirmation for the Vice Presidency in 1974 and then as a key player in blocking Rockefeller from being on the 1976 presidential ticket.
Rockefeller again sought the Republican presidential nomination in 1968. His opponents were Nixon and Governor Ronald W. Reagan of California. In the contest, Rockefeller again represented the liberals in the GOP, Reagan representing the conservative Goldwater element, and Nixon representing moderates and liberals also. Rather than formally announce his candidacy and enter the state primaries, Rockefeller spent the first half of 1968 alternating between hints that he would run, and pronouncements that he would not be a candidate. Shortly before the Republican convention, Rockefeller finally let it be known that he was available to be the nominee, and he sought to round up uncommitted delegates and woo reluctant Nixon delegates to his banner, armed with public opinion polls that showed him doing better among voters than either Nixon or Reagan against Democrat Hubert Humphrey. Despite Rockefeller's efforts, Nixon won the nomination the first ballot.
After Gerald Ford's elevation to the Presidency, Rockefeller was named Vice President, and he was initially mentioned and reportedly considered running for President for a fourth time in 1976, if Ford declined to seek his own term.
Presidential Mission to Latin America.
In April and May 1969, at the request of President Nixon, Rockefeller and a team of 23 advisors visited 20 American republics during four trips to solicit opinions of U.S. inter-American policies and to determine the needs and conditions of each country. Among the recommendations in Rockefeller's report to the President were preferential trade agreements with Latin American countries, refinancing the region's foreign debt, and removing bureaucratic impediments that prevented the efficient use of U.S. aid. The Nixon administration did little to implement the report's recommendations.
National Commission on Water Quality.
In May 1973 President Nixon appointed Rockefeller chairman of the National Commission on Water Quality, charged with determining the technological, economic, social and environmental implications of meeting water quality standards mandated by the Federal Water Pollution Control Act Amendments of 1972. The Commission issued its report in March 1976 and he testified before Congress on its findings. He served until July 1976.
Commission on Critical Choices for Americans.
In November 1973, Rockefeller worked with former Delaware Governor Russell W. Peterson to establish the Commission on Critical Choices for Americans, and served as chairman until December 1974. The Commission was a private study project on national and international policy similar to the Special Studies Project he led 15 years earlier. It was made up of a nationally representative, bipartisan group of 42 prominent Americans drawn from far-ranging fields of interest who served on a voluntary basis. Members included the majority and minority leaders of both houses of Congress. The Commission gathered information and insights to better understand the problems facing America, and to present to the American public the "critical choices" to be made in facing those problems. He resigned as Governor of New York in December 1973, devoting himself to his new commission and the possibility of another presidential run.
Vice Presidency 1974–1977.
Following President Nixon's resignation on August 9, 1974, President Gerald Ford nominated Rockefeller on August 20 to serve as Vice President of the United States. Rockefeller's top competitor had been George H.W. Bush.
This was not the first time that Rockefeller was under consideration to fill the vice presidential vacancy. He was on President Nixon's short list to replace Spiro Agnew in 1973 but the vice presidency ultimately went to Ford. If Rockefeller had been confirmed as vice president as Nixon's nominee, Rockefeller would have become president upon Nixon's resignation. Rockefeller had earlier declined the opportunity of serving as vice president when he spurned Nixon's offer to join him as running mate in the 1960 presidential election.
While acknowledging that many conservatives opposed Rockefeller, Ford believed he would bring executive expertise to the administration and broaden the ticket's appeal if they ran in 1976. Ford also felt he could demonstrate his own self-confidence by selecting a strong personality like Rockefeller for the number two spot. Although he had said he was "just not built for standby equipment", Rockefeller accepted the President's request to serve as vice president: It was entirely a question of there being a Constitutional crisis and a crisis of confidence on the part of the American people... I felt there was a duty incumbent on any American who could do anything that would contribute to a restoration of confidence in the democratic process and in the integrity of government. Rockefeller was also persuaded by Ford's promise to make him "a full partner" in his presidency, especially in domestic policy.
Rockefeller underwent extended hearings before Congress, suffering embarrassment when it was revealed he made massive gifts to senior aides, such as Henry Kissinger, and used his personal fortune to finance a scurrilous biography of political opponent Arthur Goldberg (See Peter Carroll "It Seemed Like Nothing Happened", p. 162). He had not paid all his taxes, owing nearly one million dollars in federal income taxes, but no illegalities were uncovered, and he was confirmed. Although conservative Republicans were not pleased that Rockefeller was picked, most of them voted for his confirmation. However, some, including Barry Goldwater, Jesse Helms, Trent Lott, and others voted against him. Many conservative groups campaigned against Rockefeller's nomination, including the National Right to Life Committee, the American Conservative Union, and others. The New York Conservative Party also opposed his confirmation. On the left, Americans for Democratic Action opposed Rockefeller's confirmation because it said his wealth posed too much of a conflict of interest.
Beginning his service upon taking the oath of office on December 19, 1974, Rockefeller was the second person appointed vice president under the 25th Amendment—the first being Ford himself. Rockefeller often seemed concerned that Ford gave him little or no power, and few tasks, while he was vice president. Ford initially said he wanted Rockefeller to chair the Domestic Policy Council. But Ford's new White House staff had no intention of sharing power with the vice president and his staff.
Rockefeller's attempt to take charge of domestic policy was thwarted by White House Chief of Staff Donald Rumsfeld, who objected to policy makers reporting to the president through the vice president. When Rockefeller had one of his former aides, James Cannon, appointed executive director the Domestic Council, Rumsfeld cut its budget. Rockefeller was excluded from the decision making process on many important issues. When he learned that Ford had proposed cuts in federal taxes and spending he responded: "This is the most important move the president has made, and I wasn't even consulted." Nevertheless, Ford appointed him to the Commission on the Organization of Government for the Conduct of Foreign Policy, and appointed him Chairman of the Commission on CIA Activities within the United States, the National Commission on Productivity, the Federal Compensation Committee, and the Committee on the Right to Privacy. Ford also put Rockefeller in charge of his "Whip Inflation Now" initiative.
While Rockefeller was vice president, the official vice presidential residence was established at Number One Observatory Circle on the grounds of the United States Naval Observatory. This residence had previously been the home of the Chief of Naval Operations; prior vice presidents had been responsible for maintaining their own homes at their own expense, but the necessity of massive full-time Secret Service security had made this custom impractical to continue. Rockefeller already had a well-secured Washington residence and never lived in the home as a principal residence, although he did host several official functions there. His wealth enabled him to donate millions of dollars of furnishings to the house.
Rockefeller was slow to embrace the use of the government aircraft that were provided for vice presidential transportation. Rockefeller continued to use his own Gulfstream (which had the callsign Executive Two for being a private aircraft) for the first part of his time in office. Initially Rockefeller felt he was doing the taxpayer a favor saving money by not using government funded transportation. Finally the Secret Service was able to convince him they were spending more money flying agents around to meet the needs of his protective detail and he began to fly on the DC-9 that was serving as Air Force Two at the time.
In November 1975, Rockefeller told Ford that he would not run for election as vice president in 1976, saying that he "didn't come down (to Washington) to get caught up in party squabbles which only make it more difficult for the President in a very difficult time..." At the 1976 Republican National Convention, Ford, a moderate, under pressure from the conservative wing of the party and in response to Ronald Reagan's challenge for the presidential nomination, had decided to choose the more conservative Senator Robert Dole from Kansas as his running mate. Reagan had indicated that he could not support Ford if Rockefeller were on the ticket, and Goldwater also said he did not want Rockefeller on the ticket. So Rockefeller was not nominated for the ticket because of his positions on key Republican issues. As of 2012, Ford is the last president to not have his vice president as his running mate. Ford later said not choosing Rockefeller was one of the biggest mistakes he ever made. With Dole as his running mate, Ford narrowly lost to Jimmy Carter in the presidential race. What difference Rockefeller's presence on the ticket would have made remains a matter of speculation. Rockefeller campaigned actively for the Republican ticket. In what would become an iconic photo of the 1976 campaign, Rockefeller famously responded to hecklers at a rally in Binghamton, New York with a raised middle finger. "At the time, Rockefeller's finger flashing was scandalous. Writing about the moment 20 years later, Michael Oricchio of the "San Jose Mercury News" said the action became known euphemistically as 'the Rockefeller gesture'."
On January 10, 1977, Ford presented Rockefeller with the Presidential Medal of Freedom.
Art patronage.
Rockefeller served as a trustee of the Museum of Modern Art from 1932 to 1979. He also served as treasurer, 1935–1939, and president, 1939–1941 and 1946–1953. In 1933 Rockefeller was a member of the committee selecting art for the new Rockefeller Center. For the wall opposite the main entrance of 30 Rockefeller Plaza Nelson Rockefeller wanted Henri Matisse or Pablo Picasso to paint a mural because he favored their modern style, but neither was available. Diego Rivera was one of Nelson Rockefeller's mother's favorite artists and therefore was commissioned to create the huge mural. He was given a theme: New Frontiers. Rockefeller wanted the painting to make people pause and think. Rivera submitted a sketch for a mural entitled "Man at the Crossroads Looking with Hope and High Vision to the Choosing of a New and Better Future." The sketch featured an anonymous man at the center. However, when it was painted the work caused great controversy due to the inclusion of a painting of Lenin (depicting communism) just off-center. The Directors of Rockefeller Center objected and Rockefeller asked Rivera to change the face of Lenin to that of an unknown laborer's face as was originally intended, but the painter refused.
The work was paid for on May 22, 1933, and immediately draped. Rockefeller suggested that the fresco could be donated to the Museum of Modern Art, but the trustees of the museum were not interested. People protested but it remained covered until the early weeks of 1934, when it was smashed by workers and hauled away in wheelbarrows. Rivera responded by saying that it was "cultural vandalism". At Rockefeller Center in its place is a mural by Jose Maria Sert which includes an image of Abraham Lincoln. The Rockefeller-Rivera dispute is covered in the films "Cradle Will Rock" and "Frida".
Rockefeller was a noted collector of both modern and non-Western art. During his governorship, New York State acquired major works of art for the new Empire State Plaza in Albany. He continued his mother's work at the Museum of Modern Art as president, and turned the basement of his Kykuit mansion into a gallery while placing works of sculpture around the grounds (an activity he enjoyed personally supervising, frequently moving the pieces from place to place by helicopter). While he was overseeing construction of the State University of New York system, Rockefeller built, in collaboration with his lifelong friend Roy Neuberger, the Neuberger Museum on the campus of SUNY Purchase College, designed by Philip Johnson.
He commissioned Master Santiago Martínez Delgado to make a canvas mural for the Bank of New York (City Bank) in Bogotá, Colombia; this ended up being the last work of the artist, as he died while finishing it.
Rockefeller's early visits to Mexico kindled a collecting interest in pre-Columbian and contemporary Mexican art, to which he added works of traditional African and Pacific Island art. In 1954 he established the Museum of Primitive Art devoted to the indigenous art of the Americas, Africa, Oceania and early Asia and Europe. His personal collection formed the core of the collection. The museum opened to the public in 1957 in a townhouse on West 54th Street in New York City. In 1969 he gave the museum's collection to the Metropolitan Museum of Art where it became the Michael C. Rockefeller Collection.
In 1978, Alfred A. Knopf published a book on primitive art from Rockefeller's collection. Rockefeller, impressed with the work of photographer Lee Boltin and editor/publisher Paul Anbinder on the book, co-founded Nelson Rockefeller Publications, Inc. with them, with the goal of publishing fine art books of high quality. After Rockefeller's death less than a year later, the company continued as Hudson Hills Press, Inc.
In 1977 he founded Nelson Rockefeller Collection, Inc., (NRC) an art reproduction company that produced and sold licensed reproductions of selected works from Rockefeller's collection. In the introduction to the NRC catalog he stated he was motivated by his desire to share with others "the joy of living with these beautiful objects."
Marriages.
On June 23, 1930, Rockefeller married Mary Todhunter Clark. They had five children:
Michael disappeared in New Guinea in November 1961, presumed drowned while trying to swim to shore after his dugout canoe capsized. Nelson and Mary were divorced in 1962.
On May 4, 1963, he married Margaretta Large "Happy" Fitler. He and his second wife had two sons together:
They moved to a penthouse that encompassed the top three floors at 810 Fifth Avenue. The apartment was expanded by purchasing a floor of 812 Fifth Avenue. The two spaces connected via a flight of six steps. Nelson and Happy Rockefeller used the entrance at 812 Fifth, while his first wife entered through 810 Fifth. They remained married until his death.
Death.
Rockefeller died on January 26, 1979, at age 70 from a heart attack. An initial report had incorrectly stated that he was at his office at Rockefeller Center working on a book about his art collection, and a security guard found him slumped over his desk. There was some speculation in the press regarding the possibility of an intimate relationship between Rockefeller and Marshack. For example, long-time Rockefeller aide Joe Persico said in the PBS documentary about the Rockefeller family "It became known that he had been alone with a young woman who worked for him, in undeniably intimate circumstances, and in the course of that evening had died from a heart attack." Rockefeller's four oldest children issued a statement saying they had conducted their own review, believed that their father could not have been saved, and that all those who tried to help had acted responsibly. Neither Marshack nor the family has commented since on the circumstances surrounding Rockefeller's death.
On January 29, 1979, family and close friends gathered to inter Rockefeller's ashes in a private Rockefeller family cemetery in Sleepy Hollow, New York. His remains had been cremated at Ferncliff Cemetery in nearby Hartsdale. A memorial service was held at Riverside Church in New York on February 2, attended by 2,200 people. Attendees included President Jimmy Carter, former President Gerald Ford, more than 100 members of the U.S. Senate and House of Representatives including Senator Barry Goldwater, and official representatives from 71 foreign countries. Eulogies were delivered by two of Rockefeller's children, his brother David, and former Secretary of State Henry Kissinger.
Memorials to Nelson A. Rockefeller.
The following institutions and facilities have been named in honor of Nelson A. Rockefeller:

</doc>
<doc id="19283284" url="http://en.wikipedia.org/wiki?curid=19283284" title="1909">
1909

__NOTOC__
Year 1909 (MCMIX) was a common year starting on Friday (link will display full calendar) of the Gregorian calendar and a common year starting on Thursday of the 13-day-slower Julian calendar.

</doc>
<doc id="19283299" url="http://en.wikipedia.org/wiki?curid=19283299" title="Economy of Sweden">
Economy of Sweden

The economy of Sweden is a developed export-oriented diverse economy aided by timber, hydropower, and iron ore. These constitute the resource base of an economy oriented toward foreign trade. The main industries include motor vehicles, telecommunications, pharmaceuticals, industrial machines, precision equipment, chemical goods, home goods and appliances, forestry, iron, and steel. Traditionally a modernised agricultural economy that used to employ over half of the domestic workforce, today Sweden further develops engineering, mine, steel, and pulp industries that are competitive internationally, such as the companies LM Ericsson, ASEA/ABB, SKF, Alfa Laval, Aga, and Dyno Nobel.
Sweden is a highly competitive capitalist economy featuring a generous universal welfare state financed through relatively high income taxes that ensures that income is distributed across the entire society, a model sometimes called the Nordic model. Approximately 90% of all resources and companies are privately owned, with a minority of 5% owned by the state and another 5% operating as either consumer or producer cooperatives.
Because Sweden as a neutral country did not actively participate in World War II, it did not have to rebuild its economic base, banking system, and country as a whole, as did many other European countries. Sweden has achieved a high standard of living under a mixed system of high-tech capitalism and extensive welfare benefits. Sweden has the second highest total tax revenue behind Denmark, as a share of the country's income. , total tax revenue was 44.2% of GDP, down from 48.3% in 2006.
History.
In the 19th century Sweden evolved from a largely agricultural economy into the beginnings of an industrialized, urbanized country. Poverty was still widespread in sections of the population. However, incomes were sufficiently high to finance emigration to distant places, prompting a large portion of the country to leave, especially to the USA. Economic reforms and the creation of a modern economic system, banks and corporations were enacted during the latter half of the 19th century.
By the 1930s, Sweden had what "Life" magazine called in 1938 the "world's highest standard of living". Sweden was also the first country worldwide to recover completely from the Great Depression. Sweden declared itself neutral during both world wars, thereby avoiding much physical destruction like several other neutral countries. The post-war boom propelled Sweden to greater economic prosperity, putting the country in third place in per capita GDP rankings by 1970. Beginning in the 1970s and culminating with the deep recession of the early 1990s, Swedish standards of living developed less favorably than many other industrialized countries. Since the mid-1990s the economic performance has improved.
In 2009, Sweden had the world's tenth highest GDP per capita in nominal terms and was in 
14th place in PPP terms.
Crisis of the 1990s.
Sweden has had an economic model in the post-World War II era, characterized by close cooperation between the government, labour unions and corporations. The Swedish economy has extensive and universal social benefits funded by high taxes, close to 50% of GDP. In the 1980s, a real estate and financial bubble formed, driven by a rapid increase in lending. A restructuring of the tax system, in order to emphasize low inflation combined with an international economic slowdown in the early 1990s, caused the bubble to burst. Between 1990 and 1993 GDP went down by 5% and unemployment skyrocketed, causing the worst economic crisis in Sweden since the 1930s. According to an analysis by George Berglund published in Computer Sweden in 1992, the investment level decreased drastically for information technology and computing equipment, except in the financial and banking sector, the part of the industry that created the crisis. The investment levels for IT and computers were restored as early as 1993. In 1992 there was a run on the currency, the central bank briefly jacking up interest to 500% in an unsuccessful effort to defend the currency's fixed exchange rate. Total employment fell by almost 10% during the crisis.
A real estate boom ended in a bust. The government took over nearly a quarter of banking assets at a cost of about 4% of the nation's GDP. This was known colloquially as the "Stockholm Solution". The United States Federal Reserve remarked in 2007, that "In the early 1970s, Sweden had one of the highest income levels in Europe; today, its lead has all but disappeared... So, even well-managed financial crises don't really have a happy ending."
The welfare system that had been growing rapidly since the 1970s could not be sustained with a falling GDP, lower employment and larger welfare payments. In 1994 the government budget deficit exceeded 15% of GDP. The response of the government was to cut spending and institute a multitude of reforms to improve Sweden's competitiveness. When the international economic outlook improved combined with a rapid growth in the IT sector, which Sweden was well positioned to capitalize on, the country was able to emerge from the crisis.
The crisis of the 1990s was by some viewed as the end of the much buzzed welfare model called "Svenska modellen", literally "The Swedish Model", as it proved that governmental spending at the levels previously experienced in Sweden was not long term sustainable in a global open economy. Much of the Swedish Model's acclaimed advantages actually had to be viewed as a result of the post WWII special situation, which left Sweden untouched when competitors' economies were comparatively weak.
However, the reforms enacted during the 1990s seem to have created a model in which extensive welfare benefits can be maintained in a global economy.
Contemporary economy.
Sweden is an export-oriented mixed economy featuring a modern distribution system, excellent internal and external communications, and a skilled labor force. Timber, hydropower and iron ore constitute the resource base of an economy heavily oriented toward foreign trade. Sweden's engineering sector accounts for 50% of output and exports. Telecommunications, the automotive industry and the pharmaceutical industries are also of great importance. Agriculture accounts for 2 percent of GDP and employment.
The 20 largest Sweden-registered companies by turnover in 2013 were Volvo, Ericsson, Vattenfall, Skanska, Hennes & Mauritz, Electrolux, Volvo Personvagnar, Preem, TeliaSonera, Sandvik, ICA, Atlas Copco, Nordea, Svenska Cellulosa Aktiebolaget, Scania, Securitas, Nordstjernan, SKF, ABB Norden Holding and Sony Mobile Communications AB, . Sweden's industry is overwhelmingly in private control; unlike some other industrialized Western countries, such as Austria, Italy or Finland, state owned enterprises were always of minor importance. One important exception to this rule is LKAB, which is a state-owned mining company, mostly active in the northern part of the country.
Some 4.5 million residents are working, out of which around a third with tertiary education. GDP per hour worked is the world's 9th highest at 31 USD in 2006, compared to 22 USD in Spain and 35 USD in United States. According to OECD, deregulation, globalization, and technology sector growth have been key productivity drivers. GDP per hour worked is growing per cent a year for the economy as a whole and trade-terms-balanced productivity growth 2%. Sweden is a world leader in privatized pensions and pension funding problems are small compared to many other Western European countries. Swedish labor market has become more flexible, but it still has some widely acknowledged problems. The typical worker receives only 40% of his income after the tax wedge. The slowly declining overall taxation, 51.1% of GDP in 2007, is still nearly double of that in the United States or Ireland. Civil servants amount to a third of Swedish workforce, multiple times the proportion in many other countries. Overall, GDP growth has been fast since reforms in the early 1990s, especially in manufacturing.
World Economic Forum 2012–2013 competitiveness index ranks Sweden 4th most competitive. The Index of Economic Freedom 2012 ranks Sweden the 21st most free out of 179 countries, or 10th out of 43 European countries. Sweden ranked 9th in the IMD Competitiveness Yearbook 2008, scoring high in private sector efficiency. According to the book, "The Flight of the Creative Class", by the U.S. urban studies, Professor Richard Florida of University of Toronto, Sweden is ranked as having the best creativity in Europe for business and is predicted to become a talent magnet for the world's most purposeful workers. The book compiled an index to measure the kind of creativity it claims is most useful to business – talent, technology and tolerance. Sweden's investment into research and development stood, in 2007, at over 3.5% of GDP. This is considerably higher than that of a number of MEDCs, including the United States, and is the largest among the OECD members.
Sweden rejected the Euro in a referendum in 2003, and Sweden maintains its own currency, the Swedish krona (SEK). The Swedish Riksbank—founded in 1668 and thus making it the oldest central bank in the world—is currently focusing on price stability with its inflation target of 2%. According to "Economic Survey of Sweden 2007" by OECD, the average inflation in Sweden has been one of the lowest among European countries since the mid-1990s, largely because of deregulation and quick utilization of globalization.
The largest trade flows are with Germany, United States, Norway, United Kingdom, Denmark and Finland.
The Swedish economic picture has brightened significantly since the severe recession in the early 1990s. Growth has been strong in recent years, and even though the growth in the economy slackened between 2001 and 2003, the growth rate has picked up since with an average growth rate of 3.7% in the last three years. The long-run prospects for growth remain favorable. The inflation rate is low and stable, with projections for continued low levels over the next 2–3 years.
Since the mid-1990s the export sector has been booming, acting as the main engine for economic growth. Swedish exports also have proven to be surprisingly robust. A marked shift in the structure of the exports, where services, the IT industry, and telecommunications have taken over from traditional industries such as steel, paper and pulp, has made the Swedish export sector less vulnerable to international fluctuations. However, at the same time the Swedish industry has received less money for its exports while the import prices have gone up. During the period 1995–2003 the export prices were reduced by 4% at the same time as the import prices climbed by 11%. The net effect is that the Swedish terms-of-trade fell 13%.
Government.
The government budget has improved dramatically from a record deficit of more than 12% of GDP in 1993. In the last decade, from 1998 to present, the government has run a surplus every year, except for 2003 and 2004. The surplus for 2011 is expected to be 99 billion ($15b) kronor. The new, strict budget process with spending ceilings set by the Riksdag, and a constitutional change to an independent Central Bank, have greatly improved policy credibility.
From the perspective of longer term fiscal sustainability, the long-awaited reform of old-age pensions entered into force in 1999. This entails a far more robust system vis-à-vis adverse demographic and economic trends, which should keep the ratio of total pension disbursements to the aggregate wage bill close to 20% in the decades ahead. Taken together, both fiscal consolidation and pension reform have brought public finances back on a sustainable footing. Gross public debt, which jumped from 43% of GDP in 1990 to 78% in 1994, stabilised around the middle of the 1990s and started to come down again more significantly beginning in 1999. In 2000 it fell below the key level of 60% and had declined to a level of 35% of GDP as of 2010.
Economic and monetary union.
Current economic development reflects a quite remarkable improvement of the Swedish economy since the crisis in 1991–93, so that Sweden could easily qualify for membership in the third phase of the Economic and Monetary Union of the European Union, adopting the euro as its currency. In theory, by the rules of the EMU, Sweden is obliged to join, since the country has not obtained exception by any protocol or treaty (as opposed to Denmark and the United Kingdom). Nevertheless, the Swedish government decided in 1997 against joining the common currency from its start on 1 January 1999. This choice was implemented by exploiting a legal loophole, deliberately staying out of the European Exchange Rate Mechanism. This move is currently tolerated by the European Central Bank, which however has warned that this would not be the case for newer EU members.
In the first years of the twenty-first century, a majority for joining emerged in the governing Social Democratic party, although the question was subject of heated debate, with leading personalities in the party on both sides. On 14 September 2003, a national referendum was held on the euro. A 56% majority of Swedes rejected the common currency, while 42% voted in favour of it.
Currently no plans for a new referendum or parliamentary vote on the matter are being discussed, though it has been implied that another referendum may take place in around ten years.
Unemployment.
In contrast with most other European countries, Sweden maintained an unemployment rate around 2% or 3% of the work force throughout the 1980s. This was, however, accompanied by high and accelerating inflation. It became evident that such low unemployment rates were not sustainable, and in the severe crisis of the early 1990s the rate increased to more than 8%. In 1996 the government set out a goal of reducing unemployment to 4% by 2000. During 2000 employment rose by 90,000 people, the greatest increase in 40 years, and the goal was reached in the autumn of 2000. The same autumn the government set out its new target: that 80% of the working age population will have a regular job by 2004. Some have expressed concern that meeting the employment target may come at a cost of too high a rate of wage increases hence increasing inflation. However, as of August 2006, roughly 5% of working age Swedes were unemployed, over the government-established goal. However, some of the people who cannot find work are put away in so-called "labour market political activities", referred to as "AMS-åtgärder".
According to Jan Edling, a former trade-unionist, the actual number of unemployed is far higher, and those figures are being suppressed by both the government and the Swedish Trade Union Confederation. In Edling's report he added that a further 3% of Swedes were occupied in state-organised job schemes, not in the private sector. He also claimed a further 700,000 Swedes are either on long-term sick leave or in early retirement. Edling asks how many of these people are in fact unemployed. According to his report, the "actual unemployment" rate hovers near 20%. Some critics disagree with this concept of "actual" unemployment, also termed "broad unemployment", since they do not see e.g. students who rather want a job, people on sick leave and military conscripts as "unemployed".
According to Swedish Statistics, unemployment in June 2013 was 9.1% in the general population and 29% amongst 15- to 25-year-old.
Trade unions.
Around seventy percent of the Swedish labour force is unionised. For most unions there is a counterpart employer's organization for businesses. The unions and employer organisations are independent of both the government and political parties, although the largest confederation of unions, the National Swedish Confederation of Trade Unions or "LO" (organising blue-collar workers), maintains close links to one of the two major parties, the Social Democrats.
The unionisation rate among white-collar workers is exceptionally high in Sweden – since 2008 higher than for blue-collar workers. In 2013 blue-collar density was 66% and white-collar density 73% (full-time students working part-time excluded). Just before the considerably raised fees to union unemployment funds in January 2007, blue-collar and white-collar union density was the same (77% in 2006). The average union density was the 70% both in 2011, 2012 and 2013. There are two major confederations that organise professionals and other qualified employees: the Swedish Confederation of Professional Employees ("Tjänstemännens Centralorganisation" or TCO) and the Swedish Confederation of Professional Associations ("Sveriges Akademikers Centralorganisation" or SACO). They are both independent from Sweden's political parties and never endorse candidates for office in political elections.
There is no minimum wage that is required by legislation. Instead, minimum wage standards in different sectors are normally set by collective bargaining. About 90% of all workers are covered by collective agreements, in the private sector about 85% (2012).
Sweden has still not joined the EMU (the Economic and Monetary Union / the Euro) and will not in the foreseeable future. When the issue was at the agenda, the Swedish union movement was very split. In contrast to the very positive attitude of employers' associations, the union rank-and file opinion was so split that several unions, as well as the confederations LO, TCO and SACO, abstained from taking an official position.
Labor force.
The traditionally low-wage differential has increased in recent years as a result of increased flexibility as the role of wage setting at the company level has strengthened somewhat. Still, Swedish unskilled employees are well paid while well educated Swedish employees are low-paid compared with those in competitor countries in Western Europe and the US. The average increases in real wages in recent years have been high by historical standards, in large part due to unforeseen price stability. Even so, nominal wages in recent years have been slightly above those in competitor countries. Thus, while private-sector wages rose by an average annual rate of 3.75% from 1998 to 2000 in Sweden, the comparable increase for the EU area was 1.75%. In the year 2000 the total labour force was around 4.4 million people.
Ongoing and finished privatisations.
The Swedish government has announced that it will privatise a number of wholly and partly state owned companies. ""The income from these sales will be used to pay off the government debt and reduce the burden of debt for future generations. The Government's ambition is to sell companies to a value of SEK 200 billion during 2007–2010."" [http://www.sweden.gov.se/sb/d/8909]
-Ongoing Privatisations:
-Completed Privatisations:
See also.
"Other links"

</doc>
<doc id="19283307" url="http://en.wikipedia.org/wiki?curid=19283307" title="Param Sant Tarachand">
Param Sant Tarachand

Param Sant Tarachand, popularly known as Bade Maharaj ji, was born in a poor family in Dinod village of Haryana state. His spiritual cravings were very intense and started his search for the supreme creator at the age of six. He liked the company of the mystics and saints very much. To achieve his supreme goal he was a celibate. After initiation he used to remain in continuous meditation for many days. Ram Singh Arman was so pleased that he wrote a book, "Vashiyatnama", for his successor, Tarachand.
Tarachand died on 3 January 1997.
Books.
Tarachand wrote a book titled "Anubhava Parakash".

</doc>
<doc id="19283335" url="http://en.wikipedia.org/wiki?curid=19283335" title="Great Depression">
Great Depression

The Great Depression was a severe worldwide economic depression in the decade preceding World War II. The timing of the Great Depression varied across nations, but in most countries it started in 1930 and lasted until the late 1930s or middle 1940s. It was the longest, deepest, and most widespread depression of the 20th century.
In the 21st century, the Great Depression is commonly used as an example of how far the world's economy can decline. The depression originated in the U.S., after the fall in stock prices that began around September 4, 1929, and became worldwide news with the stock market crash of October 29, 1929 (known as Black Tuesday).
The Great Depression had devastating effects in countries rich and poor. Personal income, tax revenue, profits and prices dropped, while international trade plunged by more than 50%. Unemployment in the U.S. rose to 25%, and in some countries rose as high as 33%.
Cities all around the world were hit hard, especially those dependent on heavy industry. Construction was virtually halted in many countries. Farming and rural areas suffered as crop prices fell by approximately 60%. Facing plummeting demand with few alternate sources of jobs, areas dependent on primary sector industries such as cash cropping, mining and logging suffered the most.
Some economies started to recover by the mid-1930s. In many countries, the negative effects of the Great Depression lasted until after the end of World War II.
Start.
Economic historians usually attribute the start of the Great Depression to the sudden devastating collapse of US stock market prices on October 29, 1929, known as Black Tuesday; some dispute this conclusion, and see the stock crash as a symptom, rather than a cause, of the Great Depression.
Even after the Wall Street Crash of 1929, optimism persisted for some time; John D. Rockefeller said that "These are days when many are discouraged. In the 93 years of my life, depressions have come and gone. Prosperity has always returned and will again." The stock market turned upward in early 1930, returning to early 1929 levels by April. This was still almost 30% below the peak of September 1929.
Together, government and business spent more in the first half of 1930 than in the corresponding period of the previous year. On the other hand, consumers, many of whom had suffered severe losses in the stock market the previous year, cut back their expenditures by ten percent. Likewise, beginning in mid-1930, a severe drought ravaged the agricultural heartland of the US.
By mid-1930, interest rates had dropped to low levels, but expected deflation and the continuing reluctance of people to borrow meant that consumer spending and investment were depressed. By May 1930, automobile sales had declined to below the levels of 1928. Prices in general began to decline, although wages held steady in 1930; but then a deflationary spiral started in 1931. Conditions were worse in farming areas, where commodity prices plunged, and in mining and logging areas, where unemployment was high and there were few other jobs.
The decline in the US economy was the factor that pulled down most other countries at first, then internal weaknesses or strengths in each country made conditions worse or better. Frantic attempts to shore up the economies of individual nations through protectionist policies, such as the 1930 U.S. Smoot–Hawley Tariff Act and retaliatory tariffs in other countries, exacerbated the collapse in global trade. By late 1930, a steady decline in the world economy had set in, which did not reach bottom until 1933.
Economic indicators.
Change in economic indicators 1929–32
Causes.
There were multiple causes for the first downturn in 1929. These include the structural weaknesses and specific events that turned it into a major depression and the manner in which the downturn spread from country to country. In relation to the 1929 downturn, historians emphasize structural factors like major bank failures and the stock market crash. In contrast, monetarist economists (such as Barry Eichengreen, Milton Friedman and Peter Temin) point to monetary factors such as actions by the US Federal Reserve that contracted the money supply, as well as Britain's decision to return to the gold standard at pre–World War I parities (US$4.86:£1).
Recessions and business cycles are thought to be a normal part of living in a world of inexact balances between supply and demand. What turns a normal recession or 'ordinary' business cycle into a depression is a subject of much debate and concern. Scholars have not agreed on the exact causes and their relative importance. The search for causes is closely connected to the issue of avoiding future depressions. A related question is whether the Great Depression was primarily a failure on the part of free markets or a failure of government efforts to regulate interest rates, curtail widespread bank failures, and control the money supply.
Current theories may be broadly classified into two main points of view and several heterodox points of view. There are demand-driven theories, most importantly Keynesian economics, but also including those who point to the breakdown of international trade, and Institutional economists who point to underconsumption and over-investment (causing an economic bubble), malfeasance by bankers and industrialists, or incompetence by government officials. The consensus among demand-driven theories is that a large-scale loss of confidence led to a sudden reduction in consumption and investment spending. Once panic and deflation set in, many people believed they could avoid further losses by keeping clear of the markets. Holding money became profitable as prices dropped lower and a given amount of money bought ever more goods, exacerbating the drop in demand.
There are the monetarists, who believe that the Great Depression started as an ordinary recession, but that significant policy mistakes by monetary authorities (especially the Federal Reserve), caused a shrinking of the money supply which greatly exacerbated the economic situation, causing a recession to descend into the Great Depression. Related to this explanation are those who point to debt deflation causing those who borrow to owe ever more in real terms.
There are also various heterodox theories that downplay or reject the explanations of the Keynesians and monetarists. For example, some new classical macroeconomists have argued that various labor market policies imposed at the start caused the length and severity of the Great Depression. The Austrian school of economics focuses on the macroeconomic effects of money supply, and how central banking decisions can lead to over-investment (economic bubble).
General theoretical explanations.
Mainstream theories.
Keynesian.
British economist John Maynard Keynes argued in "General Theory of Employment Interest and Money" that lower aggregate expenditures in the economy contributed to a massive decline in income and to employment that was well below the average. In such a situation, the economy reached equilibrium at low levels of economic activity and high unemployment.
Keynes' basic idea was simple: to keep people fully employed, governments have to run deficits when the economy is slowing, as the private sector would not invest enough to keep production at the normal level and bring the economy out of recession. Keynesian economists called on governments during times of economic crisis to pick up the slack by increasing government spending and/or cutting taxes.
As the Depression wore on, Franklin D. Roosevelt tried public works, farm subsidies, and other devices to restart the US economy, but never completely gave up trying to balance the budget. According to the Keynesians, this improved the economy, but Roosevelt never spent enough to bring the economy out of recession until the start of World War II.
Monetarist.
Monetarists, including Milton Friedman, argue that the Great Depression was mainly caused by monetary contraction, the consequence of poor policy-making by the American Federal Reserve System and continued crisis in the banking system. In this view, the Federal Reserve, by not acting, allowed the money supply as measured by the M2 to shrink by one-third from 1929–1933, thereby transforming a normal recession into the Great Depression. Friedman argued that the downward turn in the economy, starting with the stock market crash, would have been just another recession.
The Federal Reserve allowed some large public bank failures – particularly that of the New York Bank of the United States – which produced panic and widespread runs on local banks, and the Federal Reserve sat idly by while banks collapsed. He claimed that, if the Fed had provided emergency lending to these key banks, or simply bought government bonds on the open market to provide liquidity and increase the quantity of money after the key banks fell, all the rest of the banks would not have fallen after the large ones did, and the money supply would not have fallen as far and as fast as it did.
With significantly less money to go around, businessmen could not get new loans and could not even get their old loans renewed, forcing many to stop investing. This interpretation blames the Federal Reserve for inaction, especially the New York branch.
One reason why the Federal Reserve did not act to limit the decline of the money supply was regulation. At that time, the amount of credit the Federal Reserve could issue was limited by the Federal Reserve Act, which required 40% gold backing of Federal Reserve Notes issued. By the late 1920s, the Federal Reserve had almost hit the limit of allowable credit that could be backed by the gold in its possession. This credit was in the form of Federal Reserve demand notes.
A "promise of gold" is not as good as "gold in the hand", particularly when they only had enough gold to cover 40% of the Federal Reserve Notes outstanding. During the bank panics a portion of those demand notes were redeemed for Federal Reserve gold. Since the Federal Reserve had hit its limit on allowable credit, any reduction in gold in its vaults had to be accompanied by a greater reduction in credit. On April 5, 1933, President Roosevelt signed Executive Order 6102 making the private ownership of gold certificates, coins and bullion illegal, reducing the pressure on Federal Reserve gold.
Common position.
From the point of view of today's mainstream schools of economic thought, government should strive to keep the interconnected macroeconomic aggregates money supply and/or aggregate demand on a stable growth path. When threatened by the forecast of a depression central banks should pour liquidity into the banking system and the government should cut taxes and accelerate spending in order to keep the nominal money stock and total nominal demand from collapsing.
In contrast in the 1920s leave-it-alone liquidationism was a common position for economists to take. Those liquidationists thought that a depression is good medicine. In their opinion the function of a depression was to liquidate failed investments and businesses that have been made obsolete by technological development in order to release factors of production (capital and labor) from unproductive uses so that these could be redeployed in other sectors of the technologically dynamic economy. They argued that even if self-adjustment of the economy took mass bankruptcies, then so be it. An increasingly common view among economic historians is that the adherence of some Federal Reserve policymakers to the liquidationist thesis led to disastrous consequences. Regarding the policies of President Hoover, economists like Barry Eichengreen and J. Bradford DeLong point out that President Hoover tried to keep the federal budget balanced until 1932, when he lost confidence in his Secretary of the Treasury Andrew Mellon and replaced him. Despite liquidationist expectations, a large proportion of the capital stock was not redeployed but vanished during the first years of the Great Depression. According to a study by Olivier Blanchard and Lawrence Summers, the recession caused a drop of net capital accumulation to pre-1924 levels by 1933. Milton Friedman called the leave-it-alone liquidationism “dangerous nonsense” He wrote:
Heterodox theories.
Austrian School.
Another explanation comes from the Austrian School of economics. Theorists of the "Austrian School" who wrote about the Depression include Austrian economist Friedrich Hayek and American economist Murray Rothbard, who wrote "America's Great Depression" (1963). In their view and like the monetarists, the Federal Reserve, which was created in 1913, shoulders much of the blame; but in opposition to the monetarists, they argue that the key cause of the Depression was the expansion of the money supply in the 1920s that led to an unsustainable credit-driven boom.
In the Austrian view it was this inflation of the money supply that led to an unsustainable boom in both asset prices (stocks and bonds) and capital goods. By the time the Fed belatedly tightened in 1928, it was far too late and, in the Austrian view, a significant economic contraction was inevitable. In February 1929 Hayek published a paper predicting the Federal Reserve's actions would lead to a crisis starting in the stock and credit markets. According to the Austrians, the artificial interference in the economy was a disaster prior to the Depression, and government efforts to prop up the economy after the crash of 1929 only made things worse.
According to Rothbard, government intervention delayed the market's adjustment and made the road to complete recovery more difficult. However, Hayek, unlike Rothbard, also believed, along with the monetarists, that the Federal Reserve further contributed to the problems of the Depression by permitting the money supply to shrink during the earliest years of the Depression.
Marxist.
Karl Marx saw recession and depression as unavoidable under free-market capitalism as there are no restrictions on accumulations of capital other than the market itself. In the Marxist view, capitalism tends to create unbalanced accumulations of wealth, leading to over-accumulations of capital which inevitably lead to a crisis. This especially sharp bust is a regular feature of the boom and bust pattern of what Marxists term "chaotic" capitalist development. It is a tenet of many Marxist groupings that such crises are inevitable and will be increasingly severe until the contradictions inherent in the mismatch between the mode of production and the development of productive forces reach the final point of failure. At which point, the crisis period encourages intensified class conflict and forces societal change.
Specific theories of cause.
Debt deflation.
Irving Fisher argued that the predominant factor leading to the Great Depression was over-indebtedness and deflation. Fisher tied loose credit to over-indebtedness, which fueled speculation and asset bubbles. He then outlined 9 factors interacting with one another under conditions of debt and deflation to create the mechanics of boom to bust. The chain of events proceeded as follows:
During the Crash of 1929 preceding the Great Depression, margin requirements were only 10%. Brokerage firms, in other words, would lend $9 for every $1 an investor had deposited. When the market fell, brokers called in these loans, which could not be paid back.
Banks began to fail as debtors defaulted on debt and depositors attempted to withdraw their deposits "en masse", triggering multiple bank runs. Government guarantees and Federal Reserve banking regulations to prevent such panics were ineffective or not used. Bank failures led to the loss of billions of dollars in assets.
Outstanding debts became heavier, because prices and incomes fell by 20–50% but the debts remained at the same dollar amount. After the panic of 1929, and during the first 10 months of 1930, 744 US banks failed. (In all, 9,000 banks failed during the 1930s). By April 1933, around $7 billion in deposits had been frozen in failed banks or those left unlicensed after the March Bank Holiday.
Bank failures snowballed as desperate bankers called in loans which the borrowers did not have time or money to repay. With future profits looking poor, capital investment and construction slowed or completely ceased. In the face of bad loans and worsening future prospects, the surviving banks became even more conservative in their lending. Banks built up their capital reserves and made fewer loans, which intensified deflationary pressures. A vicious cycle developed and the downward spiral accelerated.
The liquidation of debt could not keep up with the fall of prices which it caused. The mass effect of the stampede to liquidate increased the value of each dollar owed, relative to the value of declining asset holdings. The very effort of individuals to lessen their burden of debt effectively increased it. Paradoxically, the more the debtors paid, the more they owed. This self-aggravating process turned a 1930 recession into a 1933 great depression.
Macroeconomists including Ben Bernanke, the current chairman of the U.S. Federal Reserve Bank, have revived the debt-deflation view of the Great Depression originated by Fisher.
Decline in productivity.
Recent work from a neoclassical perspective focuses on the decline in productivity that caused the initial decline in output and a prolonged recovery due to policies that affected the labor market. This work, collected by Kehoe and Prescott, decomposes the economic decline into a decline in the labor force, capital stock, and the productivity with which these inputs are used.
This study suggests that theories of the Great Depression have to explain an initial severe decline but rapid recovery in productivity, relatively little change in the capital stock, and a prolonged depression in the labor force. This analysis rejects theories that focus on the role of savings and posit a decline in the capital stock.
Breakdown of international trade.
Many economists have argued that the sharp decline in international trade after 1930 helped to worsen the depression, especially for countries significantly dependent on foreign trade. Most historians and economists partly blame the American Smoot-Hawley Tariff Act (enacted June 17, 1930) for worsening the depression by seriously reducing international trade and causing retaliatory tariffs in other countries. While foreign trade was a small part of overall economic activity in the U.S. and was concentrated in a few businesses like farming, it was a much larger factor in many other countries. The average "ad valorem" rate of duties on dutiable imports for 1921–1925 was 25.9% but under the new tariff it jumped to 50% in 1931–1935.
In dollar terms, American exports declined from about $5.2 billion in 1929 to $1.7 billion in 1933; but prices also fell, so the physical volume of exports only fell by half. Hardest hit were farm commodities such as wheat, cotton, tobacco, and lumber. According to this theory, the collapse of farm exports caused many American farmers to default on their loans, leading to the bank runs on small rural banks that characterized the early years of the Great Depression.
Inequality.
Two economists of the 1920s, Waddill Catchings and William Trufant Foster, popularized a theory that influenced many policy makers, including Herbert Hoover, Henry A. Wallace, Paul Douglas, and Marriner Eccles. It held the economy produced more than it consumed, because the consumers did not have enough income. Thus the unequal distribution of wealth throughout the 1920s caused the Great Depression.
According to this view, the root cause of the Great Depression was a global over-investment in heavy industry capacity compared to wages and earnings from independent businesses, such as farms. The solution was the government must pump money into consumers' pockets. That is, it must redistribute purchasing power, maintain the industrial base, but re-inflate prices and wages to force as much of the inflationary increase in purchasing power into consumer spending. The economy was overbuilt, and new factories were not needed. Foster and Catchings recommended federal and state governments start large construction projects, a program followed by Hoover and Roosevelt.
Productivity shock.
The first three decades of the 20th century saw economic output surge with electrification, mass production and motorized farm machinery, and because of the rapid growth in productivity there was a lot of excess production capacity and the work week was being reduced.
The dramatic rise in productivity of major industries in the U. S. and the effects of productivity on output, wages and the work week are discussed by Spurgeon Bell in his book "Productivity, Wages, and National Income" (1940).
Turning point and recovery.
In most countries of the world, recovery from the Great Depression began in 1933. In the U.S., recovery began in early 1933, but the U.S. did not return to 1929 GNP for over a decade and still had an unemployment rate of about 15% in 1940, albeit down from the high of 25% in 1933. The measurement of the unemployment rate in this time period was unsophisticated and complicated by the presence of massive underemployment, in which employers and workers engaged in rationing of jobs.
There is no consensus among economists regarding the motive force for the U.S. economic expansion that continued through most of the Roosevelt years (and the 1937 recession that interrupted it). The common view among most economists is that Roosevelt's New Deal policies either caused or accelerated the recovery, although his policies were never aggressive enough to bring the economy completely out of recession. Some economists have also called attention to the positive effects from expectations of reflation and rising nominal interest rates that Roosevelt's words and actions portended. It was the rollback of those same reflationary policies that led to the interrupting recession of 1937. GDP returned to its upward trend in 1938.
According to Christina Romer, the money supply growth caused by huge international gold inflows was a crucial source of the recovery of the United States economy, and that the economy showed little sign of self-correction. The gold inflows were partly due to devaluation of the U.S. dollar and partly due to deterioration of the political situation in Europe. In their book, "A Monetary History of the United States", Milton Friedman and Anna J. Schwartz also attributed the recovery to monetary factors, and contended that it was much slowed by poor management of money by the Federal Reserve System. Current Chairman of the Federal Reserve Ben Bernanke agrees that monetary factors played important roles both in the worldwide economic decline and eventual recovery. Bernanke, also sees a strong role for institutional factors, particularly the rebuilding and restructuring of the financial system, and points out that the Depression needs to be examined in international perspective.
Gold standard.
Some economic studies have indicated that just as the downturn was spread worldwide by the rigidities of the Gold Standard, it was suspending gold convertibility (or devaluing the currency in gold terms) that did the most to make recovery possible.
Every major currency left the gold standard during the Great Depression. Great Britain was the first to do so. Facing speculative attacks on the pound and depleting gold reserves, in September 1931 the Bank of England ceased exchanging pound notes for gold and the pound was floated on foreign exchange markets.
Great Britain, Japan, and the Scandinavian countries left the gold standard in 1931. Other countries, such as Italy and the U.S., remained on the gold standard into 1932 or 1933, while a few countries in the so-called "gold bloc", led by France and including Poland, Belgium and Switzerland, stayed on the standard until 1935–1936.
According to later analysis, the earliness with which a country left the gold standard reliably predicted its economic recovery. For example, Great Britain and Scandinavia, which left the gold standard in 1931, recovered much earlier than France and Belgium, which remained on gold much longer. Countries such as China, which had a silver standard, almost avoided the depression entirely. The connection between leaving the gold standard as a strong predictor of that country's severity of its depression and the length of time of its recovery has been shown to be consistent for dozens of countries, including developing countries. This partly explains why the experience and length of the depression differed between national economies.
World War II and recovery.
The common view among economic historians is that the Great Depression ended with the advent of World War II. Many economists believe that government spending on the war caused or at least accelerated recovery from the Great Depression, though some consider that it did not play a very large role in the recovery. It did help in reducing unemployment.
The rearmament policies leading up to World War II helped stimulate the economies of Europe in 1937–39. By 1937, unemployment in Britain had fallen to 1.5 million. The mobilisation of manpower following the outbreak of war in 1939 ended unemployment.
The US' entry into the war in 1941 finally eliminated the last effects from the Great Depression and brought the U.S. unemployment rate down below 10%. In the U.S., massive war spending doubled economic growth rates, either masking the effects of the Depression or essentially ending the Depression. Businessmen ignored the mounting national debt and heavy new taxes, redoubling their efforts for greater output to take advantage of generous government contracts.
Effects.
The majority of countries set up relief programs, and most underwent some sort of political upheaval, pushing them to the left or right. In some states, the desperate citizens turned toward nationalist demagoguery — infamously propelling Adolf Hitler to power in Germany — setting the stage for World War II.
Australia.
Australia's dependence on agricultural and industrial exports meant it was one of the hardest-hit countries in the Western world. Falling export demand and commodity prices placed massive downward pressures on wages. Further, unemployment reached a record high of 29% in 1932, with incidents of civil unrest becoming common. After 1932, an increase in wool and meat prices led to a gradual recovery.
Canada.
Harshly affected by both the global economic downturn and the Dust Bowl, Canadian industrial production had fallen to only 58% of the 1929 level by 1932, the second lowest level in the world after the United States, and well behind nations such as Britain, which saw it fall only to 83% of the 1929 level. Total national income fell to 56% of the 1929 level, again worse than any nation apart from the United States. Unemployment reached 27% at the depth of the Depression in 1933.
Chile.
The League of Nations labeled Chile the country hardest hit by the Great Depression because 80% of government revenue came from exports of copper and nitrates, which were in low demand.
Chile initially felt the impact of the Great Depression in 1930, when GDP dropped 14%, mining income declined 27%, and export earnings fell 28%. By 1932, GDP had shrunk to less than half of what it had been in 1929, exacting a terrible toll in unemployment and business failures.
Influenced profoundly by the Great Depression, many national leaders promoted the development of local industry in an effort to insulate the economy from future external shocks. After six years of government austerity measures, which succeeded in reestablishing Chile's creditworthiness, Chileans elected to office during the 1938–58 period a succession of center and left-of-center governments interested in promoting economic growth by means of government intervention.
Prompted in part by the devastating 1939 Chillán earthquake, the Popular Front government of Pedro Aguirre Cerda created the Production Development Corporation (Corporación de Fomento de la Producción, CORFO) to encourage with subsidies and direct investments an ambitious program of import substitution industrialization. Consequently, as in other Latin American countries, protectionism became an entrenched aspect of the Chilean economy.
China.
China was largely unaffected by the Depression, mainly by having stuck to the Silver standard. However, the US silver purchase act of 1934 created an intolerable demand on China's silver coins, and so in the end the silver standard was officially abandoned in 1935 in favor of the four Chinese national banks' "legal note" issues. China and the British colony of Hong Kong, which followed suit in this regard in September 1935, would be the last to abandon the silver standard. In addition, the Nationalist Government also acted energetically to modernize the legal and penal systems, stabilize prices, amortize debts, reform the banking and currency systems, build railroads and highways, improve public health facilities, legislate against traffic in narcotics and augment industrial and agricultural production. On November 3, 1935, the government instituted the fiat currency (fapi) reform, immediately stabilizing prices and also raising revenues for the government.
France.
The Depression began to affect France around 1931. France's relatively high degree of self-sufficiency meant the damage was considerably less than in nations like Germany. Hardship and unemployment were high enough to lead to rioting and the rise of the socialist Popular Front. Ultra-nationalist groups also saw increased popularity, although democracy prevailed into World War II.
Germany.
Germany's Weimar Republic was hit hard by the depression, as American loans to help rebuild the German economy now stopped. Unemployment soared, especially in larger cities, and the political system veered toward extremism. The unemployment rate reached nearly 30% in 1932, bolstering support for the Nazi (NSDAP) and Communist (KPD) parties, which both rose in the years following the crash to altogether possess a Reichstag majority following the general election in July 1932.
Repayments of the war reparations due by Germany were suspended in 1932 following the Lausanne Conference of 1932. By that time, Germany had repaid one eighth of the reparations. Hitler and the Nazi Party came to power in January 1933, establishing a totalitarian single-party state within months and initiating the path towards World War II, the most devastating conflict in world history.
Japan.
The Great Depression did not strongly affect Japan. The Japanese economy shrank by 8% during 1929–31. Japan's Finance Minister Takahashi Korekiyo was the first to implement what have come to be identified as Keynesian economic policies: first, by large fiscal stimulus involving deficit spending; and second, by devaluing the currency. Takahashi used the Bank of Japan to sterilize the deficit spending and minimize resulting inflationary pressures. Econometric studies have identified the fiscal stimulus as especially effective.
The devaluation of the currency had an immediate effect. Japanese textiles began to displace British textiles in export markets. The deficit spending proved to be most profound. The deficit spending went into the purchase of munitions for the armed forces. By 1933, Japan was already out of the depression. By 1934, Takahashi realized that the economy was in danger of overheating, and to avoid inflation, moved to reduce the deficit spending that went towards armaments and munitions.
This resulted in a strong and swift negative reaction from nationalists, especially those in the army, culminating in his assassination in the course of the February 26 Incident. This had a chilling effect on all civilian bureaucrats in the Japanese government. From 1934, the military's dominance of the government continued to grow. Instead of reducing deficit spending, the government introduced price controls and rationing schemes that reduced, but did not eliminate inflation, which would remain a problem until the end of World War II.
The deficit spending had a transformative effect on Japan. Japan's industrial production doubled during the 1930s. Further, in 1929 the list of the largest firms in Japan was dominated by light industries, especially textile companies (many of Japan's automakers, like Toyota, have their roots in the textile industry). By 1940 light industry had been displaced by heavy industry as the largest firms inside the Japanese economy.
Latin America.
Because of high levels of U.S. investment in Latin American economies, they were severely damaged by the Depression. Within the region, Chile, Bolivia and Peru were particularly badly affected.
Netherlands.
From roughly 1931–1937, the Netherlands suffered a deep and exceptionally long depression. This depression was partly caused by the after-effects of the Stock Market Crash of 1929 in the U.S., and partly by internal factors in the Netherlands. Government policy, especially the very late dropping of the Gold Standard, played a role in prolonging the depression. The Great Depression in the Netherlands led to some political instability and riots, and can be linked to the rise of the Dutch national-socialist party NSB. The depression in the Netherlands eased off somewhat at the end of 1936, when the government finally dropped the Gold Standard, but real economic stability did not return until after World War II.
Portugal.
Already under the rule of a dictatorial junta, the Ditadura Nacional, Portugal suffered no turbulent political effects of the Depression, although Antonio de Oliveira Salazar, already appointed Minister of Finance in 1928 greatly expanded his powers and in 1932 rose to Prime Minister of Portugal to found the Estado Novo, an authoritarian corporatist dictatorship.
With the budget balanced in 1929, the effects of the depression were relaxed through harsh measures towards budget balance and autarky, causing social discontent but stability and, eventually, an impressive economic growth. The regime outlived Salazar himself before being overthrown in the Carnation Revolution in 1974, initiating a road towards the restoration of democracy.
South Africa.
As world trade slumped, demand for South African agricultural and mineral exports fell drastically. The Carnegie Commission on Poor Whites had concluded in 1931 that nearly one third of Afrikaners lived as pauper. It is believed that the social discomfort caused by the depression was a contributing factor in the 1933 split between the "gesuiwerde" (purified) and "smelter" (fusionist) factions within the National Party and the National Party's subsequent fusion with the South African Party. Eventually, the gesuiwerde faction of Daniel Malan would go on to form its own party and take over the government after the 1948 election, bringing about the doctrine of apartheid, instituting and extending racial segregation, which would see an end only in 1994.
Soviet Union.
The Soviet Union was the world's sole communist state with very little international trade. At the time of the Depression, the Soviet economy was growing steadily, fueled by intensive investment in heavy industry. The apparent economic success of the Soviet Union at a time when the capitalist world was in crisis led many Western intellectuals to view the Soviet system favorably. Jennifer Burns wrote, "As the Great Depression ground on and unemployment soared, intellectuals began unfavorably comparing their faltering capitalist economy to Russian Communism. ... More than ten years after the Revolution, Communism was finally reaching full flower, according to the "New York Times" reporter Walter Duranty, a Stalin fan who vigorously debunked accounts of the Ukraine famine, a man-made disaster that would leave millions dead."
Spain.
The Depression in Spain exacerbated political and economic crises that led ultimately to the Spanish Civil War. The dictatorship of Prime Minister Miguel Primo de Rivera collapsed 1930 largely as a result of the government's inability to deal with the Depression. This was followed by the ousting of King Alfonso XIII in the following year, creating a Second Spanish Republic that had to grapple with the same social and economic problems. Economic instability and political strife continued, and after the divisive general election of 1936 the climate of violent antagonism between Left and Right expanded into the Spanish Civil War.
Sweden.
Taking place in the midst of a short-lived government and a less-than-a-decade old Swedish democracy, events such as those surrounding Ivar Kreuger (who eventually committed suicide) remain infamous in Swedish history. Eventually, the Social Democrats under Per Albin Hansson would form their first long-lived government in 1932 based on strong interventionist and welfare state policies, monopolizing the office of Prime Minister until 1976 with the sole and short-lived exception of Axel Pehrsson-Bramstorp's "summer cabinet" in 1936. During forty years of hegemony, it was the most successful political party in the history of Western liberal democracy.
Thailand.
In Thailand, then known as the Kingdom of Siam, the Great Depression contributed to the end of the absolute monarchy of King Rama VII in the Siamese revolution of 1932.
United Kingdom.
The effects on the northern industrial areas of Britain were immediate and devastating, as demand for traditional industrial products collapsed. By the end of 1930 unemployment had more than doubled from 1 million to 2.5 million (20% of the insured workforce), and exports had fallen in value by 50%. In 1933, 30% of Glaswegians were unemployed due to the severe decline in heavy industry. In some towns and cities in the north east, unemployment reached as high as 70% as shipbuilding fell 90%. The National Hunger March of September–October 1932 was the largest of a series of hunger marches in Britain in the 1920s and 1930s. About 200,000 unemployed men were sent to the work camps, which continued in operation until 1939.
In the less industrial Midlands and Southern England, the effects were short-lived and the later 1930s were a prosperous time. Growth in modern manufacture of electrical goods and a boom in the motor car industry was helped by a growing southern population and an expanding middle class. Agriculture also saw a boom during this period.
United States.
Hoover’s first measures to combat the depression were based on voluntarism by businesses not to reduce their workforce or cut wages. But businesses had little choice and wages were reduced, workers were laid off, and investments postponed.
In June 1930 Congress approved the Smoot–Hawley Tariff Act which raised tariffs on thousands of imported items. The intent of the Act was to encourage the purchase of American-made products by increasing the cost of imported goods, while raising revenue for the federal government and protecting farmers. Other nations increased tariffs on American-made goods in retaliation, reducing international trade, and worsening the Depression.
In 1931 Hoover urged bankers to set up the National Credit Corporation so that big banks could help failing banks survive. But bankers were reluctant to invest in failing banks, and the National Credit Corporation did almost nothing to address the problem.
By 1932, unemployment had reached 23.6%, and it peaked in early 1933 at 25%, drought persisted in the agricultural heartland, businesses and families defaulted on record numbers of loans, and more than 5,000 banks had failed. Hundreds of thousands of Americans found themselves homeless, and began congregating in shanty towns – dubbed "Hoovervilles" – that began to appear across the country. In response, President Hoover and Congress approved the Federal Home Loan Bank Act, to spur new home construction, and reduce foreclosures. The final attempt of the Hoover Administration to stimulate the economy was the passage of the Emergency Relief and Construction Act (ERA) which included funds for public works programs such as dams and the creation of the Reconstruction Finance Corporation (RFC) in 1932. The Reconstruction Finance Corporation was a Federal agency with the authority to lend up to $2 billion to rescue banks and restore confidence in financial institutions. But $2 billion was not enough to save all the banks, and bank runs and bank failures continued. Quarter by quarter the economy went downhill, as prices, profits and employment fell, leading to the political realignment in 1932 that brought to power Franklin Delano Roosevelt.
Shortly after President Franklin Delano Roosevelt was inaugurated in 1933, drought and erosion combined to cause the Dust Bowl, shifting hundreds of thousands of displaced persons off their farms in the Midwest. From his inauguration onward, Roosevelt argued that restructuring of the economy would be needed to prevent another depression or avoid prolonging the current one. New Deal programs sought to stimulate demand and provide work and relief for the impoverished through increased government spending and the institution of financial reforms.
During a "bank holiday" that lasted five days, the Emergency Banking Act was signed into law. It provided for a system of reopening sound banks under Treasury supervision, with federal loans available if needed. The Securities Act of 1933 comprehensively regulated the securities industry. This was followed by the Securities Exchange Act of 1934 which created the Securities and Exchange Commission. Though amended, key provisions of both Acts are still in force. Federal insurance of bank deposits was provided by the FDIC, and the Glass–Steagall Act.
The Agricultural Adjustment Act provided incentives to cut farm production in order to raise farming prices. The National Recovery Administration (NRA) made a number of sweeping changes to the American economy. It forced businesses to work with government to set price codes through the NRA to fight deflationary "cut-throat competition" by the setting of minimum prices and wages, labor standards, and competitive conditions in all industries. It encouraged unions that would raise wages, to increase the purchasing power of the working class. The NRA was deemed unconstitutional by the Supreme Court of the United States in 1935.
These reforms, together with several other relief and recovery measures, are called the First New Deal. Economic stimulus was attempted through a new alphabet soup of agencies set up in 1933 and 1934 and previously extant agencies such as the Reconstruction Finance Corporation. By 1935, the "Second New Deal" added Social Security (which was later considerably extended through the Fair Deal), a jobs program for the unemployed (the Works Progress Administration, WPA) and, through the National Labor Relations Board, a strong stimulus to the growth of labor unions. In 1929, federal expenditures constituted only 3% of the GDP. The national debt as a proportion of GNP rose under Hoover from 20% to 40%. Roosevelt kept it at 40% until the war began, when it soared to 128%.
By 1936, the main economic indicators had regained the levels of the late 1920s, except for unemployment, which remained high at 11%, although this was considerably lower than the 25% unemployment rate seen in 1933. In the spring of 1937, American industrial production exceeded that of 1929 and remained level until June 1937. In June 1937, the Roosevelt administration cut spending and increased taxation in an attempt to balance the federal budget.
The American economy then took a sharp downturn, lasting for 13 months through most of 1938. Industrial production fell almost 30 per cent within a few months and production of durable goods fell even faster. Unemployment jumped from 14.3% in 1937 to 19.0% in 1938, rising from 5 million to more than 12 million in early 1938. Manufacturing output fell by 37% from the 1937 peak and was back to 1934 levels.
Producers reduced their expenditures on durable goods, and inventories declined, but personal income was only 15% lower than it had been at the peak in 1937. As unemployment rose, consumers' expenditures declined, leading to further cutbacks in production. By May 1938 retail sales began to increase, employment improved, and industrial production turned up after June 1938. After the recovery from the Recession of 1937–1938, conservatives were able to form a bipartisan conservative coalition to stop further expansion of the New Deal and, when unemployment dropped to 2% in the early 1940s, they abolished WPA, CCC and the PWA relief programs. Social Security remained in place.
Between 1933 and 1939, federal expenditure tripled, and Roosevelt's critics charged that he was turning America into a socialist state. The Great Depression was a main factor in the implementation of social democracy and planned economies in European countries after World War II (see Marshall Plan). Keynesianism remained the most influential economic school until the 1970s, when Milton Friedman and other economists propagated Monetarism as an alternative approach.
Literature.
The Great Depression has been the subject of much writing, as authors have sought to evaluate an era that caused financial as well as emotional trauma. Perhaps the most noteworthy and famous novel written on the subject is "The Grapes of Wrath", published in 1939 and written by John Steinbeck, who was awarded both the Nobel Prize for literature and the Pulitzer Prize for the work. The novel focuses on a poor family of sharecroppers who are forced from their home as drought, economic hardship, and changes in the agricultural industry occur during the Great Depression. Steinbeck's "Of Mice and Men" is another important novel about a journey during the Great Depression. Additionally, Harper Lee's "To Kill a Mockingbird" is set during the Great Depression. Margaret Atwood's Booker prize-winning "The Blind Assassin" is likewise set in the Great Depression, centering on a privileged socialite's love affair with a Marxist revolutionary. The era spurred the resurgence of social realism, practiced by many who started their writing careers on relief programs, especially the Federal Writers' Project in the U.S.
Naming.
The term "The Great Depression" is most frequently attributed to British economist Lionel Robbins, whose 1934 book "The Great Depression" is credited with formalizing the phrase, though Hoover is widely credited with popularizing the term, informally referring to the downturn as a depression, with such uses as "Economic depression cannot be cured by legislative action or executive pronouncement" (December 1930, Message to Congress), and "I need not recount to you that the world is passing through a great depression" (1931).
The term "depression" to refer to an economic downturn dates to the 19th century, when it was used by varied Americans and British politicians and economists. Indeed, the first major American economic crisis, the Panic of 1819, was described by then-president James Monroe as "a depression", and the most recent economic crisis, the Depression of 1920–21, had been referred to as a "depression" by then-president Calvin Coolidge.
Financial crises were traditionally referred to as "panics", most recently the major Panic of 1907, and the minor Panic of 1910–1911, though the 1929 crisis was called "The Crash", and the term "panic" has since fallen out of use. At the time of the Great Depression, the term "The Great Depression" was already used to referred to the period 1873–96 (in the United Kingdom), or more narrowly 1873–79 (in the United States), which has retroactively been renamed the Long Depression.
Other "great depressions".
Other economic downturns have been called a "great depression", but none had been as widespread, or lasted for so long. Various nations have experienced brief or extended periods of economic downturns, which were referred to as "depressions", but none have had such a widespread global impact.
The collapse of the Soviet Union, and the breakdown of economic ties which followed, led to a severe economic crisis and catastrophic fall in the standards of living in the 1990s in post-Soviet states and the former Eastern Bloc, which was even worse than the Great Depression. Even before Russia's financial crisis of 1998, Russia's GDP was half of what it had been in the early 1990s, and some populations are still poorer than they were in 1989, including Ukraine, Moldova, Central Asia, and the Caucasus.
Comparison with the late-2000s recession.
Some journalists and economists have taken to calling the late-2000s recession the "Great Recession" in allusion to the Great Depression.
The causes of the Great Recession seem similar to the Great Depression, but significant differences exist. The current chairman of the Federal Reserve, Ben Bernanke, had extensively studied the Great Depression as part of his doctoral work at MIT, and is implementing policies to manipulate the money supply and interest rates in ways that were not done in the 1930s. Bernanke's policies will undoubtedly be analyzed and scrutinized in the years to come, as economists debate the wisdom of his choices. Generally speaking, the recovery of the world's financial systems tended to be quicker during the Great Depression of the 1930s as opposed to the late-2000s recession.
1928 and 1929 were the times in the 20th century that the wealth gap reached such skewed extremes; half the unemployed had been out of work for over six months, something that was not repeated until the late-2000s recession. 2007 and 2008 eventually saw the world reach new levels of wealth gap inequality that rivalled the years of 1928 and 1929.
See also.
General:

</doc>
<doc id="19283339" url="http://en.wikipedia.org/wiki?curid=19283339" title="The Official Work">
The Official Work

The Official Work is a mixtape by Atlanta-based rap duo Ying Yang Twins, released on August 26, 2008.

</doc>
<doc id="19283359" url="http://en.wikipedia.org/wiki?curid=19283359" title="LAX (album)">
LAX (album)

LAX is the third studio album by American rapper Game, released August 26, 2008 on Geffen Records. He had originally announced that Dr. Dre would be producing for the album, but neither Dr. Dre nor Aftermath Entertainment had confirmed. Production for the album was contributed by longtime collaborators Cool & Dre, Kanye West, Scott Storch, Nottz, Hi-Tek, and JellyRoll, among others. Guests featured on "LAX" include Chrisette Michele, Common, Keyshia Cole, Ludacris, Nas, Ne-Yo, Raekwon, Raheem DeVaughn, Travis Barker, Bilal, and Lil Wayne. The album was released with 2 different cases such as one on the deluxe version with Game looking at the camera with his Bandanna in his hand and another with him sitting on a couch smoking a blunt.
"LAX" debuted at number two on the US "Billboard" 200 chart, selling over 238,000 copies in its first week, just behind metal band Slipknot's "All Hope Is Gone". At first it looked like "LAX" had debuted ahead of "All Hope Is Gone" by 13 copies; with such a close difference, Slipknot's labels Warner Music Group and Roadrunner Records asked for a SoundScan recount, a historic first. Nielsen proceeded to the recount, which placed "LAX" at number two with 238,382 copies, and Slipknot in first position with 239,516 copies scanned, a margin of 1,134 copies.
Initially, "Billboard" published an article stating that The Game had secured the top spot with a margin of 13 units, in what was described as the "closest race for number one since SoundScan began tracking Data in 1991". After a recount 12 hours later, the article was rewritten and Slipknot was awarded the number one spot, having sold 239,516 units. As of September 2011, the album has sold 765,000 copies. Upon its release, "LAX" received generally favorable reviews from most music critics, with music critics praising the album's production.
Background.
After signing to Interscope Records and Dr. Dre's Aftermath Entertainment vanity label (an agreement that only happened after Interscope Records head Jimmy Iovine was debating whether to drop the young upstart or keep him, ultimately convincing Curtis "50 Cent" Jackson to take him under his wing in an effort to guarantee record sales). His debut, The Documentary, sounded entirely different from his follow-up, the Doctor's Advocate, as he gained more experience in the chosen genre.
After two albums driven by his worship of legendary West Coast producer Dr. Dre plus later feuds with fellow rappers like 50 Cent and the G-Unit crew, Game's third official effort was to be his most important release and the strongest argument yet that it just might be time to move on. So on December 31, 2007, Game announced at Nas' New Year's Eve party the title and release date of the album.
Game appeared on 106 & Park on May 16, where he confirmed LAX would be the last studio album he records, as he intends to promote his record label, Black Wall Street. However in a later interview, The Game said that he may release a fourth album titled ""D.O.C."" or ""Diary of Compton"" but only if he can get the production help of Dr. Dre, MC Ren, King Tee, DJ Yella, Ice Cube and DJ Quik. However, he recently said that "D.O.C." would not be coming out and that LAX was definitely his last album.
Recording.
On May 11, 2007, Game was arrested at his home reportedly in connection with an incident at a basketball game in South Los Angeles in February 2007. He is alleged to have threatened a person with a gun. The arrest took place after his home was searched for three hours. Game was released early the next day after posting $50,000 bail. On January 9, 2008, a Los Angeles judge scheduled February 4 as the beginning date for Game's trial on assault and weapons charges. After pleading no contest to a felony weapons charge on February 11, Game was sentenced to 60 days in jail, 150 hours of community service, and three years probation. It was reported near mid-March that The Game had been released from jail. His manager later stated that he had not yet been but was expected to be released in time to promote the album.
He was released after serving eight days, and went back to work with Cool & Dre. Dre (of Cool & Dre), stated The Game was like 'Pac, and that lyrically, he was "else right now". On July 28, 2008, The Game told J Hyphen and J. Moore of Sunday Night Sound Sessions that the album was finished and the official release date was August 26. He said the album was going to be 16 tracks long and he would leak the clean version two weeks before the official release. The Game mentioned that he recorded over 220 tracks for the album. Recording artists reported to have participated in the sessions for "LAX" at one time or another included at first Nu Jerzey Devil had stated that Lil Wayne would be making a guest appearance. It was later confirmed that Akon, André 3000, Busta Rhymes, Chris Brown, Chrisette Michele, Common, Ice Cube, Keyshia Cole, DMX, Ludacris, Snoop Dogg, Marsha Ambrosius, Mary J. Blige, Nas, Ne-Yo, Raekwon, Raheem DeVaughn, Fabolous, and Robin Thicke were all to make appearances, however they did not all make the final cut on the album. Although Travis Barker, Bilal, Keyshia Cole, Common, Raheem DeVaughn, DMX, Ice Cube, Lil Wayne, Ludacris, Chrisette Michele, Nas, Ne-Yo, Raekwon, and LaToiya Williams are the only guests on the album.
Record producers who participated at the album's recording sessions with The Game included Jelly Roll, Nottz, DJ Toomp, J. R. Rotem, Scott Storch, Kanye West, 1500 or Nothin', Travis Barker, DJ Quik, Knobody, Dahoud Darien, Hi-Tek, Ervin "EP" Pope, Cool & Dre, Irv Gotti, Tre Beatz, and Trackmasters. The Game originally had announced that Dr. Dre would be producing for the album, but neither Dr. Dre nor Aftermath Entertainment had confirmed. Nu Jerzey Devil also later confirmed Dr. Dre as a producer. On May 1, 2008 The Game told Power 106 that at the time he had worked with Just Blaze, Kanye West, Cool & Dre, Scott Storch, Timbaland, Knobody, Ervin "E.P." Pope, JellyRoll & Tre Beatz. At a listening party on June 23, 2008 it was mentioned that the Trackmasters were in part of the production of the album.
Release and promotion.
The album was pushed back to June 24, which would have had the album in competition with G-Unit's , but was later changed to July 8, as Interscope Records moved both albums. On June 8, it was announced that LAX would be pushed back a week later to July 15, the same release date as Nas' "Untitled". LAX had then been pushed back another week to July 22, and finally to August 26. On May 1, The Game told Power 106 that "Big Dreams" might not be on the album.
On August 6, 2008 iTunes revealed some information on the album via the iTunes Store. They confirmed the DJ Toomp produced track "House of Pain" would be the fourth single. They had also revealed that the standard edition would have fourteen tracks with one bonus track and the "deluxe edition" would have 18 tracks and one additional bonus track. On August 10, 2008 The Game told Friday Night Flavas that he would be dropping two mixtapes; "Superman", featuring all original tracks, which did not appear on the album, in a week and a half and "You Know What It Is Vol. 5" after the album. He also blamed DJ Haze for the leak of the Just Blaze produced track "Superman".
Promo singles.
The first street single was released on March 18, 2008 "Big Dreams" which is produced by Cool & Dre. The Game commented on Power 106 that the song would not be released on any specific album, including the upcoming "LAX". However, a modified version of the song, with different lyrics in the third verse, was later included on the bonus disc of the "Deluxe Version" of the album. Dre, from the production duo Cool & Dre commented that "'Big Dreams' is one of the most amazing records that I can honestly say that me and Cool have been a part of". Dre continued by stating "'Big Dreams' is a phenomenal record. It's very inspirational, It's gonna be big in the streets, but it's gonna be big all across the board. I feel like 'Hate It or Love It' was such an inspirational record that a lot of people didn't see coming. And this has the same feel as far as how it makes you feel, but it's gonna inspire the shit outta everybody. It's one of those records — his performance, his delivery, he's lyrically on a level that I can't even compare anyone to. He stepped it up to a notch that's amazing. He definitely has a new passion. And the fucking record is gonna really, really destroy."
The second street single was "House of Pain" and it was released on August 19, 2008. The single is produced by DJ Toomp. The song "Touchdown" also debuted on the Hot 100 without being released as a single, it charted at number 57 on the Hot R&B/Hip-Hop Songs chart.
Singles.
The first official single "Game's Pain" featuring Keyshia Cole was released on April 29, 2008. It charted moderately in the US but was met with critical acclaim ""Game’s Pain", Keyshia Cole sums up in one sentence what Game has been trying to say the whole album; "I just wanna let you know/ I’m paying homage 'cause you’ve paved the way for me." Game underscores the central themes of hip-hop’s history and his deserved place in it, meditating on the struggles he's faced." The second single was "Dope Boys" featuring Blink-182 drummer Travis Barker, the video released was deemed to be too edgy to be aired on TV by BET, but it has been lauded critically as "not only one of the best collaborations on L.A.X. but also one of the best songs of Game’s career." The third single released was "My Life" featuring Lil Wayne, when the song first leaked, many believed The Game was "dissing" Eminem. However, later on his website, Game denied this, and apologized to anyone who might have misunderstood what he had intended to say. It charted well in the US reaching a peak of 21 in "Billboard" Hot 100 and it peaked at 4 for Hot Rap Tracks. The fourth and final single was released featuring Ne-Yo called "Camera Phone" on January 12, 2009, it was only released in the UK.
Tour.
On May 21, 2009 Game announced upcoming tour dates for performances in the United Kingdom for the summer of 2009. The West Coast-bred emcee made his first United Kingdom appearance since last December 2008. Game returned to the United Kingdom with three live dates confirmed across the country. Game assembled a coast-to-coast US trek to support his latest effort, "LAX". The club/theater outing was scheduled to visit nearly 30 cities, starting with several California performances in mid-February 2009.
The tour Begin on February 18, 2009 and concluding March 29, 2009 stops included Los Angeles, California, Phoenix, Arizona, Sacramento, California, Portland, Oregon, Seattle, Washington, Boise, Idaho, Denver, Colorado, Tulsa, Oklahoma, Dallas, Texas, Houston, Texas, New Orleans, Louisiana, New Haven, Connecticut, Providence, Rhode Island, Norfolk, Virginia, New York City, New York and more. The following year Game went on the European Club Tour 2010.
Reception.
Commercial performance.
The album, which has received favorable reviews from critics, debuted at number two on the US "Billboard" 200 chart, with approximately 239,000 copies sold, just behind metal band Slipknot's "All Hope Is Gone". The album charted at number one on the Top R&B/Hip-Hop Albums chart, and at number one on the Top Rap Albums chart. At first it looked like "LAX" had debuted ahead of "All Hope Is Gone" by 13 copies, with such a close difference, Slipknot's labels Warner Music Group and Roadrunner Records asked for a SoundScan recount, a historic first. Nielsen proceeded to the recount, which made no changes, and Slipknot in 2nd position with 239,516 copies scanned. Initially, "Billboard" published an article stating that The Game had secured the top spot with a margin of 13 units, in what was described as the "closest race for number one since SoundScan began tracking Data in 1991". having sold 239,516 units. As of October 22, 2011 the album had sold 765,100 copies in the US.
"LAX" attained respectable international charting. In Australia the album entered the Australian Albums Chart at number 12, in Austria the album entered the Austrian Albums Chart at number 29, in Belgium the album entered the Belgian Albums Chart at number 22, in Canada the album entered the Canadian Albums Chart at number 2, in Denmark the album entered the Danish Album Chart at number 18, in the Netherlands the album entered the Dutch Albums Chart at number 21, in Europe the album entered the European Top 100 Albums Chart at number 14, in France the album entered the French Albums Chart at number 19, in Germany the album entered the German Albums Chart at number 33, in the Republic of Ireland the album entered the Irish Albums Chart at number 8, in Italy the album entered the Italian Albums Chart at number 2, in New Zealand the album entered the New Zealand Albums Chart at number 11, in Norway the album entered the Norwegian Albums Chart at number 18, in Sweden the album entered the Swedish Albums at number 3, in Switzerland the album entered the Swiss Albums Chart at number 8, and in the United Kingdom the album entered the UK Albums Chart at number 9, and at number 1 on the UK R&B Chart.
Critical response.
Upon its release, "LAX" received mostly favorable reviews from music critics. At Metacritic, which assigns a normalized rating out of 100 to reviews from mainstream critics, the album received an average score of 65, based on 20 reviews, which indicates "Generally favorable reviews". About.com praised songs like "Let Us Live" as "Game puts on his best Nas impersonation as he flows off the beat with intricate and polysyllabic rhymes." Allmusic writer David Jeffries noted "the cuts that truly matter on LAX aren't the ones where the rapper's hardcore, unswayable definition of loyalty comes into play but the ones that go outside the usual topics and explore both the profound (the African-American struggle) and, more surprisingly, the profane (rump shaking)" and that "this scattershot album is easy to recommend despite its flaws." "USA Today" gave the album a perfect score saying "throughout, The Game flays rivals and trumpets his own skills and success. He attacks the beats — provided by the likes of J.R. Rotem, Kanye West, Cool & Dre, Scott Storch, Hi-Tek and DJ Toomp — with lyrical ferocity." "The A.V. Club" noted "Only three major-label albums into his career, The Game has already appropriated the angst of rap's most beloved icons" and praising the album's production by stating that "The Game has always borrowed from the greats. Here, he cannibalizes his own tired shtick so extensively, he lapses even further into self-parody."
"The New York Times", which previously said "Doctor's Advocate" was the best hip-hop album of 2006, gave a mixed review saying that "Worse, the Game, never a fluid rapper, sounds positively lumpy, as if he were delivering verses while running up a steep flight of stairs, or as if the last few years of pugnacity have finally left him winded." "Entertainment Weekly" noted "As on his first two efforts, he spends L.A.X. barking gleefully ignorant gangsta fantasies over hard-knocking drums. And while the 19-track disc could use a good trimming, The Game's routine is just as entertaining the third time around." "The Guardian"'s Angus Batey stated "LAX is an intense and remarkably focused record - almost every syllable concerns Compton, gangsta rap and (as one song title has it)." Pitchfork Media stated that "Relatively, he's won Round 3 by making his third straight album that's better than it has any right to be-- but the fact that the Game can make perfectly uncompelling competence sound like victory is proof that he's a master thespian of hip-hop theater." Jordan Sargent of PopMatters complimented The Game stating that "On LAX, Game hasn’t changed, but he’s picked a group of beats that get him closer to extricating himself from both his West Coast Messiah complex and the post-G-Unit narrative. And while Game has yet to carve out his own identity as a rap artist, LAX shows that, on his third album, he might be on the right track."
"The Village Voice" commented that "Taylor's best assets remain his compellingly ruined wheeze of a voice, relentless delivery, and uncanny ear for beats" and that "Somehow, the Game is still coasting on wispy, West Coast–nostalgia fumes—chronic, red rags, lolos, etc.—but the goodwill, at this point, has pretty much exhausted itself." Sputnikmusic's Tyler Munro noted it as a "Solid hip-hop, but excessive to a fault. Too many guests, too many shifts in style and theme." Slant Magazine's Wilson McBee viewed it as The Game's best album Stating "With the Game's third and best album, LAX, which drops without the baggage of a high-profile beef, we learn more about who the rapper really is: a guy who loves hip-hop, from top to bottom, and is as comfortable giving shout-outs to Will Smith and Uncle Luke as he is to Wu-Tang and N.W.A" and "Listening to LAX is like witnessing the creation of a mural of hip-hop's history in which the artist paints himself hiding among the famous faces."
Despite favorable reviews by critics and audiences, Game was critical about his album and says: "If I had a choice, I would say fuck L.A.X. and R.E.D. because I was kind of lost in trying to re-find the love for hip-hop[...]. He also doesn't feel the album was perfect by his standards compared to his previous album.
Personnel.
Credits for "LAX" adapted from Allmusic.

</doc>
<doc id="19283361" url="http://en.wikipedia.org/wiki?curid=19283361" title="New Deal">
New Deal

The New Deal was a series of domestic programs enacted in the United States between 1933 and 1938. They involved laws passed by Congress as well as presidential executive orders during the first term of President Franklin D. Roosevelt. The programs were in response to the Great Depression, and focused on what historians call the "3 Rs": Relief, Recovery, and Reform. That is Relief for the unemployed and poor; Recovery of the economy to normal levels; and Reform of the financial system to prevent a repeat depression.
The New Deal produced a political realignment, making the Democratic Party the majority (as well as the party that held the White House for seven out of nine Presidential terms from 1933 to 1969), with its base in liberal ideas, the white South, traditional Democrats, big city machines, and the newly empowered labor unions and ethnic minorities. The Republicans were split, with conservatives opposing the entire New Deal as an enemy of business and growth, and liberals accepting some of it and promising to make it more efficient. The realignment crystallized into the New Deal Coalition that dominated most presidential elections into the 1960s, while the opposition Conservative Coalition largely controlled Congress from 1937 to 1963. By 1936 the term "liberal" typically was used for supporters of the New Deal, and "conservative" for its opponents. From 1934 to 1938, Roosevelt was assisted in his endeavours by a "pro-spender" majority in Congress (drawn from two-party, competitive, non-machine, Progressive, and Left party districts). As noted by Alexander Hicks, "Roosevelt, backed by rare, non-Southern Democrat majorities—270 non-Southern Democrat representatives and 71 non-Southern Democrat senators—spelled Second New Deal reform."
Many historians distinguish between a "First New Deal" (1933–34) and a "Second New Deal" (1935–38), with the second one more liberal and more controversial. The "First New Deal" (1933–34) dealt with diverse groups, from banking and railroads to industry and farming, all of which demanded help for economic survival. The Federal Emergency Relief Administration, for instance, provided $500 million for relief operations by states and cities, while the short-lived CWA (Civil Works Administration) gave localities money to operate make-work projects in 1933-34.
The "Second New Deal" in 1935–38 included the Wagner Act to promote labor unions, the Works Progress Administration (WPA) relief program (which made the federal government by far the largest single employer in the nation), the Social Security Act, and new programs to aid tenant farmers and migrant workers. The final major items of New Deal legislation were the creation of the United States Housing Authority and Farm Security Administration, both in 1937, and the Fair Labor Standards Act of 1938, which set maximum hours and minimum wages for most categories of workers.
The economic downturn of 1937–38, and the bitter split between the AFL and CIO labor unions led to major Republican gains in Congress in 1938. Conservative Republicans and Democrats in Congress joined in the informal Conservative Coalition. By 1942–43 they shut down relief programs such as the WPA and CCC and blocked major liberal proposals. Roosevelt himself turned his attention to the war effort, and won reelection in 1940 and 1944. The Supreme Court declared the National Recovery Administration (NRA) and the first version of the Agricultural Adjustment Act (AAA) unconstitutional, however the AAA was rewritten and then upheld. As the first Republican president elected after FDR, Dwight D. Eisenhower (1953–61) left the New Deal largely intact, even expanding it in some areas. In the 1960s, Lyndon B. Johnson's Great Society used the New Deal as inspiration for a dramatic expansion of liberal programs, which Republican Richard M. Nixon generally retained. After 1974, however, the call for deregulation of the economy gained bipartisan support. The New Deal regulation of banking (Glass–Steagall Act) was suspended in the 1990s. Many New Deal programs remain active, with some still operating under the original names, including the Federal Deposit Insurance Corporation (FDIC), the Federal Crop Insurance Corporation (FCIC), the Federal Housing Administration (FHA), and the Tennessee Valley Authority (TVA). The largest programs still in existence today are the Social Security System and the Securities and Exchange Commission (SEC).
Origins.
Economic collapse (1929–1933).
From 1929 to 1933 manufacturing output decreased by one third. Prices fell by 20%, causing a deflation which made the repayments of debts much harder. Unemployment in the U.S. increased from 4% to 25%. Additionally, one-third of all employed persons were downgraded to working part-time on much smaller paychecks. In the aggregate, almost 50% of the nation's human work-power was going unused.
Before the New Deal, there was no insurance on deposits at banks. When thousands of banks closed depositors lost (on average) 15% of their savings. At that time there was no national safety net, no public unemployment insurance, and no Social Security. Relief for the poor was the responsibility of families, private charity, and local governments, but as conditions worsened year by year, demand skyrocketed and their combined resources increasingly fell far short of demand.
The depression had devastated the nation. As Roosevelt took the oath of office at noon on March 4, 1933, the state governors had closed every bank in the nation; no one could cash a check or get at their savings.
The unemployment rate was about 25% and higher in major industrial and mining centers. Farm income had fallen by over 50% since 1929. 844,000 nonfarm mortgages had been foreclosed, 1930–33, out of five million in all. Political and business leaders feared revolution and anarchy. Joseph P. Kennedy, Sr., who remained wealthy during the Depression, stated years later that "in those days I felt and said I would be willing to part with half of what I had if I could be sure of keeping, under law and order, the other half".
New Deal (1933–1938).
Upon accepting the 1932 Democratic nomination for president, Franklin Roosevelt promised "a new deal for the American people".
Roosevelt entered office without a specific set of plans for dealing with the Great Depression; so he improvised as Congress listened to a very wide variety of voices. Among Roosevelt's more famous advisers was an informal "Brain Trust": a group that tended to view pragmatic government intervention in the economy positively. His choice for Secretary of Labor, Frances Perkins, greatly influenced his initiatives. Her list of what her priorities would be if she took the job illustrates: "a forty-hour workweek, a minimum wage, worker's compensation, unemployment compensation, a federal law banning child labor, direct federal aid for unemployment relief, Social Security, a revitalized public employment service and health insurance."
The New Deal policies drew from many different ideas proposed earlier in the 20th century. Assistant Attorney General Thurman Arnold led efforts that hearkened back to an anti-monopoly tradition rooted in American politics by figures such as Andrew Jackson and Thomas Jefferson. Supreme Court Justice Louis Brandeis, an influential adviser to many New Dealers, argued that "bigness" (referring, presumably, to corporations) was a negative economic force, producing waste and inefficiency. However, the anti-monopoly group never had a major impact on New Deal policy. Other leaders such as Hugh Johnson of the NRA took ideas from the Woodrow Wilson Administration, advocating techniques used to mobilize the economy for World War I. They brought ideas and experience from the government controls and spending of 1917–18. Other New Deal planners revived experiments suggested in the 1920s, such as the TVA.
The "First New Deal" (1933–34) encompassed the proposals offered by a wide spectrum of groups. (Not included was the Socialist Party, whose influence was all but destroyed.) This first phase of the New Deal was also characterized by fiscal conservatism (see Economy Act, below) and experimentation with several different, sometimes contradictory, cures for economic ills. The consequences were uneven. Some programs, especially the National Recovery Administration (NRA) and the silver program, have been widely seen as failures. Other programs lasted about a decade; some became permanent. The economy shot upward, with FDR's first term marking one of the fastest periods of GDP growth in history. Though a downturn in 1937–38 raised questions about just how successful the policies were, the great majority of economists and historians agree that they were an overall benefit.
The New Deal faced some vocal conservative opposition. The first organized opposition in 1934 came from the American Liberty League led by conservative Democrats such as 1924 and 1928 presidential candidates John W. Davis and Al Smith. There was also a large but loosely affiliated group of New Deal opponents, who are commonly called the Old Right. This group included politicians, intellectuals, writers, and newspaper editors of various philosophical persuasions including classical liberals and conservatives, both Democrats and Republicans.
The New Deal represented a significant shift in politics and domestic policy. It especially led to greatly increased federal regulation of the economy. It also marked the beginning of complex social programs and growing power of labor unions. The effects of the New Deal remain a source of controversy and debate among economists and historians.
World comparisons.
The Great Depression began in the United States and quickly spread worldwide. It had severe effects in countries both rich and poor. Personal income, consumption, industrial output, tax revenue, profits and prices dropped, while international trade plunged by more than 50%. Unemployment in the U.S. rose to 25%, and in some countries rose as high as 33%.
Cities all around the world were hit hard, especially those dependent on heavy industry. Construction was virtually halted in many countries. Farming and rural areas suffered as crop prices fell by approximately 60%. Facing plummeting demand with few alternate sources of jobs, areas dependent on primary sector industries such as grain farming, mining and logging, as well as construction, suffered the most.
Most economies started to recover by 1933-34, the negative economic impact often lasted until the beginning of World War II, when war industries stimulated recovery.
There is little agreement on what caused the Great Depression, and the topic has become highly politicized. At the time the great majority of economists around the world recommended the "orthodox" solution of cutting government spending and raising taxes. However, British economist John Maynard Keynes advocated large-scale government deficit spending to make up for the failure of private investment. No major nation adopted his policies in the 1930s.
First New Deal (1933–1934).
The First Hundred Days (1933).
The American people were generally extremely dissatisfied with the crumbling economy, mass unemployment, declining wages and profits and especially Hoover's policies such as the Smoot–Hawley Tariff Act and the Revenue Act of 1932. Roosevelt entered office with enormous political capital. Americans of all political persuasions were demanding immediate action, and Roosevelt responded with a remarkable series of new programs in the "first hundred days" of the administration, in which he met with Congress for 100 days. During those 100 days of lawmaking, Congress granted every request Roosevelt asked, and passed a few programs (such as the FDIC to insure bank accounts) that he opposed. Ever since, presidents have been judged against FDR for what they accomplished in their first 100 days. Walter Lippmann famously noted:
The economy had hit bottom in March 1933 and then started to expand. Economic indicators show the economy reached nadir in the first days of March, then began a steady, sharp upward recovery. Thus the Federal Reserve Index of Industrial Production sank to its lowest point of 52.8 in July 1932 (with 1935–39 = 100) and was practically unchanged at 54.3 in March 1933; however by July 1933, it reached 85.5, a dramatic rebound of 57% in four months. Recovery was steady and strong until 1937. Except for employment, the economy by 1937 surpassed the levels of the late 1920s. The Recession of 1937 was a temporary downturn. Private sector employment, especially in manufacturing, recovered to the level of the 1920s but failed to advance further until the war. Chart 2 shows the growth in employment without adjusting for population growth. The U.S. population was 124,840,471 in 1932 and 128,824,829 in 1937, an increase of 3,984,468. The ratio of these numbers, times the number of jobs in 1932, means there was a need for 938,000 more 1937 jobs to maintain the same employment level.
Fiscal policy.
The Economy Act, drafted by Budget Director Lewis Williams Douglas, was passed on March 14, 1933. The act proposed to balance the "regular" (non-emergency) federal budget by cutting the salaries of government employees and cutting pensions to veterans by fifteen percent. It saved $500 million per year and reassured deficit hawks, such as Douglas, that the new President was fiscally conservative. Roosevelt argued there were two budgets: the "regular" federal budget, which he balanced, and the "emergency budget", which was needed to defeat the depression; it was imbalanced on a temporary basis.
Roosevelt was initially in favor of balancing the budget, but he soon found himself running spending deficits in order to fund the numerous programs he created. Douglas, however, rejecting the distinction between a regular and emergency budget, resigned in 1934 and became an outspoken critic of the New Deal. Roosevelt strenuously opposed the Bonus Bill that would give World War I veterans a cash bonus. Finally, Congress passed it over his veto in 1936, and the Treasury distributed $1.5 billion in cash as bonus welfare benefits to 4 million veterans just before the 1936 election.
New Dealers never accepted the Keynesian argument for government spending as a vehicle for recovery. Most economists of the era, along with Henry Morgenthau of the Treasury Department, rejected Keynesian solutions and favored balanced budgets.
Banking reform.
At the beginning of the Great Depression the economy was destabilized by bank failures followed by credit crunches. The initial reasons were substantial losses in investment banking, followed by bank runs. Bank runs occurred when a large number of customers withdraw their deposits because they believed the bank might become insolvent. As the bank run progressed, it generated a self-fulfilling prophecy: as more people withdraw their deposits, the likelihood of default increased, and this encouraged further withdrawals. It destabilized many banks to the point where they faced bankruptcy. Between 1929 and 1933 40% of all banks (9.490 out of 23.697 banks) went bankrupt. Much of the Great Depression's economic damage was caused directly by bank runs.
Herbert Hoover had already considered a "bank holiday" to prevent further bank runs, but rejected the idea because he was afraid to trip a panic. Roosevelt, however, gave a radio address, held in the atmosphere of a Fireside Chat, and explained to the public in simple terms the causes of the banking crisis, what the government will do and how the population could help. He closed all the banks in the country and kept them all closed until he could pass new legislation.
On March 9, Roosevelt sent to Congress the Emergency Banking Act, drafted in large part by Hoover's top advisors. The act was passed and signed into law the same day. It provided for a system of reopening sound banks under Treasury supervision, with federal loans available if needed. Three-quarters of the banks in the Federal Reserve System reopened within the next three days. Billions of dollars in hoarded currency and gold flowed back into them within a month, thus stabilizing the banking system. By the end of 1933, 4,004 small local banks were permanently closed and merged into larger banks. Their deposits totalled $3.6 billion; depositors lost a total of $540 million, and eventually received on average 85 cents on the dollar of their deposits; it is a common myth that they received nothing back. The Glass–Steagall Act limited commercial bank securities activities and affiliations between commercial banks and securities firms to regulate speculations. It also established the Federal Deposit Insurance Corporation (FDIC), which insured deposits for up to $2,500, ending the risk of runs on banks.
This banking reform offered unprecedented stability: While throughout the 1920s more than five hundred banks failed per year; it was less than ten banks per year after 1933.
Monetary reform.
Under the gold standard the United States kept the Dollar convertible to gold. If the gold reserves fell, the Federal Reserve System would be forced to reduce the money supply. At the end of the 1920s the United States was confronted with a bigger outflow of gold, thus in 1928 the Federal Reserve System began to raise its discount rate to stem the outflow of American gold. This deflationary policy was successful in containing the gold reserves but restricted economic activity. In the 1970s monetarists like Milton Friedman explored, that the rise of the discount rate in 1931 from 1.5% to 3.5% alone caused a 25% fall in industrial production.
In March and April in a series of laws and executive orders, the government suspended the gold standard. Roosevelt stopped the outflow of gold by forbidding the export of gold except under license from the treasury. Anyone holding significant amounts of gold coinage was mandated to exchange it for the existing fixed price of US dollars, after which the US would no longer pay gold on demand for the dollar, and gold would no longer be considered valid legal tender for debts in private and public contracts.
The dollar was allowed to float freely on foreign exchange markets with no guaranteed price in gold. With the passage of the Gold Reserve Act in 1934 the nominal price of gold was changed from $20.67 per troy ounce to $35. These measures enabled the Fed to increase the amount of money in circulation to the level the economy needed. Markets immediately responded well to the suspension, in the hope that the decline in prices would finally end. In her work "What ended the Great Depression?" (1992) Christina Romer argued that this policy raised industrial production by 25% until 1937 and by 50% until 1942.
Securities regulation.
Before the Wall Street Crash of 1929, there was no regulation of securities at the federal level. Even firms whose securities were publicly traded published no regular reports or even worse rather misleading reports based on arbitrarily selected data. To avoid another Wall Street Crash the Securities Act of 1933 was enacted. It required the disclosure of the balance sheet, profit and loss statement, the names and compensations of corporate officers, about firms whose securities were traded. Additionally those reports had to be verified by independent auditors. In 1934 the U.S. Securities and Exchange Commission was established to regulate the stock market and prevent corporate abuses relating to the sale of securities and corporate reporting.
Repeal of Prohibition.
In a measure that garnered substantial popular support for his New Deal, Roosevelt, on March 13, 1933, moved to put to rest one of the most divisive cultural issues of the 1920s. Just nine days later he signed the bill to legalize the manufacture and sale of alcohol, an interim measure pending the repeal of Prohibition, for which a constitutional amendment (the 21st) was already in process. The repeal amendment was ratified later in 1933. States and cities gained additional new revenue, and Roosevelt secured his popularity in the cities for supporting or permitting the legal production and sale of alcoholic beverages.
Relief.
Relief was the immediate effort to help the one-third of the population that was hardest hit by the depression.
Also, relief was aimed at providing temporary help to suffering and unemployed Americans.
Public works.
To prime the pump and cut unemployment, the NIRA created the Public Works Administration (PWA), a major program of public works, which organized and provided funds for the building of useful works such as government buildings, airports, hospitals, schools, roads, bridges, and dams. From 1933 to 1935 PWA spent $3.3 billion with private companies to build 34,599 projects, many of them quite large.
Under Roosevelt, many unemployed persons were put to work on a wide range of government financed public works projects, building bridges, airports, dams, post offices, courthouses, and thousands of miles of road. Through reforestation and flood control, they reclaimed millions of hectares of soil from erosion and devastation. As noted by one authority, Roosevelt's New Deal "was literally stamped on the American landscape".
Farm and rural programs.
Rural America was a high priority for Roosevelt and his energetic Secretary of Agriculture, Henry A. Wallace. FDR believed that full economic recovery depended upon the recovery of agriculture, and raising farm prices was a major tool, even though it meant higher food prices for the poor living in cities.
Many rural people lived in severe poverty, especially in the South. Major programs addressed to their needs included the Resettlement Administration (RA), the Rural Electrification Administration (REA), rural welfare projects sponsored by the WPA, National Youth Administration (NYA), Forest Service and Civilian Conservation Corps (CCC), including school lunches, building new schools, opening roads in remote areas, reforestation, and purchase of marginal lands to enlarge national forests. In 1933, the Administration launched the Tennessee Valley Authority, a project involving dam construction planning on an unprecedented scale in order to curb flooding, generate electricity, and modernize the very poor farms in the Tennessee Valley region of the Southern United States. Under the Farmers' Relief Act of 1933, the government paid compensation to farmers who reduced output, thereby rising prices. As a result of this legislation, the average income of farmers almost doubled by 1937.
In the 1920s farm production had increased dramatically thanks to mechanization, more potent insecticides and increased use of fertilizer. Due to an overproduction of agricultural products farmers faced a severe and chronic agricultural depression throughout the 1920s. The Depression even worsened the agricultural crises. At the beginning of 1933 agricultural markets nearly faced collapse. Farm prices were so low that for example in Montana wheat was rotting in the fields because it could not be profitably harvested. In Oregon sheep were slaughtered and left to the buzzards because meat prices were not sufficient to warrant transportation to markets.
Roosevelt was keenly interested in farm issues and believed that true prosperity would not return until farming was prosperous. Many different programs were directed at farmers. The first 100 days produced the Farm Security Act to raise farm incomes by raising the prices farmers received, which was achieved by reducing total farm output. The Agricultural Adjustment Act created the Agricultural Adjustment Administration (AAA) in May 1933. The act reflected the demands of leaders of major farm organizations, especially the Farm Bureau, and reflected debates among Roosevelt's farm advisers such as Secretary of Agriculture Henry A. Wallace, M.L. Wilson, Rexford Tugwell, and George Peek.
The aim of the AAA was to raise prices for commodities through artificial scarcity. The AAA used a system of "domestic allotments", setting total output of corn, cotton, dairy products, hogs, rice, tobacco, and wheat. The farmers themselves had a voice in the process of using government to benefit their incomes. The AAA paid land owners subsidies for leaving some of their land idle with funds provided by a new tax on food processing. To force up farm prices to the point of "parity" of growing cotton was plowed up, bountiful crops were left to rot, and six million piglets were killed and discarded.
The idea was to give farmers a "fair exchange value" for their products in relation to the general economy ("parity level"). Farm incomes and the income for the general population recovered fast since the Beginning of 1933. Still, food prices remained well below the 1929 peak. John T. Flynn stated that the department of Agriculture issued a bulletin telling the nation that the great problem of our time was "our failure to produce enough food to provide the people with a mere subsistence diet". In fact the problem of agricultural overproduction, especially food and cotton, remained until World War II, the AAA just downsized the level of overproduction.
The AAA established an important and long-lasting federal role in the planning on the entire agricultural sector of the economy and was the first program on such a scale on behalf of the troubled agricultural economy. The original AAA did not provide for any sharecroppers or tenants or farm laborers who might become unemployed, but there were other New Deal programs especially for them.
A Gallup Poll printed in the "Washington Post" revealed that a majority of the American public opposed the AAA. In 1936, the Supreme Court declared the AAA to be unconstitutional, stating that "a statutory plan to regulate and control agricultural production, a matter beyond the powers delegated to the federal government". The AAA was replaced by a similar program that did win Court approval. Instead of paying farmers for letting fields lie barren, this program instead subsidized them for planting soil enriching crops such as alfalfa that would not be sold on the market. Federal regulation of agricultural production has been modified many times since then, but together with large subsidies is still in effect in 2012.
The last major New Deal legislation concerning farming was in 1937, when the Farm Tenancy Act was created which in turn created the Farm Security Administration (FSA), replacing the Resettlement Administration.
A major new welfare program for the urban poor was the Food Stamp Plan was established in 1939 to provide stamps to poor people who could use them to purchase food at retail outlets. The program ended during wartime prosperity in 1943, but it was restored in 1961. It survived into the 21st century with little controversy because it was seen to benefit the urban poor, food producers, grocers and wholesalers, as well as farmers. Thus it gained support from both liberal and conservative Congressmen. However in 2013 Tea Party activists in the House tried to end the program, now known as the Supplemental Nutrition Assistance Program, while the Senate tried to preserve it.
Recovery.
Recovery was the effort in numerous programs to restore the economy to normal health. By most economic indicators this was achieved by 1937—except for unemployment, which remained stubbornly high until World War II began.
Recovery was designed to help the economy bounce back from depression.
NRA "Blue Eagle" campaign.
Roosevelt's advisers believed, that excessive competition and technical progress had led to overproduction and lowered wages and prices, which they believed lowered demand and employment (Deflation). He argued that government economic planning was necessary to remedy this:
...A mere builder of more industrial plants, a creator of more railroad systems, an organizer of more corporations, is as likely to be a danger as a help. Our task is not ... necessarily producing more goods. It is the soberer, less dramatic business of administering resources
and plants already in hand.
From 1929 to 1933, the industrial economy had been suffering from a vicious cycle of deflation. Since 1931, the U.S. Chamber of Commerce, the voice of the nation's organized business, promoted an anti-deflationary scheme that would permit trade associations to cooperate in government-instigated cartels to stabilize prices within their industries. While existing antitrust laws clearly forbade such practices, organized business found a receptive ear in the Roosevelt Administration.
New Deal economists argued that cut-throat competition had hurt many businesses and that with prices having fallen 20% and more, "deflation" exacerbated the burden of debt and would delay recovery. They rejected a strong move in Congress to limit the workweek to 30 hours. Instead their remedy, designed in cooperation with big business, was the NIRA. It included stimulus funds for the WPA to spend, and sought to raise prices, give more bargaining power for unions (so the workers could purchase more) and reduce harmful competition. At the center of the NIRA was the National Recovery Administration (NRA), headed by former General Hugh Johnson, who had been a senior economic official in World War I. Johnson called on every business establishment in the nation to accept a stopgap "blanket code": a minimum wage of between 20 and 45 cents per hour, a maximum workweek of 35–45 hours, and the abolition of child labor. Johnson and Roosevelt contended that the "blanket code" would raise consumer purchasing power and increase employment.
To mobilize political support for the NRA, Johnson launched the "NRA Blue Eagle" publicity campaign to boost what he called "industrial self-government". The NRA brought together leaders in each industry to design specific sets of codes for that industry; the most important provisions were anti-deflationary floors below which no company would lower prices or wages, and agreements on maintaining employment and production. In a remarkably short time, the NRA announced agreements from almost every major industry in the nation. By March 1934, industrial production was 45% higher than in March 1933. Donald Richberg, who soon replaced Johnson as the head of the NRA, said:
There is no choice presented to American business between intelligently planned and uncontrolled industrial operations and a return to the gold-plated anarchy that masqueraded as "rugged individualism" ... Unless industry is sufficiently socialized by its private owners and managers so that great essential industries are operated under public obligation appropriate to the public interest in them, the advance of political control over private industry is inevitable.
By the time NRA ended in May 1935, industrial production was 55% higher than in May 1933. In addition, well over 2 million employers accepted the new standards laid down by the NRA, which had introduced a minimum wage and an eight-hour workday, together with abolishing child labor. On May 27, 1935, the NRA was found to be unconstitutional by a unanimous decision of the U.S. Supreme Court in the case of "Schechter v. United States". On that same day, the Court unanimously struck down the Frazier-Lemke Act portion of the New Deal as unconstitutional. After the end of the NRA quotas in the oil industry were fixed by the Railroad Commission of Texas with Tom Connally's federal Hot Oil Act of 1935, which guaranteed that illegal "hot oil" would not be sold.
Employment in private sector factories recovered to the level of the late 1920s by 1937 but did not grow much bigger until the war came and manufacturing employment leaped from 11 million in 1940 to 18 million in 1943.
Housing Sector.
The New Deal had an important impact in the housing field. The New Deal followed and increased President Hoover's lead and seek measures. The New Deal sought to stimulate the private home building industry and increase the number of individuals who owned homes. The New Deal implemented two new housing agencies; Home Owners' Loan Corporation (HOLC) and the Federal Housing Administration (FHA). HOLC set uniform national appraisal methods and simplified the mortgage process. The Federal Housing Administration (FHA) created national standards for home construction.
The New Deal helped increase the number of Americans who owned homes. Before the New Deal only four out of 10 Americans owned homes; this was because the standard mortgage lasted only five to 10 years and had interest as high as 8%. These conditions severely limited the accessibility to housing for most Americans. Under the New Deal, Americans had access to 30-year mortgages, the standardized appraisal and construction standards helped open up the housing market to more Americans.
Reform.
Reform was based on the assumption that the depression was caused by the inherent instability of the market and that government intervention was necessary to rationalize and stabilize the economy, and to balance the interests of farmers, business and labor.
Reforms targeted the causes of the depression and sought to prevent a crisis like it from happening again. In other words, financially rebuilding the U.S. while ensuring not to repeat history.
Trade liberalization.
There is consensus amongst economic historians that protectionist policies, culminating in the Smoot-Hawley Act of 1930 worsened the Depression. Franklin D. Roosevelt already spoke against the act while campaigning for president during 1932. In 1934 the Reciprocal Tariff Act was drafted by Cordell Hull. It gave the president power to negotiate bilateral, reciprocal trade agreements with other countries. The act enabled Roosevelt to liberalize American trade policy around the globe. It is widely credited with ushering in the era of liberal trade policy that persists to this day.
Puerto Rico.
A separate set of programs operated in Puerto Rico, headed by the Puerto Rico Reconstruction Administration. It promoted land reform and helped small farms; it set up farm cooperatives, promoted crop diversification, and helped local industry. The Puerto Rico Reconstruction Administration was directed by Juan Pablo Montoya Sr. from 1935 to 1937.
Second New Deal (1935–1938).
In the spring of 1935, responding to the setbacks in the Court, a new scepticism in Congress, and the growing popular clamour for more dramatic action, the Administration proposed or endorsed several important new initiatives. Historians refer to them as the "Second New Deal" and note that
it was more liberal and more controversial than the "First New Deal" of 1933–34.
Social Security Act.
Until 1935 there were just a dozen states that had old age insurance laws but these programs were woefully underfunded and therefore almost worthless. Just one state (Wisconsin) had an insurance program. The United States was the only modern industrial country, where people faced the Depression without any national system of social security. Even the work programs of the "First New Deal" were just meant as immediate relief, destined to run less than a decade.
The most important program of 1935, and perhaps the New Deal as a whole, was the Social Security Act, drafted by Frances Perkins. It established a permanent system of universal retirement pensions (Social Security), unemployment insurance, and welfare benefits for the handicapped and needy children in families without father present. It established the framework for the U.S. welfare system. Roosevelt insisted that it should be funded by payroll taxes rather than from the general fund; he said, "We put those payroll contributions there so as to give the contributors a legal, moral, and political right to collect their pensions and unemployment benefits. With those taxes in there, no damn politician can ever scrap my social security program."
Compared with the social security systems in western European countries, the Social Security Act of 1935 was rather conservative. But for the first time the federal government took responsibility for the economic security of the aged, the temporarily unemployed, dependent children and the handicapped.
Labor relations.
The National Labor Relations Act of 1935, also known as the Wagner Act, finally guaranteed workers the rights to collective bargaining through unions of their own choice. The Act also established the National Labor Relations Board (NLRB) to facilitate wage agreements and to suppress the repeated labor disturbances. The Wagner Act did not compel employers to reach agreement with their employees, but it opened possibilities for American labor. The result was a tremendous growth of membership in the labor unions, especially in the mass-production sector, composing the American Federation of Labor. Labor thus became a major component of the New Deal political coalition.
The Fair Labor Standards Act of 1938 set maximum hours (44 per week) and minimum wages (25 cents per hour) for most categories of workers. Child labour of children under the age of 16 was forbidden, children under 18 years were forbidden to work in hazardous employment. As a result the wages of 300,000 people were increased and the hours of 1.3 million were reduced. It was the last major New Deal legislation that Roosevelt succeeded in enacting into law before the Conservative Coalition of Republicans and conservative Democrats won control of Congress that year. While he could usually use the veto to restrain Congress, it could block any Roosevelt legislation it disliked.
Works Progress Administration.
Roosevelt nationalized unemployment relief through the Works Progress Administration (WPA), headed by close friend Harry Hopkins. Roosevelt had insisted that the projects had to be costly in terms of labor, long-term beneficial, and the WPA was forbidden to compete with private enterprises (therefore the workers had to be paid smaller wages). The Works Progress Administration (WPA) was created to return the unemployed to the work force. The WPA financed a variety of projects such as hospitals, schools, and roads, and employed more than 8.5 million workers who built 650,000 miles of highways and roads, 125,000 public buildings, as well as bridges, reservoirs, irrigation systems, parks, playgrounds and so on.
Prominent projects were the Lincoln Tunnel, the Triborough Bridge, the LaGuardia Airport, the Overseas Highway and the San Francisco – Oakland Bay Bridge. The Rural Electrification Administration used co-ops to bring electricity to rural areas, many of which still operate. The National Youth Administration was another the semi-autonomous WPA program for youth. Its Texas director, Lyndon Baines Johnson, later used the NYA as a model for some of his Great Society programs in the 1960s. The WPA was organized by states, but New York City had its own branch Federal One, which created jobs for writers, musicians, artists, and theater personnel. It became a hunting ground for conservatives searching for Communist employees.
The Federal Writers' Project operated in every state, where it created a famous guide book; it also catalogued local archives and hired many writers, including Margaret Walker, Zora Neale Hurston, and Anzia Yezierska, to document folklore. Other writers interviewed elderly ex-slaves and recorded their stories. Under the Federal Theater Project, headed by charismatic Hallie Flanagan, actresses and actors, technicians, writers, and directors put on stage productions. The tickets were inexpensive or sometimes free, making theater available to audiences unaccustomed to attending plays. One Federal Art Project paid 162 trained woman artists on relief to paint murals or create statues for newly built post offices and courthouses. Many of these works of art can still be seen in public buildings around the country, along with murals sponsored by the Treasury Relief Art Project of the Treasury Department. During its existence, the Federal Theatre Project provided jobs for circus people, musicians, actors, artists, and playwrights, together with increasing public appreciation of the arts.
Tax policy.
In 1935, Roosevelt called for a tax program called the "Wealth Tax Act" (Revenue Act of 1935) to redistribute wealth. But there was more rhetoric than revenue in that proposal. The bill imposed an income tax of 79% on incomes over $5 million. Since that was an extraordinary high income in the 1930s, the highest tax rate actually covered just one individual – John D. Rockefeller. The bill was expected to raise only about $250 million in additional funds, so revenue was not the primary goal. Morgenthau called it "more or less a campaign document". In a private conversation with Raymond Moley, Roosevelt admitted that the purpose of the bill was "stealing Huey Long's thunder" by making Long's supporters his own. At the same time, it raised the bitterness of the rich who called Roosevelt "a traitor to his class" and the wealth tax act a "soak the rich tax".
A tax called the undistributed profits tax was enacted in 1936. This time the primary purpose was revenue, since Congress had enacted the Adjusted Compensation Payment Act, calling for payments of $2 billion to World War I veterans. The bill established the persisting principle that retained corporate earnings could be taxed. Paid dividends were tax deductible by corporations. The bill was designed to replace all other corporation taxes. The purpose was to stimulate corporations to distribute earnings and thus put more cash and spending power in the hands of individuals. In the end, Congress watered down the bill, setting the tax rates at 7 to 27% and largely exempting small enterprises. Facing widespread and fierce criticism, the tax deduction of paid dividends was repealed in 1938.
Housing Act of 1937.
One of the last New Deal agencies was the United States Housing Authority, created in 1937 with some Republican support to abolish slums.
Court-packing plan and jurisprudential shift.
When Roosevelt took office a majority of the nine judges of the Supreme Court were appointed by Republican Party Presidents. Four especially conservative judges (nicknamed the Four Horsemen) often managed to convince the fifth judge Owen Roberts to strike down progressive legislation. Roosevelt increasingly saw the issue of the Supreme Court as one of unelected officials stifling the work of a democratically elected government. Early in the year 1937, he asked Congress to pass the Judiciary Reorganization Bill of 1937. That proposal would have given the president the power to appoint a new justice whenever an existing judge reached the age of 70 and failed to retire within six months. In that way Roosevelt hoped to preserve the New Deal legislation. But he had stirred up a hornet`s nest since many congressmen feared he might start to retire them at 70 next. Many congressmen considered the proposal unconstitutional. In the end the proposal failed.
In one sense, however, it succeeded: Justice Owen Roberts switched positions and began voting to uphold New Deal measures, effectively creating a liberal majority in "West Coast Hotel Co. v. Parrish" and "National Labor Relations Board v. Jones & Laughlin Steel Corporation", thus departing from the "Lochner v. New York" era and giving the government more power in questions of economic policies. Journalists called this change "the switch in time that saved nine". Recent scholars have noted that since the vote in Parrish took place several months before the court-packing plan was announced, other factors, like evolving jurisprudence, must have contributed to the Court's swing. The opinions handed down in the spring of 1937, favorable to the government, also contributed to the downfall of the plan. In any case, the "court packing plan", as it was known, did lasting political damage to Roosevelt.
With the retirement of Justice Willis Van Devanter, the Court's composition began to move solidly in support of Roosevelt's legislative agenda. In the end Roosevelt had lost the battle for the Judiciary Reorganization Bill but won the war for control of the Supreme Court in a constitutional way. Since he managed to serve in office for more than twelve years he got the chance to appoint eight of the nine Justices of the Court. Former Supreme Court Chief Justice William Rehnquist noted that in this way the Constitution provides for ultimate responsibility of the Court to the political branches of government.
Recession of 1937 and recovery.
The Roosevelt Administration was under assault during FDR's second term, which presided over a new dip in the Great Depression in the fall of 1937 that continued until most of 1938. Production and profits declined sharply. Unemployment jumped from 14.3% in 1937 to 19.0% in 1938. The downturn was perhaps due to nothing more than the familiar rhythms of the business cycle. But until 1937 Roosevelt had claimed responsibility for the excellent economic performance. That backfired in the recession and the heated political atmosphere of 1937.
Business-oriented conservatives explained the recession by arguing that the New Deal had been very hostile to business expansion in 1935–37, had threatened massive anti-trust legal attacks on big corporations and by the huge strikes caused by the organizing activities of the Congress of Industrial Organizations (CIO) and the American Federation of Labor (AFL). The recovery was explained by the conservatives in terms of the diminishing of those threats sharply after 1938. For example, the antitrust efforts fizzled out without major cases. The CIO and AFL unions started battling each other more than corporations, and tax policy became more favorable to long-term growth.
"When The Gallup Organization's poll in 1939 asked, 'Do you think the attitude of the Roosevelt administration toward business is delaying business recovery?' the American people responded 'yes' by a margin of more than two-to-one. The business community felt even more strongly so." "Fortune"'s Roper poll found in May 1939 that 39% of Americans thought the administration had been delaying recovery by undermining business confidence, while 37% thought it had not. But it also found that opinions on the issue were highly polarized by economic status and occupation. In addition, AIPO found in the same time that 57% believed that business attitudes toward the administration were delaying recovery, while 26% thought they were not, emphasizing that fairly subtle differences in wording can evoke substantially different polling responses.
Keynesian economists stated that the recession of 1937 was a result of a premature effort to curb government spending and balance the budget.
Roosevelt had been cautious not to run large deficits. In 1937 he actually achieved a balanced budget. Therefore he did not fully utilize deficit spending. Between 1933 and 1941 the average federal budget deficit was 3% per year.
In November 1937 Roosevelt decided that big business were trying to ruin the New Deal by causing another depression that voters would react against by voting Republican. It was a "capital strike" said Roosevelt, and he ordered the Federal Bureau of Investigation to look for a criminal conspiracy (they found none). Roosevelt moved left and unleashed a rhetorical campaign against monopoly power, which was cast as the cause of the new crisis. Ickes attacked automaker Henry Ford, steelmaker Tom Girdler, and the super rich "Sixty Families" who supposedly comprised "the living center of the modern industrial oligarchy which dominates the United States".
Left unchecked, Ickes warned, they would create "big-business Fascist America—an enslaved America". The President appointed Robert Jackson as the aggressive new director of the antitrust division of the Justice Department, but this effort lost its effectiveness once World War II began and big business was urgently needed to produce war supplies. But the Administration's other response to the 1937 dip that stalled recovery from the Great Depression had more tangible results.
Ignoring the requests of the Treasury Department and responding to the urgings of the converts to Keynesian economics and others in his Administration, Roosevelt embarked on an antidote to the depression, reluctantly abandoning his efforts to balance the budget and launching a $5 billion spending program in the spring of 1938, an effort to increase mass purchasing power. Roosevelt explained his program in a fireside chat in which he told the American people that it was up to the government to "create an economic upturn" by making "additions to the purchasing power of the nation".
World War II and full employment.
The U.S. reached full employment after entering World War II in December 1941. Under the special circumstances of war mobilization, massive war spending doubled the GNP (Gross National Product). Military Keynesianism brought full employment. Federal contracts were cost-plus. Instead of competitive bidding to get lower prices, the government gave out contracts that promised to pay all the expenses plus a modest profit. Factories hired everyone they could find regardless of their lack of skills; they simplified work tasks and trained the workers, with the federal government paying all the costs. Millions of farmers left marginal operations, students quit school, and housewives joined the labor force.
The emphasis was for war supplies as soon as possible, regardless of cost and inefficiencies. Industry quickly absorbed the slack in the labor force, and the tables turned such that employers needed to actively and aggressively recruit workers. As the military grew, new labor sources were needed to replace the 12 million men serving in the military. Propaganda campaigns pleading for people to work in the war factories. The barriers for married women, the old, the unskilled—and (in the North and West) the barriers for racial minorities—were lowered.
In 1929, federal expenditures accounted for only 3% of GNP. Between 1933 and 1939, federal expenditure tripled, but the national debt as percent of GNP hardly changed. However, spending on the New Deal was far smaller than spending on the war effort, which passed 40% of GNP in 1944. The war economy grew so fast after deemphasizing free enterprise and imposing strict controls on prices and wages, as a result of government/business cooperation, with government subsidizing business, directly and indirectly.
Despite conservative domination of Congress during the early 1940s, a number of progressive measures supported by business in the name of efficiency and safety were legislated. The Coal Mines Inspection and Investigation Act of 1941 significantly reduced fatality rates in the coal-mining industry, while the Servicemen's Dependents Allowance Act of 1942 provided family allowances for dependents of enlisted men of the Army, Navy, Marine Corps, and the Coast Guard, while emergency grants to States were authorized that same year for programs for day care for children of working mothers. In 1944, pensions were authorized for all physically or mentally helpless children of deceased veterans regardless of the age of the child at the date the claim was filed or at the time of the veteran's death, provided the child was disabled at the age of sixteen and that the disability continued to the date of the claim. The Public Health Service Act, which was passed that same year, expanded Federal-State public health programs, and increased the annual amount for grants for public health services. In response to the March on Washington Movement led by A. Philip Randolph, Roosevelt promulgated Executive Order 8802 in June 1941, which established the President's Committee on Fair Employment Practices (FEPC) "to receive and investigate complaints of discrimination" so that "there shall be no discrimination in the employment of workers in defense industries or government because of race, creed, color, or national origin."
The New Dealers wanted benefits for everyone according to need. Conservatives, however, proposed benefits based on national service, and their approach won out. The "G.I. Bill" (Servicemen's Readjustment Act of 1944) was a landmark piece of legislation, providing 16 million returning veterans with benefits such as housing, educational, and unemployment assistance, and played a major role in the postwar expansion of the American middle class.
A major result of the full employment at high wages was a sharp, long lasting decrease in the level of income inequality (Great Compression). The gap between rich and poor narrowed dramatically in the area of nutrition, because food rationing and price controls provided a reasonably priced diet to everyone. White collar workers did not typically receive overtime and therefore the gap between white collar and blue collar income narrowed. Large families that had been poor during the 1930s had four or more wage earners, and these families shot to the top one-third income bracket. Overtime provided large paychecks in war industries, and average living standards rose steadily, with real wages rising by 44% in the four years of war, while the percentage of families with an annual income of less than $2,000 fell from 75% to 25% of the population.
In 1941, 40% of all American families lived on less than the $1,500 per year defined as necessary by the Works Progress Administration for a modest standard of living. The median income stood at $2,000 a year, while 8 million workers eared below the legal minimum. From 1939 to 1944, however, wages and salaries more than doubled, with overtime pay and the expansion of jobs leading to a 70% rise in average weekly earnings during the course of the war. Membership in organized labor increased by 50% between 1941 and 1945, and because the War Labor Board sought labor-management peace, new workers were encouraged to participate in the existing labor organizations, thereby receiving all the benefits of union membership such as improved working conditions, better fringe benefits, and higher wages. As noted by William H. Chafe
"with full employment, higher wages and social welfare benefits provided under government regulations, American workers experienced a level of well-being that, for many, had never occurred before."
As a result of the new prosperity, consumer expenditures rose by nearly 50%, from $61.7 billion at the start of the war to $98.5 billion by 1944. Individual savings accounts climbed almost sevenfold during the course of the war. The share of total income held by the top 5% of wage earners fell from 22% to 17%, while the bottom 40% increased their share of the economic pie. In addition, during the course of the war, the proportion of the American population earning less than $3,000 (in 1968 dollars) fell by half.
Legacy and historiography.
Analysts agree the New Deal produced a new political coalition that sustained the Democratic Party as the majority party in national politics for more than a generation after its own end.
However there is disagreement about whether it marked a permanent change in values. Cowie and Salvatore in 2008 argued that it was a response to depression and did not mark a commitment to a welfare state because America has always been too individualistic. MacLean rejected the idea of a definitive political culture. She says they overemphasized individualism and ignored the enormous power of big capital wields, the Constitutional restraints on radicalism, and the role of racism, antifeminism, and homophobia. She warns that accepting Cowie and Salvatore's argument that conservatism's ascendancy is inevitable would dismay and discourage activists on the left. Klein responds that the New Deal did not die a natural death; it was killed off in the 1970s by a business coalition mobilized by such groups as the Business Roundtable, the Chamber of Commerce, trade organizations, conservative think tanks, and decades of sustained legal and political attacks.
Historians generally agree that during Roosevelt's 12 years in office, there was a dramatic increase in the power of the federal government as a whole. Roosevelt also established the presidency as the prominent center of authority within the federal government. Roosevelt created a large array of agencies protecting various groups of citizens—workers, farmers, and others—who suffered from the crisis, and thus enabled them to challenge the powers of the corporations. In this way, the Roosevelt Administration generated a set of political ideas—known as New Deal liberalism—that remained a source of inspiration and controversy for decades. New Deal liberalism lay the foundation of a new consensus. Between 1940 and 1980 there was the liberal consensus about the prospects for the widespread distribution of prosperity within an expanding capitalist economy. Especially Harry S. Trumans Fair Deal and in the 1960s, Lyndon B. Johnson's Great Society used the New Deal as inspiration for a dramatic expansion of liberal programs.
While it is essentially consensus among historians and academics that the New Deal brought about a large increase in the power of the federal government, there has been some scholarly debate concerning the results of this federal expansion. Historians like Arthur M. Schlesinger and James T. Patterson have argued that the augmentation of the federal government exacerbated tensions between the federal and state governments. However, contemporaries such as Ira Katznelson have suggested that, due to certain conditions on the allocation of federal funds, namely that the individual states get to control them, the federal government managed to avoid any tension with states over their rights. 
This is a prominent debate concerning the historiography of federalism in the United States and, as Schlesinger and Patterson have observed, the New Deal marked an era when the federal-state power balance shifted further in favor of the federal government, which heightened tensions between the two levels of government in the United States.
Ira Katznelson has argued that although the federal government expanded its power and began providing welfare benefits on a scale previously unknown in the United States, it often allowed individual states to control the allocation of the funds provided for such welfare. This meant that the states controlled who had access to these funds, which in turn meant many southern states were able to racially segregate – or in some cases, like a number of counties in Georgia, completely exclude African-Americans – the allocation of federal funds. This enabled these states to continue to relatively exercise their rights and also to preserve the institutionalization of the racist order of their societies. While Katznelson has conceded that the expansion of the federal government had the potential to lead to federal-state tension, he has argued it was avoided as these states managed to retain some control. As Katznelson has observed, “furthermore, they governments in the South had to manage the strain that potentially might be placed on local practices by investing authority in federal bureaucracies… To guard against this outcome, they key mechanism deployed was a separation of the source of funding from decisions about how to spend the new monies.”
However, Schlesinger has disputed Katznelson’s claim and has argued that the increase in the power of the federal government was perceived to come at the cost of states’ rights, thereby aggravating state governments, which exacerbated federal-state tensions. Schlesinger has utilized quotes from the time to highlight this point, for example, Schlesinger has observed, “the actions of the New Deal, L. Mills said, “abolish the sovereignty of the States. They make of a government of limited powers one of unlimited authority over the lives of us all.”
Moreover, Schlesinger has argued that this federal-state tension was not a one-way street, and that the federal government became just as aggravated with the state governments, as they did with it. State governments were often guilty of inhibiting or delaying federal policies. Whether through intentional methods, like sabotage, or unintentional ones, like simple administrative overload; either way these problems aggravated the federal government and thus heightened federal-state tensions. As Schlesinger has also noted, “students of public administration have never taken sufficient account of the capacity of lower levels of government to sabotage or defy even a masterful President.”
James T. Patterson has reiterated this argument, however he observes that this increased tension can be accounted for not just from a political perspective, but from an economic one, too. Patterson has argued that the tension between the federal and state governments also, at least partly, resulted from the economic strain under which the states had been put by the federal government’s various policies and agencies. Some states were either simply unable to cope with the federal government’s demand, and thus refused to work with them, or admonished the economic restraints and actively decided to sabotage federal policies. This was demonstrated, Patterson has noted, with the handling of federal relief money by Ohio governor, Martin L. Davey. The case in Ohio became so detrimental to the federal government that Harry Hopkins, supervisor of the Federal Emergency Relief Administration, had to federalize Ohio relief. Although this argument differs somewhat from Schlesinger’s, the source of federal-state tension remained the growth of the federal government. As Patterson has asserted, “though the record of the FERA was remarkably good – almost revolutionary – in these respects, it was inevitable, given the financial requirements imposed on deficit-ridden states, that friction would develop between governors and federal officials.”
In this dispute it can be inferred that Katznelson and, Schlesinger and Patterson, have only disagreed on their inference of the historical evidence. While both parties have agreed that the federal government expanded and, even, that states had a degree of control over the allocation of federal funds, they have disputed the consequences of these claims. Katznelson has asserted that it created mutual acquiescence between the levels of government, while Schlesinger and Patterson have suggested that it provoked contempt for the state governments on the part of the federal government, and vice versa, thus exacerbating their relations. In short, irrespective of the interpretation this era marked an important time in the historiography of federalism and also nevertheless provided some narrative on the legacy of federal-state relations.
The New Deal's enduring appeal on voters fostered its acceptance by moderate and liberal Republicans.
As the first Republican president elected after FDR, Dwight D. Eisenhower (1953–61) built on the New Deal in a manner that embodied his thoughts on efficiency and cost-effectiveness. He sanctioned a major expansion of Social Security by a self-financed program. He supported such New Deal programs as the minimum wage and public housing; he greatly expanded federal aid to education and built the Interstate Highway system primarily as defense programs (rather than jobs program). In a private letter Eisenhower wrote:
In 1964 Barry Goldwater, an unreconstructed anti-New Dealer, was the Republican presidential candidate on a platform that attacked the New Deal. The Democrats under Lyndon B. Johnson won a massive landslide and Johnson's Great Society programs extended the New Deal. However the supporters of Goldwater formed the New Right which helped to bring Ronald Reagan into the White House in the 1980 presidential election. Reagan, at the time an ardent New Dealer, had turned against the New Deal and moved the nation in new directions, with his emphasis on government as the problem, not the solution.
The New Deal in Retrospect.
Race and Gender.
African Americans.
Although many Americans suffered economically during the Great Depression, African Americans also had to deal with social ills, such as racism, discrimination, and segregation.
Some leading white New Dealers, especially Eleanor Roosevelt, Harold Ickes, and Aubrey Williams worked to ensure blacks received at least 10% of welfare assistance payments. There was no attempt whatsoever to end segregation, or to increase black rights in the South. Roosevelt appointed an unprecedented number of blacks to second-level positions in his administration; these appointees were collectively called the Black Cabinet. Roosevelt and Hopkins worked with several big city mayors to encourage the transition of black political organizations from the Republican Party to the Democratic Party from 1934 to 1936, most notably in Chicago. The black community responded favorably, so that by 1936 the majority who voted (usually in the North) were voting Democratic. This was a sharp realignment from 1932, when most African Americans voted the Republican ticket. New Deal policies helped establish a political alliance between blacks and the Democratic Party that survives into the 21st century.
The WPA, NYA, and CCC relief programs allocated 10% of their budgets to blacks (who comprised about 10% of the total population, and 20% of the poor). They operated separate all-black units with the same pay and conditions as white units.
However, these benefits were small in comparison to the economic and political advantages that whites received. Most unions excluded blacks from joining. Enforcement of anti-discrimination laws in the South was virtually impossible, especially since most blacks worked in hospitality and agricultural sectors.
The Farm Service Agency (FSA), a government relief agency for tenant farmers, created in 1937, made efforts to empower African Americans by appointing them to agency committees in the South. Senator James F. Byrnes of South Carolina raised opposition to the appointments because he stood for white farmers who were threatened by an agency that could organize and empower tenant farmers.
Initially, the FSA stood behind their appointments, but after feeling national pressure FSA was forced to release the African Americans of their positions. The goals of the FSA were notoriously liberal and not cohesive with the southern voting elite.
The wartime FEPC executive orders that forbade job discrimination against African Americans, women, and ethnic groups was a major breakthrough that brought better jobs and pay to millions of minority Americans. Historians usually treat FEPC as part of the war effort and not part of the New Deal itself.
Women and the New Deal.
At first the New Deal created programs primarily for men. It was assumed that the husband was the "breadwinner" (the provider) and if they had jobs, whole families would benefit. It was the social norm for women to give up jobs when they married; in many states there were laws that prevented both husband and wife holding regular jobs with the government. So too in the relief world, it was rare for both husband and wife to have a relief job on FERA or the WPA. This prevailing social norm of the breadwinner failed to take into account the numerous households headed by women, but it soon became clear that the government needed to help women as well.
Many women were employed on FERA projects run by the states with federal funds. The first New Deal program to directly assist women was the Works Progress Administration (WPA), begun in 1935. It hired single women, widows, or women with disabled or absent husbands. While men were given unskilled manual labor jobs, usually on construction projects, women were assigned mostly to sewing projects. They made clothing and bedding to be given away to charities and hospitals. Women also were hired for the WPA's school lunch program.
Both men and women were hired for the arts programs (such as music, theater and writing). The Social Security program was designed to help retired workers and widows, but did not include domestic workers, farmers or farm laborers, the jobs most often held by blacks. Social Security however was not a relief program and it was not designed for short-term needs, as very few people received benefits before 1942.
Charges of radicalism.
Communists in government.
During the New Deal the Communists established a network of a dozen or so members working for the government. Harold Ware led the largest group which worked in the Agriculture Adjustment Administration (AAA). Secretary of Agriculture Wallace got rid of them all in a famous purge in 1935. Ware died in 1935 and some individuals such as Alger Hiss moved to other government jobs. Other Communists worked for the National Labor Relations Board, the National Youth Administration, the Works Progress Administration, the Federal Theater Project, the Treasury, and the Department of State.
The issue of Communists in government became a favorite conservative attacking point in the late 1930s. In 1938 Congressman Martin Dies, a Texas Democrat, and his newly created House Un-American Activities Committee investigated Communist subversion of labor unions and gained national headlines. In 1935-39, American Communist followed Stalin's "Popular front" approach and supported the New Deal. The Party's membership grew as it exercised greater influence and achieved new acceptance; it operated as a pressure group on the New Deal political coalition. The most important Party base in the Congress of Industrial Organizations (CIO), but by 1937 the CIO was spending much of its energy battling the older, more conservative American Federation of Labor (AFL). Klehr (1984) argues that the American Communist Party of the 1930s obediently followed directives from Moscow and suppressed individual initiative. In 1939 the Communists suddenly reversed course within days of the agreement between Hitler and Stalin in August that signaled friendship between the two bitter enemies. The Communists now denounced all enemies of Hitler and especially attacked President Roosevelt as a war-monger for his support for Britain in its war against Germany. Many members quit the Party in disgust.
Charges of fascism.
Worldwide, the Great Depression had the most profound impact in the German Reich and the United States. In both countries the pressure to reform and the perception of the economic crisis were strikingly similar. When Hitler came to power he was faced with exactly the same task that faced Roosevelt, overcoming mass unemployment and the global Depression. The political responses to the crises were essentially different: while American democracy remained strong, Germany replaced democracy with a Nazi dictatorship.
The initial perception of the New Deal was mixed. On the one hand the eyes of the world were upon America, because many democrats in Europe and the United States saw in Roosevelt´s reform program a positive counterweight to the seductive powers of the two great alternative systems, communism and fascism. As the historian Isaiah Berlin wrote in 1955, ″The only light in the darkness was the administration of Mr. Roosevelt and the New Deal in the United States.″
By contrast, enemies of the New Deal sometimes called it "fascist", but they meant very different things. Communists denounced the New Deal in 1933 and 1934 as fascist, meaning it was under the control of big business. They dropped that line of thought when Stalin switched to the "Popular Front" plan of cooperation with liberals. Libertarian Murray Rothbard described the NRA as fascist because it imposed "compulsory cartelization of American business." 
In 1934, Roosevelt defended himself against those critics in a "fireside chat". Some people, he said:
will try to give you new and strange names for what we are doing. Sometimes they will call it 'Fascism', sometimes 'Communism', sometimes 'Regimentation', sometimes 'Socialism'. But, in so doing, they are trying to make very complex and theoretical something that is really very simple and very practical... Plausible self-seekers and theoretical die-hards will tell you of the loss of individual liberty. Answer this question out of the facts of your own life. Have you lost any of your rights or liberty or constitutional freedom of action and choice?
After 1945 only few observers continued to see similarities. Later on some scholars such as Kiran Klaus Patel, Heinrich August Winkler and John Garraty came to the conclusion that comparisons of the alternative systems don´t have to end in an apology for Nazism since comparisons rely on the examination of both similarities and differences. Their preliminary studies on the origins of the fascist dictatorships and the American (reformed) democracy came to the conclusion that besides essential differences "the crises led to a limited degree of convergence" on the level of economic and social policy. The most important cause was the growth of state interventionism since in the face of the catastrophic economic situation both societies no longer counted on the power of the market to heal itself.
John Garraty wrote that the National Recovery Administration (NRA) was based on economic experiments in Nazi Germany and Fascist Italy, without establishing a totalitarian dictatorship. Contrary to that historians such as Hawley have examined the origins of the NRA in detail, showing the main inspiration came from Senators Hugo Black and Robert F. Wagner and from American business leaders such as the Chamber of Commerce. The model for the NRA was Woodrow Wilson's War Industries Board, in which Johnson had been involved too. Historians argue that direct comparisons between Fascism and New Deal are invalid since there is no distinctive form of fascist economic organization. Gerald Feldman wrote that fascism has not contributed anything to economic thought and had no original vision of a new economic order replacing capitalism. His argument correlates with Mason´s that economic factors alone are an insufficient approach to understand fascism and that decisions taken by fascists in power cannot be explained within a logical economic framework. In economic terms both ideas were within the general tendency of the 1930s to intervene in the free-market capitalist economy, at the price of its laissez-faire character, "to protect the capitalist structure endangered by endogenous crises tendencies and processes of impaired self-regulation".
Stanley Payne, a historian of fascism, examined possible fascist influences in the United States by looking at the KKK and its offshoots, and movements led by Father Coughlin and Huey Long. He concluded that "the various populist, nativist, and rightist movements in the United States during the 1920s and 1930s fell distinctly short of fascism." According to Kevin Passmore, lecturer in History at Cardiff University, the failure of fascism in the United States was due to the social policies of the New Deal that channelled anti-establishment populism into the left rather than the extreme right.
New Left critique.
For decades the New Deal was generally held in very high regard in the scholarship and the textbooks. That changed in the 1960s when New Left historians began a revisionist critique that said the New Deal was a bandaid for a patient that needed radical surgery to reform capitalism, put private property in its place, and lift up workers, women and minorities. The New Left believed in participatory democracy and therefore rejected the autocratic machine politics typical of the big city Democratic organizations.
In the 1960s, "New Left" historians have been among the New Deal's harsh critics. Barton J. Bernstein, in a 1968 essay, compiled a chronicle of missed opportunities and inadequate responses to problems. The New Deal may have saved capitalism from itself, Bernstein charged, but it had failed to help – and in many cases actually harmed – those groups most in need of assistance. Paul K. Conkin in "The New Deal" (1967) similarly chastised the government of the 1930s for its weak policies toward marginal farmers, for its failure to institute sufficiently progressive tax reform, and its excessive generosity toward select business interests. Howard Zinn, in 1966, criticized the New Deal for working actively to actually preserve the worst evils of capitalism.
By the 1970s liberal historians were responding with a defense of the New Deal based on numerous local and microscopic studies. Praise increasingly focused on Eleanor Roosevelt, seen as a more appropriate crusading reformer than her husband. Since then research on the New Deal has been less interested in the question of whether the New Deal was a "conservative", "liberal", or "revolutionary" phenomenon than in the question of constraints within which it was operating.
Political sociologist Theda Skocpol, in a series of articles, has emphasized the issue of "state capacity" as an often-crippling constraint. Ambitious reform ideas often failed, she argued, because of the absence of a government bureaucracy with significant strength and expertise to administer them. Other more recent works have stressed the political constraints that the New Deal encountered. Conservative skepticism about the efficacy of government was strong both in Congress and among many citizens. Thus some scholars have stressed that the New Deal was not just a product of its liberal backers, but also a product of the pressures of its conservative opponents.
Political metaphor.
Since 1933, politicians and pundits have often called for a "new deal" regarding an object. That is, they demand a completely new, large-scale approach to a project. As Arthur A. Ekirch Jr. (1971) has shown, the New Deal stimulated utopianism in American political and social thought on a wide range of issues. In Canada, Conservative Prime Minister Richard B. Bennett in 1935 proposed a "new deal" of regulation, taxation, and social insurance that was a copy of the American program; Bennett's proposals were not enacted, and he was defeated for reelection in October 1935. In accordance with the rise of the use of U.S. political phraseology in Britain, the Labour Government of Tony Blair has termed some of its employment programs "new deal", in contrast to the Conservative Party's promise of the 'British Dream'.
Evaluation of New Deal policies.
Many historians argue that Roosevelt restored hope and self-respect to tens of millions of desperate people, built labor unions, upgraded the national infrastructure and saved capitalism in his first term when he could have destroyed it and easily nationalized the banks and the railroads. Some critics from the left, however, have denounced Roosevelt for rescuing capitalism when the opportunity was at hand to nationalize banking, railroads and other industries. Still others have complained that he enlarged the powers of the federal government, built up labor unions and weakened the business community.
Historians generally agree that, apart from building up labor unions, the New Deal did not substantially alter the distribution of power within American capitalism. "The New Deal brought about limited change in the nation's power structure." The New Deal preserved democracy in the United States in an historic period of uncertainty and crises when in many other countries democracy failed.
Fiscal policy.
Julian Zelizer (2000) has argued that fiscal conservatism was a key component of the New Deal. A fiscally conservative approach was supported by Wall Street and local investors and most of the business community; mainstream academic economists believed in it, as apparently did the majority of the public. Conservative southern Democrats, who favored balanced budgets and opposed new taxes, controlled Congress and its major committees. Even liberal Democrats at the time regarded balanced budgets as essential to economic stability in the long run, although they were more willing to accept short-term deficits. As Zelizer notes, public opinion polls consistently showed public opposition to deficits and debt. Throughout his terms, Roosevelt recruited fiscal conservatives to serve in his Administration, most notably Lewis Douglas the Director of Budget in 1933–1934, and Henry Morgenthau Jr., Secretary of the Treasury from 1934 to 1945. They defined policy in terms of budgetary cost and tax burdens rather than needs, rights, obligations, or political benefits. Personally the President embraced their fiscal conservatism. Politically, he realized that fiscal conservatism enjoyed a strong wide base of support among voters, leading Democrats, and businessmen. On the other hand, there was enormous pressure to act – and spending money on high visibility work programs with millions of paychecks a week.
Douglas proved too inflexible, and he quit in 1934. Morgenthau made it his highest priority to stay close to Roosevelt, no matter what. Douglas's position, like many of the Old Right, was grounded in a basic distrust of politicians and the deeply ingrained fear that government spending always involved a degree of patronage and corruption that offended his Progressive sense of efficiency. The Economy Act of 1933, passed early in the Hundred Days, was Douglas's great achievement. It reduced federal expenditures by $500 million, to be achieved by reducing veterans' payments and federal salaries. Douglas cut government spending through executive orders that cut the military budget by $125 million, $75 million from the Post Office, $12 million from Commerce, $75 million from government salaries, and $100 million from staff layoffs. As Freidel concludes, "The economy program was not a minor aberration of the spring of 1933, or a hypocritical concession to delighted conservatives. Rather it was an integral part of Roosevelt's overall New Deal."
Revenues were so low that borrowing was necessary (only the richest 3% paid any income tax between 1926 and 1940). Douglas therefore hated the relief programs, which he said reduced business confidence, threatened the government's future credit, and had the "destructive psychological effects of making mendicants of self-respecting American citizens". Roosevelt was pulled toward greater spending by Hopkins and Ickes, and as the 1936 election approached he decided to gain votes by attacking big business.
Morgenthau shifted with FDR, but at all times tried to inject fiscal responsibility; he deeply believed in balanced budgets, stable currency, reduction of the national debt, and the need for more private investment. The Wagner Act met Morgenthau's requirement because it strengthened the party's political base and involved no new spending. In contrast to Douglas, Morgenthau accepted Roosevelt's double budget as legitimate – that is a balanced regular budget, and an "emergency" budget for agencies, like the WPA, PWA and CCC, that would be temporary until full recovery was at hand. He fought against the veterans' bonus until Congress finally overrode Roosevelt's veto and gave out $2.2 billion in 1936. His biggest success was the new Social Security program; he managed to reverse the proposals to fund it from general revenue and insisted it be funded by new taxes on employees. It was Morgenthau who insisted on excluding farm workers and domestic servants from Social Security because workers outside industry would not be paying their way.
Relief.
The New Deal expanded the role of the federal government, particularly to help the poor, the unemployed, youth, the elderly, and stranded rural communities. The Hoover administration started the system of funding state relief programs, whereby the states hired people on relief. With the CCC in 1933 and the WPA in 1935 the federal government now became involved in directly hiring people on relief. in granting direct relief or benefits. Total federal, state and local spending on relief rose from 3.9% of GNP in 1929, to 6.4% in 1932, and 9.7% in 1934; the return of prosperity in 1944 lowered the rate to 4.1%. In 1935-40, welfare spending accounted for 49% of the federal, state and local government budgets.
In his memoirs, Milton Friedman said that the New Deal relief programs were an appropriate response. He and his wife were not on relief but they were employed by the WPA as statisticians. Friedman said that programs like the CCC and WPA were justified as temporary responses to an emergency. Friedman said that Roosevelt deserved considerable credit for relieving immediate distress and restoring confidence.
Recovery.
Keynesian interpretation.
At the beginning of the Great Depression many economists traditionally argued against deficit spending that government spending would "crowd out" private investment and spending and thus not have any effect on the economy, a proposition known as the Treasury view. Keynesian economics rejected that view. They argued that by spending vastly more money—using fiscal policy—the government could provide the needed stimulus through the multiplier effect. Without that stimulus business simply would not hire more people, especially the low skilled and supposedly "untrainable" men who had been unemployed for years and lost any job skill they once had. Keynes visited the White House in 1934 to urge President Roosevelt to increase deficit spending. Roosevelt afterwards complained that, "he left a whole rigmarole of figures – he must be a mathematician rather than a political economist."
The New Deal tried public works, farm subsidies, and other devices to reduce unemployment, but Roosevelt never completely gave up trying to balance the budget. Between 1933 and 1941 the average federal budget deficit was 3% per year. Roosevelt did not fully utilize deficit spending. The effects of federal public works spending were largely offset by Herbert Hoovers large tax increase in 1932, whose full effects for the first time were felt in 1933, and it was undercut by spending cuts especially the economy act. According to Keynesians like Paul Krugman the New Deal therefore was not as successful in the short run as it was in the long run.
Monetarist interpretation.
In recent years more influential among economists has been the monetarist interpretation of Milton Friedman, which did include a full-scale monetary history of what he calls the "Great Contraction". Friedman concentrated on the failures before 1933. He pointed out that between 1929 and 1932, the Federal Reserve allowed the money supply to fall by a third which is seen as the major cause that turned a normal recession into a Great Depression. Friedman specially criticized the decisions of Hoover and the Fed not to save banks going bankrupt. Monetarists state that the banking and monetary reforms were a necessary and sufficient response to the crises. They reject the approach of Keynesian deficit spending.
Economic growth and unemployment (1933-1941).
In the years 1933 to 1941 the economy expanded at an average rate of 7.7% per year. Despite high economic growth rates unemployment fell slowly.
John Maynard Keynes explained that situation as an Underemployment equilibrium where skeptic business prospects prevent companies from hiring new employees. It was seen as a form of cyclical unemployment.
There are different assumptions as well. According to Richard L. Jensen cyclical unemployment was a grave matter primarily until 1935. Between 1935 und 1941 structural unemployment became the bigger problem. Especially the unions successes in demanding higher wages pushed management into introducing new efficiency-oriented hiring standards. It ended inefficient labor such as child labor, casual unskilled work for subminimum wages, and sweatshop conditions. In the long term the shift to efficiancy wages led to high productivity, high wages and a high standard of living. But it necessitated a well-educated, well-trained, hard-working labor force. It was not before war time brought full employment that the supply of unskilled labor (that caused structural unemployment) downsized.
Effect on the Depression.
Following the Keynesian consensus (that lasted until the 1970s) the traditional view was that federal fiscal policies associated with the war brought full-employment output while monetary policy was just aiding the process. Challenging the traditional view J. Bradford DeLong, Lawrence Summers and Christina Romer argue that recovery was essentially complete prior to 1942 and that monetary policy was the crucial source of pre-1942 recovery.
According to Peter Temin, Barry Wigmore, Gauti B. Eggertsson and Christina Romer the biggest primary impact of the New Deal on the economy and the key to recovery and to end the Great Depression was brought about by a successful management of public expectations. Before the first New Deal measures people expected a contractionary economic situation (recession, deflation) to persist. Roosevelt's fiscal and monetary policy regime change helped to make his policy objectives credible. Expectations changed towards an expansionary development (economic growth, inflation). The expectation of higher future income and higher future inflation stimulated demand and investments. The analysis suggests that the elimination of the policy dogmas of the gold standard, balanced budget and small government led endogenously to a large shift in expectation that accounts for about 70–80 percent of the recovery of output and prices from 1933 to 1937. If the regime change had not happened and the Hoover policy had continued, the economy would have continued its free fall in 1933, and output would have been 30 percent lower in 1937 than in 1933.
Harold L. Cole and Lee E. Ohanian are among those who believe the New Deal caused the Depression to persist longer than it would otherwise have, concluding in a study that the "New Deal labor and industrial policies did not lift the economy out of the Depression as President Roosevelt and his economic planners had hoped," but that the "New Deal policies are an important contributing factor to the persistence of the Great Depression." They claim that the New Deal "cartelization policies are a key factor behind the weak recovery". They say that the "abandonment of these policies coincided with the strong economic recovery of the 1940s". Cole and Ohanian claimed that FDR's policies prolonged the Depression by 7 years. However, Cole and Ohanian's argument relies on hypotheticals, including an unprecedented growth rate necessary to end the Depression by 1936, and by not counting workers employed through New Deal programs. Such programs built or renovated 2,500 hospitals, 45,000 schools, 13,000 parks and playgrounds, 7,800 bridges, of roads, 1,000 airfields and employed 50,000 teachers through programs that rebuilt the country's entire rural school system. Lowell E. Gallaway and Richard K. Vedder argue that the "Great Depression was very significantly prolonged in both its duration and its magnitude by the impact of New Deal programs." They suggest that without Social Security, work relief, unemployment insurance, mandatory minimum wages, and without special government-granted privileges for labor unions, business would have hired more workers and the unemployment rate during the New Deal years would have been 6.7% instead of 17.2%. In reply, economic historian Brad DeLong wrote that there is "literally nothing" to the arguments made by Gallaway and Vedder, and the duo made "flawed conclusions" based on "flawed foundations", and the entire foundation "is made out of mud".
In a survey of economic historians conducted by Robert Whaples, Professor of Economics at Wake Forest University, anonymous questionnaires were sent to members of the "Economic History Association". Members were asked to either "disagree", "agree", or "agree with provisos" with the statement that read: "Taken as a whole, government policies of the New Deal served to lengthen and deepen the Great Depression." While only 6% of economic historians who worked in the history department of their universities agreed with the statement, 27% of those that work in the economics department agreed. Almost an identical percent of the two groups (21% and 22%) agreed with the statement "with provisos" (a conditional stipulation), while 74% of those who worked in the history department, and 51% in the economic department disagreed with the statement outright.
Reform.
The economic reforms were mainly intended to rescue the capitalist system by providing a more rational framework in which it could operate. The banking system was made less vulnerable. The regulation of the stock market and the prevention of some corporate abuses relating to the sale of securities and corporate reporting addressed the worst excesses. Roosevelt allowed trade unions to take their place in labor relations and created the triangular partnership between employers, employees and government.
David M. Kennedy wrote that "the achievements of the New Deal years surely played a role in determining the degree and the duration of the postwar prosperity".
Paul Krugman stated that the institutions built by the New Deal remain the bedrock of the United States economic stability. Against the background of the 2007–2012 global financial crisis he explained that the financial crises would have been much worse if the New Deals Federal Deposit Insurance Corporation had not insured most bank deposits and older Americans would have felt much more insecure without Social Security. Libertarian economist Milton Friedman after 1960 attacked Social Security from a free market view stating that it had created welfare dependency.
The works of art and music.
The Works Progress Administration subsidized artists, musicians, painters and writers on relief with a group of projects called Federal One. While the WPA program was by far the most widespread, it was preceded by three programs administered by the US Treasury which hired commercial artists at usual commissions to add murals and sculptures to federal buildings. The first of these efforts was the short-lived Public Works of Art Project, organized by Edward Bruce, an American businessman and artist. Bruce also led the Treasury Department's Section of Painting and Sculpture (later renamed the Section of Fine Arts) and the Treasury Relief Art Project (TRAP). The Resettlement Administration (RA) and Farm Security Administration (FSA) had major photography programs. The New Deal arts programs emphasized regionalism, social realism, class conflict, proletarian interpretations, and audience participation. The unstoppable collective powers of common man, contrasted to the failure of individualism, was a favorite theme.
Post Office murals and other public art, painted by artists in this time, can still be found at many locations around the U.S. The New Deal particularly helped American novelists. For journalists, and the novelists who wrote non-fiction, the agencies and programs that the New Deal provided, allowed these writers to describe about what they really saw around the country.
Many writers chose to write about the New Deal, and whether they were for or against it, and if it was helping the country out. Some of these writers were Ruth McKenney, Edmund Wilson, and Scott Fitzgerald. Another subject that was very popular for novelists was the condition of labor. They ranged from subjects on social protest, to strikes.
Under the WPA, the Federal Theatre project flourished. Countless theatre productions around the country were staged. This allowed thousands of actors and directors to be employed, among them were Orson Welles, and John Huston.
The FSA photography project is most responsible for creating the image of the Depression in the U.S. Many of the images appeared in popular magazines. The photographers were under instruction from Washington as to what overall impression the New Deal wanted to give out. Director Roy Stryker's agenda focused on his faith in social engineering, the poor conditions among cotton tenant farmers, and the very poor conditions among migrant farm workers; above all he was committed to social reform through New Deal intervention in people's lives. Stryker demanded photographs that "related people to the land and vice versa" because these photographs reinforced the RA's position that poverty could be controlled by "changing land practices". Though Stryker did not dictate to his photographers how they should compose the shots, he did send them lists of desirable themes, such as "church", "court day", "barns".
Films of the late New Deal era such as "Citizen Kane" (1941) ridiculed so-called "great men", while the heroism of the common man appeared in numerous movies, such as "The Grapes of Wrath" (1940). Thus in Frank Capra's famous films, including "Mr. Smith Goes to Washington" (1939), "Meet John Doe" (1941) and "It's a Wonderful Life" (1946), the common people come together to battle and overcome villains who are corrupt politicians controlled by very rich, greedy capitalists.
By contrast there was also a smaller but influential stream of anti-New Deal art. Thus Gutzon Borglum's sculptures on Mount Rushmore emphasized great men in history (his designs had the approval of Calvin Coolidge). Gertrude Stein and Ernest Hemingway disliked the New Deal and celebrated the organic autonomy of perfected written work in opposition to the New Deal trope of writing as performative labor. The Southern Agrarians celebrated a premodern regionalism and opposed the TVA as a modernizing, disruptive force. Cass Gilbert, a conservative who believed architecture should reflect historic traditions and the established social order, designed the new Supreme Court building (1935). Its classical lines and small size contrasted sharply with the gargantuan modernistic federal buildings going up in the Washington Mall that he detested. Hollywood managed to synthesize liberal and conservative streams, as in Busby Berkeley's "Gold Digger" musicals, where the storylines exalt individual autonomy while the spectacular musical numbers show abstract populations of interchangeable dancers securely contained within patterns beyond their control.
New Deal Programs.
The New Deal had many programs and new agencies, most of which were universally known by their initials. Most were abolished during World War II; others remain in operation today. They included the following:
Statistics.
Depression statistics.
"Most indexes worsened until the summer of 1932, which may be called the low point of the depression economically and psychologically." Economic indicators show the American economy reached nadir in summer 1932 to February 1933, then began recovering until the recession of 1937–1938. Thus the Federal Reserve Industrial Production Index hit its low of 52.8 on 1932-07-01 and was practically unchanged at 54.3 on 1933-03-01; however by 1933-07-01, it reached 85.5 (with 1935–39 = 100, and for comparison 2005 = 1,342).
In Roosevelt's 12 years in office, the economy had an 8.5% compound annual growth of GDP, the highest growth rate in the history of any industrial country, however, recovery was slow; by 1939, Gross Domestic Product (GDP) per adult was still 27% below trend.
See also.
General:

</doc>
<doc id="19283367" url="http://en.wikipedia.org/wiki?curid=19283367" title="Kate Horan">
Kate Horan

Kate Horan (born 9 June 1975 in Wellington, New Zealand) is a New Zealand paralympics runner, who secured a silver medal for the Women's 200m at the 2008 Summer Paralympics for her country after two runners ahead of her tripped.

</doc>
<doc id="19283374" url="http://en.wikipedia.org/wiki?curid=19283374" title="Babicka">
Babicka

Babicka or Babička means "grandmother" in the Czech language. The word may refer to one of the following.

</doc>
<doc id="19283410" url="http://en.wikipedia.org/wiki?curid=19283410" title="WDRK">
WDRK

WDRK could refer to:

</doc>
<doc id="19283413" url="http://en.wikipedia.org/wiki?curid=19283413" title="Michael Bumpus">
Michael Bumpus

Michael Leron Bumpus (born December 13, 1985 in Honolulu, Hawaii) is an American football wide receiver who is currently a free agent. He was signed by the Seattle Seahawks as an undrafted free agent in 2008. He played college football at Washington State.
Early years.
Bumpus attended Culver City Middle School.
Bumpus played high school football at Culver City High School where he graduated in 2004. While playing for Culver City, he was named to the first-team "Best In The West" by the Long Beach Press Telegram.
Personal life.
Michael is the only child of Renee Bumpus. Raised on the Westside of Los Angeles in a small suburban community of Culver City California, where he excelled in 4 sports. Ms Bumpus kept her son involved in sports to give him something positive to do after school and to keep him busy while she worked. Michael grew to love sports, especially soccer. As a youngster he excelled in basketball and soccer. Michael traveled to Europe in 2001, where he competed against local soccer teams. Upon his return he joined the Culver City Centaurs football team (Michael was not allowed to play football until high school).
College career.
Bumpus played his college ball for the Washington State Cougars, where he put up impressive statistics of 195 receptions (a school record), 2,022 yards, and 8 touchdowns. In his freshman year, Bumpus was named to The Sporting News Pac-10 All Freshman Team.
Professional career.
Seattle Seahawks.
On September 21, 2008 against the St. Louis Rams, Bumpus scored his lone NFL touchdown on a 10-yard pass from Matt Hasselbeck. He was waived by the team on September 1, 2009 during the first round of cuts.
BC Lions.
On September 16, 2010, Bumpus was added to the BC Lions roster.
Spokane Shock.
Signed on December 10, 2010 to Spokane Shock of the Arena Football League.

</doc>
<doc id="19283420" url="http://en.wikipedia.org/wiki?curid=19283420" title="Boriana Stoyanova">
Boriana Stoyanova

Boriana Stoyanova (born November 3, 1969) is a retired Bulgarian artistic gymnast. She was the 1983 World Champion on the vault and represented Bulgaria at the 1984 Friendship Games (Oloumouc) and the 1988 Olympics in Seoul.

</doc>
<doc id="19283524" url="http://en.wikipedia.org/wiki?curid=19283524" title="Partner (2008 film)">
Partner (2008 film)

Partner ( "Force") (2008) is a Bengali film Directed by Shankar Ray.This film is more or less same as more than four decades ago in 1964 a film named “Jiban Kahini based on a story by Shaktipada Rajguru was a smashing hit. The main star cast of that film were Bikash Roy, Anup Kumar and Sandhya Roy. The director of that film was Rajen Tarafdar.
Plot.
Partner is all about a funny and dangerous contract between two haggards Dasu and Ayan (Jeet). The two met at the Sealdah North Railway Tracks while arriving for suicide at the same time. While Dasu (Santu Mukhopadhyay) realising his faults, Ayan (Jeet) remains bent on ending his life. Dasu is a rich man turned poor (after his only son's death) who is constantly truncated by debtors while Ayan is a frustrated rich brat who is devastated after crushing in his business (share and stocks) and being betrayed by his girlfriend Rina. Dasu convinces Ayan not to commit suicide at the heat of the moment when Ayan still insists to die Dasu sorts out a peculiar contract between themselves. He begs Ayan to die exactly after three months. This is because Dasu, who is a part-time insurance agent, wants Ayan to buy a life insurance policy which would mature after three months. Dasu, meanwhile would pay the premium of 10 thousand rupees by any means whatsoever and after three months when Ayan would commit suicide Dasu would get the entire value of the insurance policy (Rs. 10 lacs) as Ayan's only nominee. Ayan evaluates Dasu's proposal and thinks to make some penance for his sins before his death. Ayan starts living in Dasu's residence for the next three months. Meanwhile Ayan falls in love with Dasu's only daughter Priya (Swastika Mukherjee) and Ayan's father Ramen Roy appoints his brother-in-law Gobordhan Ghoshal (private investigator, played by Kharaj Mukherjee) to find out his missing son. After lots of fun, frolic and confusion Ayan is barred by Dasu from committing suicide. Dasu's debts are cleared by the hearty Romen Roy while Ayan marries lady love Priya.

</doc>
<doc id="19283549" url="http://en.wikipedia.org/wiki?curid=19283549" title="California Proposition 1A (2008)">
California Proposition 1A (2008)

Proposition 1A (or the Safe, Reliable High-Speed Passenger Train Bond Act for the 21st Century) is a law that was approved by California voters in the November, 2008 state elections. It was a ballot proposition and bond measure, that allocated funds for the California High-Speed Rail Authority. It now forms Chapter 20 of the "California Streets and Highways Code".
Background.
The proposition was put before voters by the state legislature. It was originally to appear on the 2004 state election ballot, but was delayed to the 2006 state election because of budgetary concerns raised by Governor Arnold Schwarzenegger. In January 2006, the Governor omitted the initial funds for the project from his $222.6 billion dollar Public Works Bond for the next 10 years. The Governor did include $14.3 million in the 2006-07 budget for the California High-Speed Rail Authority, enough for it to begin some preliminary engineering and detailed study. The proposition was delayed again from 2006 to 2008 to avoid competition with a large infrastructure bond, Proposition 1B, which passed in 2006.
The original proposition would have appeared in the 2008 general election as Proposition 1, but the state legislature enacted Assembly Bill 3034, which replaced that measure with an updated proposal called Proposition 1A. The updated measure included an additional funding requirement and oversight.
Provisions.
The law allocates $9.95 billion to the California High-Speed Rail Authority. Of that sum, $9 billion will be used to construct the core segments of the rail line from San Francisco to the Los Angeles area and the rest will be spent on improvements to local railroad systems that will connect locations away from the high-speed rail mainline to the high-speed system. The project also requires federal matching funds, since the $9.95 billion bond covers only part of the estimated cost of the initial core segment. The money will be raised through general obligation bonds that are paid off over a period of 30 years.
Controversy.
Soon after passage of the initiative in 2008, cost estimates almost tripled, ridership projections fell significantly, and the estimated time of travel along the length of the project doubled. By March 2013, according to a Public Policy Institute of California poll, only 43 percent of likely voters supported the project, a decline of 10 percent from when the measure passed in 2008.
Supporters.
The following people were listed in the official voter information guide as supporters:
Opponents.
The following people were listed in the official voter information guide as opponents:

</doc>
<doc id="19283584" url="http://en.wikipedia.org/wiki?curid=19283584" title="Yeddioymaq, Masally">
Yeddioymaq, Masally

Yeddioymaq, Masally may refer to:

</doc>
<doc id="19283589" url="http://en.wikipedia.org/wiki?curid=19283589" title="Urinal (health care)">
Urinal (health care)

A urinal is a bottle for urination. It is most frequently used in health care for patients who find it impossible or difficult to get out of bed. Urinals allow the patient who has cognition and movement of their arms to toilet independently.
Urinals can also be used for measuring the amount of urine produced by a patient on input & output (I & O), even if not used by the patient for toileting.
Generally, patients who are able to are encouraged to walk to the bathroom or use a bedside commode as opposed to a urinal. The prolonged use of a urinal has been shown to lead to constipation or trouble urinating.
Urinals are most frequently used for male patients, since they are easier to use with male anatomy. While female urinals exist, they are more difficult to use, and the common practice for females is to use a bedpan. Female urinals require a wider opening and must be placed between the legs. For many women, female urinals are more practical in a wheelchair rather than in a bed.

</doc>
<doc id="19283635" url="http://en.wikipedia.org/wiki?curid=19283635" title="1881 Atlantic hurricane season">
1881 Atlantic hurricane season

The 1881 Atlantic hurricane season ran through the summer and early fall of 1881. This is the period of each year when most tropical cyclones form in the Atlantic basin. In the 1881 Atlantic season there were three tropical storms and four hurricanes, none of which became major hurricanes (Category 3+). However, in the absence of modern satellite and other remote-sensing technologies, only storms that affected populated land areas or encountered ships at sea were recorded, so the actual total could be higher. An undercount bias of zero to six tropical cyclones per year between 1851 and 1885 and zero to four per year between 1886 and 1910 has been estimated. Of the known 1881 cyclones, Hurricane Three and Tropical Storm Seven were both first documented in 1996 by Jose Fernandez-Partagas and Henry Diaz. They also proposed changes to the known tracks of Hurricane Four and Hurricane Five.
__TOC__
Season Summary.
The Atlantic hurricane database (HURDAT) recognizes seven tropical cyclones for the 1881 season. In the 1881 Atlantic season there were three tropical storms, two Category 1 hurricanes and two Category 2 hurricanes. Five of these storms were active in August and two in September. No major hurricanes, Category 3 or greater, are known for this year. Tropical Storm One impacted Mississippi in the first week of August. Later that month, Tropical Storm Two hit Corpus Christi, Texas. At the same time in mid-August, Hurricane Three, a Category 1 hurricane was active in the tropical Atlantic without making a landfall anywhere. A tropical storm developed into Hurricane Four, a Category 1 hurricane, on August 19 north-east of the Bahamas. It became extratropical on August 21. Hurricane Five was the most destructive storm of 1881. It impacted the Georgia coast on August 27 as a Category 2 hurricane and was responsible for a large loss of life. Hurricane Six was a Category 2 hurricane that uprooted trees and demolished several buildings across North Carolina and Virginia in September. The last storm of the year was Tropical Storm Seven. It was active between September 18 and September 24, to the northwest of Bermuda, without making landfall.
Storms.
Tropical Storm One.
A tropical storm formed on August 1 to the northwest of the western tip of Cuba. It tracked northward, and hit Mississippi before dissipating on August 4. 
Tropical Storm Two.
A tropical storm hit Corpus Christi, Texas in the middle of August, but caused no reported deaths. Signals were blown down at the harbor, and one boat was lost.
Hurricane Three.
From August 11 to August 16, a Category 1 hurricane existed in the tropical Atlantic before turning northward and weakening. It continued northward as a tropical storm throughout August 17 and 18.
Hurricane Four.
A tropical storm developed on August 16 off the eastern coast of the Yucatán Peninsula. It tracked northeastward throughout its lifetime, passing through Cuba, the Florida Keys, and the Bahamas before becoming a hurricane on August 19. It weakened to a tropical storm on August 21 and became extratropical the same day.
Hurricane Five.
A tropical storm moved westward through the northeastern Lesser Antilles on August 22. It reached hurricane strength on August 24, and continued northwestward until making landfall between St. Simons Island and Savannah, Georgia on the 27th as a Category 2 hurricane. In Savannah, the lowest pressure recorded was at 9.20 PM that evening. A wind speed of 80 mph was recorded there before the anemometer was destroyed. Landfall coincided with high tide and proved very destructive. The hurricane moved inland, dissipating on August 29 over northwestern Mississippi. 335 people were reported as being killed in Savannah and the total death toll due to the hurricane was put at 700,
making the hurricane among the deadliest to strike the United States.
Hurricane Six.
A hurricane existed north of Hispanola on September 7. It moved northwestward, reaching a peak of 100 mph prior to hitting southern North Carolina. The hurricane centre moved northward across the Wilmington-Wrightsville Beach area and then close to Norfolk, Virginia, before turning out to sea. Trees were uprooted and buildings demolished at both Smithville (Southport) and Wilmington. An anemometer at Wilmington indicated wind speeds of 90 mph before it was destroyed. The hurricane weakened to a tropical storm over land, bringing heavy, yet beneficial, rain to Washington and other states. It moved out to sea, dissipating near Cape Cod.
Tropical Storm Seven.
A tropical storm was first seen on September 18 to the northwest of Bermuda. It tracked to the northeast, reaching a peak of 70 mph (113 km/h) on the 19th while southeast of the Canadian Maritimes. It weakened over the north Atlantic, becoming extratropical on the 22nd and finally dissipating by September 24.

</doc>
<doc id="19283639" url="http://en.wikipedia.org/wiki?curid=19283639" title="1882 Atlantic hurricane season">
1882 Atlantic hurricane season

The 1882 Atlantic hurricane season ran through the summer and early fall of 1882. This is the period of each year when most tropical cyclones form in the Atlantic basin. In the 1882 Atlantic season there were two tropical storms, two Category 1 hurricanes, and two major hurricanes (Category 3+). However, in the absence of modern satellite and other remote-sensing technologies, only storms that affected populated land areas or encountered ships at sea were recorded, so the actual total could be higher. An undercount bias of zero to six tropical cyclones per year between 1851 and 1885 and zero to four per year between 1886 and 1910 has been estimated. Of the known 1882 cyclones, Hurricane One and Hurricane Five were both first documented in 1996 by Jose Fernandez-Partagas and Henry Diaz, while Tropical Storm Three was first recognised in 1997. Partagas and Diaz also proposed large changes to the known track of Hurricane Two while further re-analysis, in 2000, led to the peak strengths of both Hurricane Two and Hurricane Six being increased. In 2011 the third storm of the year was downgraded from a hurricane to a tropical storm.
__TOC__
Season summary.
The Atlantic hurricane database (HURDAT) recognizes six tropical cyclones for the 1882 season. In the 1882 Atlantic season there were two tropical storms, two Category 1 hurricanes, and two major hurricanes. Hurricane One is known, from ship reports, to have been active in the north Atlantic on August 24 and 25. Early in September, Hurricane Two impacted Cuba, Florida, Georgia and both South and North Carolina. The storm caused flooding and damaged property but is not known to have caused any loss of life. Tropical Storm Three formed in the Gulf of Mexico and made landfall near the Texas/Louisiana border on September 15. Tropical Storm Four formed north of the Bahamas and caused extensive flooding from North Carolina to Massachusetts. It eventually dissipated near Long Island on September 23.A tropical storm developed into a hurricane on September 25 but Hurricane Five remained at sea and did not make landfall. As a Category 4 hurricane, Hurricane Six was the strongest storm of 1882. The storm hit Cuba at that intensity but quickly weakened over the island and hit Florida as a tropical storm. The storm caused some considerable damage in Florida before moving out to sea. It dissipated on October 15.
Storms.
Hurricane One.
Based on reports from two ships, the 'Case' and 'Ida', a hurricane was active on August 24 in the North Atlantic. Its prior track is unknown, but the storm continued to the north-northeast. It was last seen on the 25th to the southeast of Newfoundland.
Hurricane Two.
A tropical storm was first seen to the north of the Mona Passage on September 2. It moved to the west-northwest, reaching winds of 100 mph (160 km/h) before hitting Cuba. It crossed the island, and turned north in the Gulf of Mexico. The hurricane peaked at 125 mph (205 km/h) before hitting near Pensacola, Florida on September 10. It accelerated over the southeastern United States, crossing central Georgia, the western area of South Carolina and entered North Carolina on September 11. Continuing northward the storm moved offshore at Chesapeake Bay and after reaching the Atlantic Ocean, became extratropical near Nova Scotia. At Pensacola, the hurricane damaged crops,shipping and buildings. In Louisiana, half of the rice crop in Plaquemines Parish was destroyed by flooding. Flooding also occurred at Quarantine,Louisiana. It caused a landslide, and property damage throughout North Carolina but no deaths were reported.
Tropical Storm Three.
A tropical storm was first observed in the Gulf of Mexico on September 14. Its prior track is unknown, but it moved to the west-northwest, and hit land at the mouth of the Sabine River
near the Texas/Louisiana border on September 15. Port Eads, Louisiana recorded winds of 70 mph and a pressure of 29.38 inches. The storm brought a storm surge to Sabine Pass, causing moderate damage, and injured one person.
Tropical Storm Four.
A tropical storm formed north of the Bahamas on September 21. It moved north into North Carolina, landfalling near Cape Lookout. Near Wares Wharf on the Lower Rappahannock four mills were destroyed. Extensive flooding was reported from North Carolina to Massachusetts. In North Carolina bridges were swept away and railroads badly damaged. The storm moved over the mid-Atlantic coast, bringing heavy rain to Washington D.C. and eleven inches of rain to Philadelphia. The storm passed into Chesapeake Bay before moving out to sea on September 23. It dissipated on the 24th near Long Island.
Hurricane Five.
On September 24, a tropical storm was first seen off the coast of South Carolina. It moved to the northeast, and reached hurricane strength the next day. The hurricane turned to the east-northeast, and was last seen on September 28 to the southeast of Newfoundland.
Hurricane Six.
On October 5, a tropical storm formed in the western Caribbean Sea. It drifted northward, and as it approached the coast of Cuba, it rapidly intensified to a 140 mph (230 km/h) major hurricane. It weakened greatly over the island, never recovering while moving northward over the Gulf of Mexico. It made landfall on Florida as a tropical storm with maximum wind speeds of 44 mph at Jacksonville and 56 mph at Cedar Key
.The storm caused considerable damage in North Florida to telegraph lines, wharves and small boats. It crossed Florida and went out to sea, dissipating on October 15. Its remnants brought heavy rain to Labrador, and left 140 casualties in its path.

</doc>
<doc id="19283641" url="http://en.wikipedia.org/wiki?curid=19283641" title="Helen Zerefos">
Helen Zerefos

Helen Zerefos OAM, is an Australian coloratura soprano. Her career has spanned almost 50 years on stage, television, in nightclubs, concerts and recordings. 
Zerefos was born in Australia of Greek parents, Paul and Katina. Her career was launched in 1961, at a time when musical variety shows were popular on television. She was the first person of Greek heritage to be a regular artist on Australian television.
She was a regular member of the Revue 20 headed by Claire Poole. This was a twenty-piece choral ensemble which provided background choruses for Seven network variety programs such (Digby Wolfe's) Revue '61 and '62), (John Laws') Startime and Studio A hosted firstly by The LeGarde Twins and then by Colin Croft and Curtain Call, a late night variety program hosted by Revue 20 member John Wickham-Hall. She played Tup Tim in a professional tent production of The King and I in January 1966 at Warringah Mall in Brookvale on Sydney's Northern Beaches. She had also been engaged to play Maria in a later production of The Sound of Music (with June Bronhill who had played Maris in the original Australian tour of the musical, who was to play the Mother Abbess). The project was to scheduled to present six musicals presented, however only three (The King and I, Annie Get Your Gun and The Music Man) were and the project was abandoned for lack of both audience support and a dwindled capital base. The remaining musicals (Gypsy, Bye Bye Birdie and The Sound of Music) were never presented.
Zerefos was encouraged to pursue a career in America but her marriage in 1971 and strong family ties kept her in Australia.
Zerefos is also a tireless charity worker for the Ageing Research Centre at Sydney’s Prince of Wales Hospital. Her father died suddenly in 1981 and her mother never recovered from that shock and soon began to show signs of Alzheimer’s disease. Her mother needed constant attention, so Zerefos became her primary carer and she was soon fund-raising to assist research into this illness. Her husband, Raymond, died in 2002 after a lengthy debilitating illness.
Her extravagant costumary earned her the nickname Helen Fairy Floss. She also made a number of LP recordings.
Awards.
Zerefos won the 2006 Mo Award for Classical/Opera Performer.

</doc>
<doc id="19283659" url="http://en.wikipedia.org/wiki?curid=19283659" title="Golden Valley High School (Santa Clarita)">
Golden Valley High School (Santa Clarita)

Golden Valley High School is a high school in the William S. Hart Union High School District in the community of Canyon Country, California located in Santa Clarita, California. The school first opened in 2004 with a brand new campus. The student population is approximately 2500 in grades 9 through 12. The first class graduated in 2007.
Golden Valley High School has been the filming location for many new television shows, including "Heroes", "House", The Unit, and "".
Academics.
Golden Valley offers academic programs with emphasis on preparation for higher education. All preparatory courses required by the University of California and many other institutions of higher learning are offered. The school contains several different clubs and learning organizations established to aid preparation for college, such as the National Honors Society (3.5 GPA and above), (JOOI) Octagon Club, Speech and Debate, and Key Club. Two Honors Scholars graduated from the school in 2012. To be an honors scholar, one must maintain a 4.0 GPA or above by the end of senior year.
Mission statement.
Diversity is our strength, unity is our goal.
Theatre Arts.
Golden Valley High School has a magnificent theatre arts program. With their newly renovated theatre center, and under the direction of Janie Prucha, two plays are put on each school year. Generally a Comedy during the Fall semester, and a Musical during the Spring semester. The plays are generally very high budget, and are very popular among the student population of the district. You can view photos of theatre events that take place in the golden valley theatre at this website HW PHOTO/VIDEO
TV show.
Golden Valley High School runs a daily news show called GVTV (Golden Valley Television) which is aired on the school's closed-circuit-television channel: 16. The show is supervised by Mr. Charles Deuschle. Every other Friday, an alternative to GVTV was aired, until being cancelled in the 2011-2012 season for lack of student involvement and enjoyment. The segment was entitled "GVTV Take Two". Take Two was a pre-taped show produced by a separate team of students, who took a thought to be more humorous approach to school news. GVTV Take Two was reinstated by a new team of students during the 2012-2013 school year. It was then cancelled on the 30th of November due to offensive material. Supposedly, some of the teachers could not take a joke so they petitioned for the segment to be cancelled.
Vision statement.
Golden Valley High School is dedicated to advancing student achievement through assessment while students further develop personal interests and talents that prepare them for future endeavors. Within a safe and caring environment, Golden Valley is committed to developing quality relationships, promoting character traits, appreciating diversity, and fostering school and community pride. The school's motto is "Diversity Is Our Strength, Unity Is Our Goal".
Athletics.
Golden Valley started their athletics program when the school first opened in 2004. 
In 2011 the boys cross country team succeeded in achieving their first state championship in CIF Division II. They also placed 11th at the Nike Cross Nationals in Portland, Oregon. The boys track & field team proceeded to win back-to-back league championships during the 2012 and 2013 seasons and the girls won their first title in 2013.
In 2009 and 2010, the Boys Basketball team took the Foothill League title going 51-10 in those two seasons.
In 2013, the girls golf team took home their first ever Foothill League title.
Junior ROTC.
Golden Valley High School currently supports the ROTC program and is the center of participants from all schools in the Hart District area. It is for students who wish to be in the military as a part of their future career. The program is open to all who sign up. The school is home of the CA-20063rd AFJROTC unit, a Junior Reserve Officers' Training Corps. The CA-20063 uses military techniques to build a cadet run program where students from three schools in the district meet to develop leadership and physical fitness.

</doc>
<doc id="19283660" url="http://en.wikipedia.org/wiki?curid=19283660" title="2008 Palanca Awards">
2008 Palanca Awards

The Carlos Palanca Memorial Awards for Literature winners for the year 2008 (rank, name of author, title of winning entry (italicized, in parentheses)). The awarding ceremonies were held on September 1, 2008.
__NOTOC__
Filipino Division.
Dulang Pampelikula
Dulang Ganap Ang Haba
Dulaang May Isang Yugto
Sanaysay
Kabataan Sanaysay
Tula
Maikling Kuwentong Pambata
Maikling Kuwento
Nobela
Regional Division.
Maikling Kuwento - Cebuano
Maikling Kuwento - Hiligaynon
Maikling Kuwento - Iluko
English Division.
Full-Length Play
One-Act Play
Poetry
Short Story
Short Story For Children
Essay
Kabataan Essay
Novel

</doc>
<doc id="19283666" url="http://en.wikipedia.org/wiki?curid=19283666" title="Golden Valley High School">
Golden Valley High School

Golden Valley High School is the name of many schools named after agriculturally productive Golden Valley:
Schools with the full name:

</doc>
<doc id="19283670" url="http://en.wikipedia.org/wiki?curid=19283670" title="Marcus Holden">
Marcus Holden

Marcus Holden is an English-born Cypriot semi-professional rugby player who previously played his rugby in Wales for the North Wales region team RGC 1404. The team consists of all the best players in North Wales. As of now he plays his rugby in the Netherlands for Rugby Club Hilversum the current Dutch Champions. He also currently plays for Cyprus national rugby union team and is the leading points scorer. Holden initially started as a Winger but quickly moved to Fullback because of his kicking ability. He currently has 18 international caps and has started 16, scoring in all games he has started in. He has also trialled for Ospreys but failed to make a big impression over former coach Lyn Jones.
He is an ex footballer who played for Liverpool Academy. He has also played Cypriot division 1 football playing centre back for APEP Pitsilia.
His father is ex Port Vale midfielder Stephen Davies.

</doc>
<doc id="19283683" url="http://en.wikipedia.org/wiki?curid=19283683" title="Shibaji">
Shibaji

Shibaji ( "") (2008) is a Bengali film Directed by Babu Ray.
Plot.
The story of this film revolves around the main character Shivaji (Prosenjit) who is a goon. He was hired by the villain Bishal Sarkar to kill the judge Prasanta Mullick (Ranjit Mullick) as he refuses to receive the bribe and announces death sentence for his youngest brother, Vicky Sarkar. Shivaji's wife Durga (Swastika Mukherjee) on the other hand being insulted and humiliated again and again by people tries to commit suicide. Shivaji ultimately saves her life and leaves all criminal activities. Prasanta Mullick and his wife (Mousumi Saha) help him to start a business of a fast-food centre. But one day Bishal Sarkar's brother along with goons attack the restaurant and kills a person. Inspector Satyaprakash (Tapas Paul) arrests innocent Shivaji and he was imprisoned. Prasanta Mullick resigns from the post of a judge, and tries to prove Shivaji innocent. In this mean time the three brothers of Bishal Sarkar attacks Shivaji's house, kills his daughter Tumpa and rapes his wife. Durga commits suicide. To seek revenge Shivaji escapes from the police custody and starts killing Bishal Sarkar's brothers one by one. He also kidnaps Insp. Satyaprakash's daughter Jaya (Tathoi). Jaya was a lonely child as her father and mother (Satabdi Roy) none has time for her. She was brought up by her Appa (Chumki Choudhury). Shivaji starts loving her as his daughter, and Jaya also forgets her loneliness. But police separated them and produces Shivaji in court. Suddenly Bishal Sarkar kidnaps Jaya and Shivaji saves her. But while doing so he gets injured though kill all villains. Doctors declare him dead but Tathoi or Jaya magically saves his life by singing in front of God and after 20 years with the marriage of Jaya the movie ends.

</doc>
<doc id="19283703" url="http://en.wikipedia.org/wiki?curid=19283703" title="2008 unrest in Bolivia">
2008 unrest in Bolivia

The 2008 unrest in Bolivia began with protests against President Evo Morales and calls for greater autonomy for the country's eastern departments. Demonstrators escalated the protests by seizing natural gas infrastructure and government buildings. Violence between supporters of Morales and opponents resulted in at least 30 deaths.
Protests begin.
On August 19, the eastern departments of Santa Cruz, Beni, Pando, Tarija, and Chuquisaca called strikes and protests in opposition to central government plans to divert part of the national direct tax on hydrocarbons towards its "Renta Dignidad" pension plan. Brief clashes occurred in the Santa Cruz de la Sierra, capital of Santa Cruz, between police and armed youths enforcing the strike. In Tarija protesters seized and occupied government buildings. In response to the unrest Morales ordered the Bolivian Army to protect oil and gas infrastructure in the five departments.
The governors of the departments warned on September 3, 2008 that if the government didn't change its course that the protests could lead to a cut-off of natural gas exports to Argentina and Brazil. They also threatened setting up roadblocks in the five departments in addition to road blocks set up on roads leading to Argentina and Paraguay. The governors also demanded government troops withdraw from Trinidad, the capital of Beni department, following clashes between MPs and protesters trying to seize facilities of the National Tax Service in the city. President Morales accused the governors of launching a "civil coup" against his government. In Beni, militant rightwing civil groups threatened to take over military posts and expel the regional army chief if he did not agree to come under the command of the governor.
Violence escalates.
Protesters caused the explosion of a natural gas pipeline on September 10, 2008 according to the head of Bolivia's state energy company. He called the attack a "terrorist attack" and said it result in 10% cut in exports to Brazil. President Morales sent additional troops to the region following the attack. The next day clashes erupted between supporters and opponents of the government in the northeastern town of Cobija, capital of Pando department, resulting in 20 deaths. Morales said his government would be patient with the unrest but warned that "patience has its limits, really." A spokesman for Morales said the unrest was opening the way to "a sort of civil war."
The leader of the national opposition, Jorge Quiroga, accused the central government of organizing militias to retake the city of Cobija. Central government work had also ceased while American Airlines was suspending flights to the airport. Peasant supporters of Morales were also threatening to encircle Santa Cruz. Venezuelan President Hugo Chávez warned that if Morales was overthrown or killed Venezuela would give a "green light" to conduct military operations in Bolivia. Bolivia's army said it rejected "external intervention of any nature" in response to Chávez. Morales ruled out the use of force against protesters, calling for talks with opposition leaders.
The Governor of Tarija department, Mario Cossío, went to La Paz on September 12, 2008 to hold negotiations representing three other opposition governors who had rejected talks with the central government. Morales said he was open to dialogue not only with the governors but with mayors and different social sectors. Before the meeting Mario Cossio called for dialogue saying "The first task is to pacify the country, and we hope to agree with President Morales on that. Our presence has to do with that clear will to lay the foundations and hopefully launch a process of dialogue that ends in a great agreement for national reconciliation." Vice President Álvaro García declared a day of national mourning for 20 people killed in Pando most of whom were pro-Morales farmers shot dead by people the government claims were associated with the opposition.
Pando state of emergency.
Bolivian authorities declared a state of emergency in Pando which began at midnight on September 12, 2008. During the state of emergency, constitutional guarantees are suspended, private vehicles without authorization are banned from the streets, groups are not allowed to meet; bars, restaurants and discos must close at midnight, and residents are prohibited from carrying firearms. Morales said martial law was not needed anywhere else in the country.
Following the declaration of a state of emergency, Bolivian troops took control of the airport in Pando's capital, Cobija, and prepared to retake the city. Morales accused the governor of Pando of orchestrating "a massacre" of farmers supporting Morales. Pando Governor Leopoldo Fernández rejected the accusation, saying "They've accused me of using hit men, when everyone knows those socialist peasants, those fake peasants, were armed." In a speech in Cochabamba, Morales condemned the opposition governors, saying they were "conspiring against us with a fascist, racist coup," and said they were "the enemies of all Bolivians." While promising to adopt a constitution opposed by the governors Morales said Bolivia's "democratic revolution" had to be seen through saying "We have always cried 'fatherland or death'. If we don't emerge victorious, we have to die for the country and the Bolivian people." Morales also said he would not hesitate to extend the state of emergency to other opposition-controlled departments. Rubén Costas, the governor of Santa Cruz, belittled the chances of a breakthrough in talks adding that "if there is just one more death or person wounded, any possibility of dialogue will be broken."
Opposition protest leader and pro-autonomy businessman Branko Marinkovic announced on September 14, 2008 that the demonstrators he led would be removing their road blocks as "a sign of good will" to allow dialogue to prosper and calling on the government to end "repression and genocide in the department of Pando." Troops who had landed at Cobija also began patrolling the streets before dawn and began uncovering more dead bodies from the September 11 clash in Pando between Morales supporters and opposition protesters. Alfredo Rada, government minister for Pando, referring to casualty figures, said "We are nearing the 30 mark." An aide to the opposition governor in Pando denied the army was in control of the departmental capital. Troops were also hunting for Pando governor Fernández with orders to arrest him.
A spokesman for Morales said blockades remained on the highway and said "an armed group" had set fire to the town hall in Filadelfia, a municipality near Cobija. The Pando government spokesman said the citizens of Cobija did not want the Army to enter the city, and that they were not going to follow martial law.
Bolivia's army arrested as many as 10 people for alleged involvement in the deadly clashes. Leopoldo Fernández was also taken into custody by the armed forces on September 16, to be flown to La Paz to face accusations that he hired hitmen to fire on pro-government supporters. He was charged with committing genocide.
The U.S. began evacuating Peace Corps volunteers from Bolivia and organized at least two evacuation flights in response. In spite of the arrest, opposition governors agreed to talks, conditioned on anti-Morales protesters ending occupations of government buildings. Matters up for discussion include the opposition drive for more autonomy for their provinces and a larger share of state energy revenue. Talks were expected to begin on Thursday. The army also professed its support for Morales.
Morales appointed Navy Rear Admiral Landelino Bandeiras as the replacement for the governor of Pando September 20, 2008. Difficulties were reported in the peace talks by presidential spokesman Ivan Canelas, who said the position of the opposition governors could hinder peace talks and condemned the "lack of political will of these authorities to backup the efforts being made by the central government to preserve peace and national unity."
Supporters of Morales have threatened to storm the city of Santa Cruz if the talks should fail.
On 25 September 2008, Morales rejected autonomy proposals by the Eastern provinces, putting the talks on hold. On 20 October 2008, Morales and the opposition agreed to hold the referendum on 25 January 2009 and early elections in December 2009; Morales in turn promised he would not run again in 2014 after his likely reelection in 2009, despite being allowed to do so under the new constitution.
Diplomatic response.
Accusing the United States of supporting the opposition governors and attempting to overthrow his government, Morales declared the United States Ambassador to Bolivia Philip Goldberg persona non grata, and ordered him to leave the country. The U.S. responded by expelling Bolivia's ambassador in Washington. U.S. State Department spokesman Sean McCormack expressed regret at the diplomatic fallout saying it will "prejudice the interests of both countries, undermine the ongoing fight against drug trafficking and will have serious regional implications." President Morales said he does not want to break diplomatic ties with the U.S. but said the actions of the ambassador were "very serious", claiming he met with provincial leaders and instigated the unrest. Before his departure the American ambassador warned Bolivia that it would face "serious consequences" and had "not correctly evaluated" the retaliation from Washington.
Venezuela's President Hugo Chávez ordered the U.S. ambassador in Caracas to also leave saying it was in part out of solidarity with Bolivia. Chávez also said he was recalling Venezuela's ambassador to the U.S. until a new government takes office. Chávez accused the United States of being involved in the unrest saying "The U.S. is behind the plan against Bolivia, behind the terrorism." 
State Department spokesman Sean McCormack said the expulsions by Bolivia and Venezuela reflect "the weakness and desperation of these leaders as they face internal challenges." Morales responded that the act was "not of weakness, but of dignity," and was about freeing Bolivia from "the American Empire."

</doc>
<doc id="19283730" url="http://en.wikipedia.org/wiki?curid=19283730" title="Pharmacotoxicology">
Pharmacotoxicology

Pharmacotoxicology entails the study of the consequences of toxic exposure to pharmaceutical drugs and agents in the health care field. The field of pharmacotoxicology also involves the treatment and prevention of pharmaceutically induced side effects.
Information gleaned from pharmacotoxicology is used in forensics and determination of therapeutic dose adjustment

</doc>
<doc id="19283748" url="http://en.wikipedia.org/wiki?curid=19283748" title="F51">
F51

F51, F.51 or F-51 may refer to :
and also :

</doc>
<doc id="19283753" url="http://en.wikipedia.org/wiki?curid=19283753" title="Westpac">
Westpac

Westpac (a portmanteau of "Western-Pacific"), is an Australian bank and financial-services provider headquartered in Sydney. It is one of Australia's big four banks and is also the second-largest bank in New Zealand. 
As of November 2011, Westpac had 12.2 million customers, Australia's largest branch network with almost 1200 branches and a network with more than 2900 ATMs. The bank is Australia's second-largest bank by assets.
History.
Established in Sydney in 1817, the Bank of New South Wales (BNSW) was the first bank in Australia with Edward Smith Hall as its first cashier and secretary. During the 19th and early 20th century, the Bank opened branches first throughout Australia and Oceania, at Moreton Bay (Brisbane) in 1850, then in Victoria (1851), New Zealand (1861), South Australia (1877), Western Australia (1883), Fiji (1901), Papua New Guinea (1910) and Tasmania (1910).
Core business activities.
Westpac's core business consists of nine business units (five customer facing) through which it serves around 8.2 million customers. These business areas are:
Westpac Retail and Business Banking (RBB).
This includes deposit taking, transaction accounts, credit cards, mortgages, and other lending. Westpac is a major home loan provider and also serves the financial needs of business customers with a turnover of up to $20 million. Investment, superannuation and general and life insurance products are also sold through the branch network.
In the past, RBB operated as "Consumer Financial Services". The name was changed when all retail products were combined into the one division.
Westpac Institutional Bank (WIB).
Customers include governments, property, financial institutions, and medium to large corporates. In the Australian market, WIB holds a prominent position in providing banking services amongst the big four Australian banks and global investment banks, with an increasing presence in the Asia/Pacific region. Business units within WIB include Corporate and Institutional Banking, Global Transactional Services, Capital and Loan Markets, Financial Markets, Equities, Funds Management, International Operations, WIB Finance, WIB Risk, and WIB Technology.
Product and Operations (P&O).
P&O provides end-to-end banking solutions to small and medium enterprises across Australia.
BT Financial Group.
BT Financial Group, Westpac's wealth management division, includes managed investments, life insurance, superannuation and discount broking products in Australia and New Zealand. BT also offers custody and settlement services to institutional customers and fund managers.
Westpac (and BT) has been granted a MySuper authority, enabling it to continue to receive default superannuation contribution from 1 Jan 2014.
St.George Bank Group (including BankSA, Bank of Melbourne and RAMS).
St.George was founded in 1937 as a housing-based financial institution, becoming Australia's largest building society before achieving full banking status in July 1992. Westpac merged with St.George on 1 December 2008, but has pledged to keep the branch network for a minimum of 3 years and to maintain a corporate presence in the Sydney suburb of Kogarah.
BankSA is the largest financial institution in South Australia, and the State's main provider of housing, personal finance and rural banking services. It also provides funds management, life and general insurance, and superannuation and investment services. Westpac acquired BankSA in December 2008. It continues to be a division of St.George Bank.
Westpac acquired RAMS home loans in January 2008 and operates the RAMS distribution business and brands as part of the St.George group.
Westpac New Zealand.
In 1861 the Bank of New South Wales opened seven branches in New Zealand. Some of the old buildings still stand, including one in Oamaru and another in Tokomaru Bay. Today this unit offers a whole range of consumer and corporate services to clients throughout New Zealand. It is the dominant provider of banking services to small to medium business, corporate and institutional organisations, and is the banker of the New Zealand government. Currently Westpac is the second largest bank in New Zealand, after the merger of ANZ and National Bank of New Zealand, with around 1.5 million customers, 3,000 shareholders and 197 branches nationwide.
On 29 September 2006 the New Zealand Commerce Commission forced Westpac to pay NZ$5.1 million for hidden foreign transaction fees; most of the fine is reimbursement to affected customers, in the order of 12% of the fees actually charged. All other banks operating in New Zealand have either already been fined or are awaiting a court case.
In October 2009 Westpac Banking Corporation (New Zealand branch) was ordered to pay the Inland Revenue Department (New Zealand) NZ$961 million in avoided taxes.
Other business units.
Servicing the group, the three remaining business units are Group Finance, Human Resources and Group Risk. These are sometimes collectively referred to as the 'Corporate Core'.
ATM Alliance.
Westpac is a member of the Global ATM Alliance, a joint venture of several major international banks that allows customers of the banks to use their ATM card or check card at another bank within the Global ATM Alliance with no fees when traveling internationally. Other participating banks are Allied Irish Banks (Ireland), Barclays (in the United Kingdom, Spain and parts of Africa), Bank of America (United States), BNP Paribas (France), Ukrsibbank (Ukraine), Deutsche Bank (in Germany, Spain, Italy, Poland), and Scotiabank (in Canada, Chile, Mexico among many other countries).
Westpac Migrant Banking.
This unit of both the Australian and New Zealand Bank offers banking facilities to those migrating to either New Zealand or Australia. Bank accounts for migrants can be opened before people arrive in the country using their easy account opening process. Credit cards and mortgages can even be approved before arrival. Westpac Migrant Banking has a representative office in London where accounts can be arranged, although the process can be done remotely from any country. Westpac plans to open a retail branch in London in 2011.
Pacific Banking.
Westpac operates in seven south Pacific nations; the unit is headquartered in Sydney, Australia. The financial services offered include electronic banking (via online banking, ATMs and EFTPOS), deposit, loan, transaction accounts and international trade facilities to personal and business customers. Westpac Fiji is Westpac's Fijian operation. It is one of the largest banks in the country and has a 40% market share.
Controversy.
US Federal reserve borrowings.
In 2009, a Westpac owned entity secured US$1.09 billion from the US Federal Reserve. Commentary suggests this was an unusual move for the bank, given its relatively minor position in North America. The borrowings by Westpac occurred at the height of the Global Financial Crisis and was part of a Federal Reserve move to stabilise financial markets globally. The public and Government attention of the borrowings followed the release of the information by the Federal Reserve in 2011, not Westpac.
Corporate responsibility.
In 2002 released a Social Impact Report that outlined the bank's plan to meet the highest international standards in the area of corporate social responsibility. This led to Westpac being assessed as the global sustainability leader for the banking sector in the Dow Jones Sustainability Index from 2004–2007.
Westpac has been criticised for backing logging operations on the Solomon Islands that destroy virgin rainforests.
Because of this engagement, the Australian Greens have called for the Banksia Awards to be withdrawn from Westpac.

</doc>
<doc id="19283764" url="http://en.wikipedia.org/wiki?curid=19283764" title="Satsuki">
Satsuki

Possible writings.
Satsuki can be written using different kanji characters and can mean:
The given name can also be written in hiragana or katakana.
Notable people with the name include:
Given name:
Surname:

</doc>
<doc id="19283769" url="http://en.wikipedia.org/wiki?curid=19283769" title="Ausar">
Ausar

Ausar may refer to:

</doc>
<doc id="19283786" url="http://en.wikipedia.org/wiki?curid=19283786" title="Charles W. Morse">
Charles W. Morse

Charles Wyman Morse (October 21, 1856 – January 12, 1933) was a notorious businessman and speculator on Wall Street in the early 20th century.
Early life.
Morse was born in Bath, Maine, in 1856, the son of Benjamin Wyman and Anna Eliza Jane (Rodbird) Morse. His father had a large role in the towing business on the Kennebec River. Charles was already involved in the shipping business while a student at Bowdoin College, and at his graduation in 1877 he had accumulated a sizable capital. After college he went into business with his father and a cousin, Harry F. Morse, forming C.W. Morse & Company and engaging in an extensive business shipping ice and lumber.
On April 14, 1884, he married Hattie Bishop Hussey of Brooklyn, New York (granddaughter of Maine shipbuilder T. J. Southard). She bore him three sons and a daughter, and died about 1897.
As his business interests grew, Morse moved to Boston and, in 1897, New York City.
The "Ice Trust".
He organized the Consolidated Ice Company in 1897 and went into the ice business. In 1899 he merged it with several other companies to form the American Ice Company which, grossly overcapitalized at $60 million, held a virtual monopoly for ice in New York. Morse quickly became known as "The Ice King". At that time commercial ice was cut from frozen rivers, much of it in Morse's native state of Maine. 
On May 1, 1900, Morse attempted to use his monopoly to raise the price of ice. The plan backfired, however, and it was revealed by the "New York Journal and Advertiser" that Morse had obtained special privileges from Tammany Hall to run his business, and in exchange Robert Van Wyck (New York City's first mayor over the five united boroughs) had been given a substantial ownership share in the ice companies (by then known as the "Ice Trust") as had Richard Croker, the boss of Tammany Hall. Having formed a holding company called the Ice Securities Company, Morse manipulated its stock and left the ice business with a profit of some $12 million.
Shipping and banking.
On June 18, 1901 he married Clemence Dodge, a divorcee from Atlanta, at the Fifth Avenue Presbyterian Church in Manhattan. The Morses lived at 724 Fifth Avenue, before moving to Lakewood Township, New Jersey. They maintained a summer home in Bath, Maine. Their marriage was annulled, however, in 1904 when it was determined that Clemence's divorce from her first husband, Charles F. Dodge, was not legal and she was therefore still married to him. Undeterred, she was represented by Samuel Untermyer, who restored her marital rights; she remained devotedly at the side of Morse until her death in 1926.
Morse returned to the realm of shipping in 1901 when he established the Eastern Steamship Company as a consolidation of three existing lines. These were the Boston and Bangor Steamship Company, dating from 1834; the Portland Steam Packet Company, organized in 1843; and the International Steamship Company, established in 1859.
In 1902 Morse acquired control of both overnight steamboat lines on the Hudson River - the People's Line, established in 1835, and the Citizens' Line, established in 1872 - and organized the Hudson Navigation Company to operate them. They were collectively known as the Hudson River Night Line. The People's Line named its new 411-foot steamer "C.W. Morse" in his honor in 1904. (Morse's uncle James Thomas Morse, his father's brother, was the namesake of the Rockland-Bar Harbor, Maine, steamer "J.T. Morse", also built in 1904.)
Morse acquired control of the Metropolitan Steamship Company from the Whitney interests in 1906. He organized the Consolidated Steamship Company in January 1907 as a holding company for the Eastern Steamship Company, Metropolitan Steamship Company, Clyde Steamship Company and Mallory Steamship Company. Despite an initial announcement of such a sale, Morse failed in an attempt to purchase the Long Island Sound steamers of the New York, New Haven and Hartford Railroad. He did, however, acquire control of the New York and Cuba Mail Steamship Company and the New York and Porto Rico Steamship Company in 1907.
He parlayed this success into a prominent role in high finance in New York City. Morse controlled the National Bank of North America, the New Amsterdam National Bank and was a large owner of the Mercantile National Bank. He became a close associate of F. Augustus Heinze, who became president of Mercantile National, and E.R. Thomas, a young man of large inherited fortune. Their influence grew—Heinze and Morse served as directors together on at least six national banks, 10 state banks, five trust companies and four insurance companies.
Panic of 1907.
Along with Augustus Heinze's brothers, Morse helped create a pool of money to drive up and corner the stock of United Copper. On October 15, 1907 this corner failed so spectacularly that depositors with Morse's banks began to pull out their deposits. On October 20, the New York Clearing House, which had a critical role clearing checks between banks, forced Morse to resign from his banking interests. This did not stop the panic, however, which went on to topple the Knickerbocker Trust Company, New York's third largest trust, and led to financial turmoil across the country through November. The Morse-controlled steamship lines went into receivership, for varying periods, in February 1908.
Indicted by United States District Attorney Henry L. Stimson, Morse was convicted of violations of federal banking laws. He was sentenced to 15 years in the Atlanta federal penitentiary in November 1908 but remained free on appeal.
On October 8, 1909, the assets of the Metropolitan Steamship Company were sold at foreclosure sale to John W. McKinnon of Chicago. The company was reincorporated three days later in Maine with Morse as president. The Metropolitan Steamship Company and Maine Steamship Company were consolidated with the Eastern Steamship Company in 1911 to form Eastern Steamship Corporation. This concern went into receivership in 1914 and emerged in 1917 as Eastern Steamship Lines.
Having exhausted his legal appeals, Morse departed for Atlanta penitentiary on January 2, 1910. In Atlanta he was a prisoner alongside Charles Ponzi, who would go on to create an eponymous fraudulent financial scheme, the Ponzi scheme, and earn a legacy as one of the most famous swindlers in American history. Because of Morse's wealth and connections, he launched a campaign of lawyers, lobbyists and famous journalists like Clarence W. Barron who urged President William Howard Taft for leniency. In 1912 Morse became ill, and a panel of Army doctors declared that he suffered from Bright's disease and other maladies and would soon die if he remained in prison. Taft signed his pardon, and Morse departed for medical treatment at Wiesbaden. However, it soon became known to the Justice Department that he had feigned illness by drinking a combination of soapsuds and chemicals. Taft later said that the case "shakes one's faith in expert examination."
Later life and death.
On his return from Europe, Morse returned to the shipping business. He still controlled the Hudson Navigation Company, which had not been involved in the crash of the Consolidated Steamship Company in 1907. Morse announced on January 11, 1916, plans for a new transoceanic steamship line, which he organized as the United States Shipping Company. This holding company exchanged its stock for that of 16 subsidiary companies, each organized around a steamship.
During World War I he was president of United States Steamship Company, which was the parent company of Groton Iron Works and Virginia Shipbuilding Corporation. The Virginia Shipbuilding Company won contracts to build 36 vessels for the war effort. The freighters were ordered by the United States Shipping Board, and Morse borrowed from the Emergency Fleet Corporation funds to carry out the contracts. Ultimately, 22 of the ships were completed; the other 14 were cancelled.
Morse controlled the Hudson Navigation Company until its bankruptcy in 1921. The receivers quickly changed the name of the "C.W. Morse" to "Fort Orange".
In 1922 Morse was accused of misrepresentation of his facilities for ship construction; misapplication of funds intended for the building of ships to the building of shipyards; misappropriation of equipment for his own purposes; and failure to turn over to the government the profits of ships it had leased to him. Indicted for war profiteering and fraud, soon after he was confronted with charges of mail fraud involving sales solicitations for stock of the United States Shipping Company. The trial on the war profiteering charges resulted in an acquittal, but a civil suit in 1925 against the Virginia Shipbuilding Company resulted in a judgment for the government of over $11.5 million. The mail fraud case against Morse ended when he was adjudged too ill to stand trial, and after a jury had disagreed the charges against his sons were quashed.
His second wife, Clemence, died in July 1926. Suffering from paralysis, Morse was placed under the guardianship of the probate court of Bath on September 7, 1926, adjudged incompetent to handle his affairs. Having suffered several strokes, he died of pneumonia at Bath, Maine, on January 12, 1933.

</doc>
<doc id="19283787" url="http://en.wikipedia.org/wiki?curid=19283787" title="Dibbapalem">
Dibbapalem

Dibbapalem is a suburban area in visakhapatnam.
Geography.
Dibbapalem is located at . This village comes under the Greater Visakhapatnam Corporation in Andhra Pradesh, India.
Beach.
The beach in Dibbapalem is called Gangavaram Beach. It was named after the town which existed before the Visakhaptnam Steel Plant was constructed.
Transport.
One can reach by buses are 65F,63 etc. from vizag city.
People.
There are 3,000 families living in the village, and most of its residents are either fishermen or employees at the Visakhapatnam Steel Plant. Fifty percent of the residents are fishermen, while the others are mostly the employees of the steel plant. The castes in the village are 'Kaapu', 'Palli' and 'Jaalari'. Mostly all the people of Dibbapalem are Hindus.
Temples.
There are three temples in the village. The Paidithalli Amma temple is located on the village outskirts. In the middle of the village, there are temples of Sita Ram, Lakshman and Hanuman. The temples of Trimurthies; Brahma, Vishnu and Maheswara are near the sea.
Festivals.
All of the Hindu festivals are celebrated in the village. The Maaghapournami is celebrated in February through March, the people from surrounding villages bathe in the sea. There is even a small fair on this day. The Paidithalamma Ammavaari Panduga festival is celebrated every other year in mid-June.
Village demolished.
The village was removed from the area and all the families are shifted to two places. Most of the people working at Visakhapatnam Steel plant are shifter near Srinagar and the people who have their living from fishing are located near peda gantyada.During this, there was lot of tension which went on for several months leading to strikes and police firing.While 1,270 families accepted the compensation package and agreed to vacate their houses, 350 owing allegiance to the Vedika refused to leave the village, demanding categorical assurance from the management of the private port to provide permanent jobs to at least one member of the project-affected family.
Gangavaram Port.
The whole village is vacated and Construction of the port began in December 2005, and commercial operations commenced in August 2008. The port was formally inaugurated in July 2009, by the Andhra Pradesh Chief Minister Y S Rajasekhara Reddy.The port is called Gangavaram Port after the name of the beach.This is the deepest port in India.
Fishermen in the Gangavaram and Dibbapalem villages, who were directly affected by the construction of the port, demanded construction of an alternative jetty and a relief and rehabilitation package.

</doc>
<doc id="19283801" url="http://en.wikipedia.org/wiki?curid=19283801" title="The Truth About Chernobyl">
The Truth About Chernobyl

The Truth About Chernobyl is a 1991 book by Grigori Medvedev. Medvedev served as deputy chief engineer at the No. 1 reactor unit of the Chernobyl Nuclear Power Plant in the 1970s. At the time of the Chernobyl disaster in 1986, Medvedev was deputy director of the main industrial department in the Soviet Ministry of Energy dealing with the construction of nuclear power stations. Since Medvedev knew the Chernobyl plant well, he was sent back as a special investigator immediately after the 1986 catastrophe.
In his book, Medvedev provides extensive first-hand testimony, based on many interviews, describing minute by minute precisely what was and was not done both before and after the explosion. It has been described as a tragic tale of pervasive, institutionalized, bureaucratic incompetence leading up to the accident; and heroic, heartbreaking sacrifice among those who had to deal with the emergency afterwards.
In 1991 it was awarded the Los Angeles Times Book Prize, Science and technology.

</doc>
<doc id="19283802" url="http://en.wikipedia.org/wiki?curid=19283802" title="George Holden (English footballer)">
George Holden (English footballer)

George Henry Holden (6 October 1859 – 1920s) was an English footballer who, playing as an outside-right, made four appearances for England in the 1880s.
Football career.
Holden was born in West Bromwich and attended St. John's School in Wednesbury. He began his football career with Old Park F.C. in Wednesbury in 1876 and then joined Wednesbury St. James F.C. in 1877. From there he joined Wednesbury Old Athletic in 1878. Holden established a reputation for being an extremely fast winger with exceptional dribbling skills. 
During his first spell with Wednesbury Old Athletic, he won four international caps. He made his international debut for England on 12 March 1881 against Scotland. England went down to a "humiliating" 6–1 defeat.
His next appearances came three years later in 1884, when he played in all three matches in the inaugural British Home Championship. England's opening match was against Ireland at Ballynafeigh Park, Belfast on 26 January 1884. England "won the match with ease" 8–1, with Henry Cursham scoring a hat-trick on his final England appearance, with the remaining goals coming from Edward Johnson (2), Charles Bambridge (2) and his brother Arthur. In a close match against Scotland at Cathkin Park on 15 March, the Scots won 1–0. Despite this defeat, Holden retained his place for the final match of the tournament against Wales on 17 March, which England won comfortably 4–0, including two goals from William Bromley-Davenport. Scotland's victory over England enabled them to claim the Home Championship, which they were to dominate for the next few seasons.
Holden's performances for England attracted the attention of West Bromwich Albion, whom he joined in May 1886, staying for a single season. Although Holden appeared in the early rounds of the FA Cup, including scoring in the First Round 6–0 victory over Burton Wanderers, he was no longer part of the side when Albion reached the Cup Final where they were to lose 2–0 to Aston Villa.
He rejoined Wednesbury Old Athletic for a final time between 1887 and 1888, before finishing his career with Derby Midland in 1888; he also represented Birmingham and Staffordshire FAs.
Holden died in the 1920s.

</doc>
<doc id="19283806" url="http://en.wikipedia.org/wiki?curid=19283806" title="San Francisco Bay Area">
San Francisco Bay Area

The San Francisco Bay Area, commonly known as the Bay Area, is a populated region that surrounds the San Francisco and San Pablo estuaries in Northern California, United States. The region encompasses the major cities and metropolitan areas of San Francisco, Oakland, and San Jose, along with smaller urban and rural areas. The Bay Area's nine counties are Alameda, Contra Costa, Marin, Napa, San Francisco, San Mateo, Santa Clara, Solano, and Sonoma. Home to approximately 7.15 million people, the nine-county Bay Area contains many cities, towns, airports, and associated regional, state, and national parks, connected by a network of roads, highways, railroads, bridges, tunnels and commuter rail. The combined urban area of San Francisco and San Jose is the second largest in California (after the Greater Los Angeles area), the fifth largest in the United States, and the 56th largest urban area in the world.
The United States Office of Management and Budget (OMB) does not use the nine-county definition of the San Francisco Bay Area. The OMB has designated a more extensive 12-county Combined Statistical Area (CSA) titled the San Jose-San Francisco-Oakland, CA Combined Statistical Area which also includes the three counties of San Joaquin, Santa Cruz, and San Benito that do not border San Francisco Bay, but are economically tied to the nine counties that do.
The San Francisco Bay Area is known for its natural beauty, liberal politics, entrepreneurship, and diversity. The area has high incomes; it includes the five highest California counties by per capita income and two of the top 25 wealthiest counties in the United States. The Bay Area was the largest driver of population growth in the state in 2012, with four of the five fastest growing counties (Santa Clara, Alameda, San Mateo and San Francisco), located in the Bay Area.
Sub-regions.
East Bay.
The eastern side of the bay, consisting of Alameda and Contra Costa counties, is known locally as the East Bay. The East Bay can be loosely divided into two regions, the inner East Bay, which adjoins the Bay shoreline, and the outer East Bay, consisting of inland valleys separated from the inner East Bay by hills and mountains.
North Bay.
The region north of the Golden Gate Bridge is known locally as the North Bay. This area encompasses Marin County, Sonoma County, Napa County and extends eastward into Solano County. The city of Fairfield, being part of Solano County, is often considered the easternmost city of the North Bay.
With few exceptions, this region is quite affluent: Marin County is ranked as the wealthiest in the state. The North Bay is relatively rural compared to the remainder of the Bay Area, with many areas of undeveloped open space, farmland and vineyards. Santa Rosa in Sonoma County is the North Bay's largest city, with a population of 167,815 and a Metropolitan Statistical Area population of 466,891, making it the fifth largest city in the San Francisco Bay Area.
The North Bay is the only section of the Bay Area that is not currently served by a commuter rail service. The lack of transportation services is mainly because of the lack of population mass in the North Bay, and the fact that it is separated completely from the rest of the Bay Area by water, the only access points being the Golden Gate Bridge leading to San Francisco, the Richmond-San Rafael and Carquinez Bridges leading to Richmond, and the Benicia-Martinez Bridge leading to Martinez.
Peninsula.
The area from San Francisco to the Silicon Valley, geographically part of the San Francisco Peninsula, is known locally as "The Peninsula". This area consists of a series of cities and suburban communities in San Mateo County and the northwestern part of Santa Clara County, as well as various towns along the Pacific coast, such as Pacifica and Half Moon Bay. This area is extremely diverse. Many of the cities and towns had originally been centers of rural life until the post-World War II era when large numbers of middle and upper class Bay Area residents moved in and developed the small villages. Since the 1980s the area has seen a large growth rate of middle and upper-class families who have settled in cities like Palo Alto, Los Altos, Portola Valley, Woodside, and Atherton as part of the technology boom of Silicon Valley. Many of these families are of foreign background and have significantly contributed to the diversity of the area. Peninsula cities include: Atherton, Belmont, Brisbane, Burlingame, Colma, Daly City, East Palo Alto, Foster City, Half Moon Bay, Hillsborough, Los Altos, Los Altos Hills, Menlo Park, Millbrae, Mountain View, Palo Alto, Pacifica, Portola Valley, Redwood City, Redwood Shores, San Bruno, San Carlos, San Mateo, South San Francisco and Woodside.
Whereas the term "peninsula" technically refers to the entire geographical San Franciscan Peninsula, in local terms, "The Peninsula" does not include the city of San Francisco itself.
San Francisco.
San Francisco is surrounded by water on three sides; the north, east, and west. The city squeezes approximately 805,000 people in under , making it the most densely populated major city in North America after New York City. On any given day, there can be as many as 1 million people in the city because of the commuting population and tourism. San Francisco also has the largest commuter population of the Bay Area cities. The limitations of land area, however, make continued population growth challenging, and also has resulted in increased real estate prices. Though San Francisco is located at the tip of the peninsula, when "the peninsula" is discussed, it usually refers to the communities and geographic locations south of the city proper.
San Jose and Silicon Valley.
The communities at the southern region of the San Francisco Bay Area are primarily located in what is known as Silicon Valley, or the Santa Clara Valley. These include the major city of San Jose, and its suburbs, including the high-tech hubs of Santa Clara, Milpitas, Cupertino, Sunnyvale as well as many other cities like Saratoga, Campbell, Los Gatos and the exurbs of Morgan Hill and Gilroy. Some Peninsula and East Bay towns are sometimes recognized as being in the Santa Clara Valley. Generally, the term South Bay refers to Santa Clara County, but the northwest portion of the county (Palo Alto, Mountain View, Los Altos and Los Altos Hills) is considered part of the Peninsula (even though these cities are in Santa Clara County).
Silicon Valley was primarily an agricultural center from the time of California's founding until World War II. During and after the war, working and middle-class families migrated to the area to settle and work in the burgeoning aerospace and electronics industries. The South Bay Area experienced rapid growth as agriculture was gradually replaced by high-technology. During this period, the Santa Clara Valley gradually became an urbanized metropolitan region. Today, the growth continues, fueled primarily by technology jobs, the weather, and immigrant labor. Urbanization is gradually replacing suburbanization as the population density of the valley increases. This trend has resulted in a huge increase in property values, forcing many middle-class families out of the area or into lower income neighborhoods in older sections of the region. The Santa Clara Valley also came to be known as Silicon Valley, as the area became the premier technology center of the United States. Some notable tech companies headquartered in the South Bay are AMD, Adobe, Intel, Netflix, Cisco Systems, Hewlett-Packard, Apple, Google, eBay, Facebook and Yahoo!. Largely a result of the high technology sector, the San Jose-Sunnyvale-Santa Clara, CA Metropolitan Statistical Area has the most millionaires and the most billionaires in the United States per capita.
The population of the entire valley is part of the San Jose-Sunnyvale-Santa Clara metropolitan area, which has about 2 million residents. San Jose, the largest city in the Silicon Valley area, is the tenth most populous city in the United States and the most populous city in the Bay Area. San Jose is the oldest city in California and was its first capital. The city prides itself on being an environmentally conscious city. It recycles a greater percentage of its waste than any other large American city. Over the past several decades, the South Bay Area has experienced rapid growth. To limit the effects of urban sprawl, planned communities were laid out to control growth. Urban growth boundaries have been established to protect remaining open space (primarily in the surrounding hills and southern border) from development. Most new growth has been urban infill in the form of high density housing to increase density rate. The growth rate has slowed, but the area continues to have steady growth.
San Jose is home to many sports teams both amateur and professional, such as the San Jose Sharks of the NHL, and the San Jose Earthquakes of MLS. The San Francisco 49ers will move from that city to a new stadium in Santa Clara in 2014.
The South Bay has a large transportation infrastructure that includes many freeways, VTA bus service and light rail, Amtrak, and commuter rail such as Caltrain. The San Jose International Airport serves air traffic in the South Bay Area and is conveniently located just north of downtown in the center of Silicon Valley. The height of buildings in Downtown is limited (due to FAA regulations and city ordinance) because it is situated directly under the flight path. The South Bay is poised to have a more efficient transportation network with the extension of the BART system to San Jose, which would allow elevated/subway travel into San Francisco. San Jose will also be a major stop on the proposed California High-Speed Rail system.
Santa Cruz, San Benito and San Joaquin.
Whether Santa Cruz, San Benito and San Joaquin counties are considered part of the San Francisco Bay Area depends on the observer. For example, the regional governments in the San Francisco Bay Area, including the Association of Bay Area Governments, the Metropolitan Transportation Commission, the Bay Area Air Quality Management District (BAAQMD), and the San Francisco Bay Regional Water Quality Control Board (RWQCB) include only the nine counties above in their boundaries or membership. (The BAAQMD includes all of the nine counties except the northern portions of Sonoma and Solano; the RWQCB includes all of San Francisco and the portions of the other eight counties that drain to San Francisco Bay or to the Pacific Ocean.) However, the United States Census Bureau defines the San Jose-San Francisco-Oakland Consolidated Statistical Area as a twelve-county region, including the nine counties above plus Santa Cruz, San Benito and San Joaquin Counties. Meanwhile, the California State Parks Department defines the Bay Area as including ten counties, including Santa Cruz but excluding San Benito and San Joaquin. On the other hand, Santa Cruz and San Benito along with Monterey County are part of a different regional government organization called the Association of Monterey Bay Area Governments. Local media in the San Francisco Bay Area and travel guides often consider these two counties as part of the South Bay subregion, as they are greatly connected geographically, economically, and historically. San Joaquin County is usually regarded as part of the California Central Valley.
Economy.
In 2011 the San Francisco Bay Area had a GDP of $535 billion, which would rank 19th among countries.
The Silicon Valley is located within the southern reaches of the Bay Area. The leading high technology region in the world, Silicon Valley covers San Jose and several cities of South Bay. The Valley is home to many of the industry leaders in technology such as Google, Yahoo!, Facebook, eBay, Cisco, Apple, Oracle, Marvell, Intel, and Hewlett-Packard. Major corporations in San Francisco, San Jose, Oakland, and the surrounding cities help make the region second in the nation in concentration of Fortune 500 companies, after New York. The region's northern counties encompass California's famous Wine Country, home to hundreds of vineyards and wineries. The Bay Area is a leader in sustainable agriculture, organic farming, and sustainable energy and for being a leading producer of high quality food, wine, and innovation in the culinary arts. California Cuisine was developed primarily in the Bay Area, as was the organic farming movement. The area is renowned for its natural beauty. It is also known as being one of the most expensive regions to live in the country.
Oakland, on the east side of the bay, has the fifth largest container shipping port in the United States. The city is also a major rail terminus.
Changes in house prices for the Bay Area are publicly tracked on a regular basis using the Case–Shiller index; the statistic is published by Standard & Poor's and is also a component of S&P's 10-city composite index of the value of the U.S. residential real estate market.
The Bay Area led the United States in sustainable energy and "clean tech" development in 2012, with San Francisco and San Jose having significantly higher ratings than any other US cities, according to Clean Edge.
Metropolitan area.
The United States Office of Management and Budget has designated seven Metropolitan Statistical Areas (MSAs) and one Combined Statistical Area (CSA) for the greater San Francisco Bay Area. These comprise:
The United States Census Bureau estimates the population for these areas as of July 1, 2012 as follows:
The San Jose-San Francisco-Oakland, CA Combined Statistical Area ranks as the fifth most populous metropolitan area of the United States.
Demographics.
According to the 2010 United States Census, the population was 7.15 million in the nine counties bordering the San Francisco Bay, with 49.6% male and 50.4% female. In 2010 the racial makeup of the nine-county Bay Area was 52.5% White including white Hispanic, 6.7% non-Hispanic African American, 0.7% Native American, 23.3% Asian (7.9% Chinese, 5.1% Filipino, 3.3% Indian, 2.5% Vietnamese, 1.0% Korean, 0.9% Japanese, 0.2% Pakistani, 0.2% Cambodian, 0.2% Laotian, 0.1% Thai, 0.1% Burmese), 0.6% Pacific Islander (0.1% Tongan, 0.1% Samoan, 0.1% Fijian, >0.1% Guamanian, >0.1% Native Hawaiian), 10.8% from other races, and 5.4% from two or more races. The population was 23.5% Hispanic or Latino of any race (17.9% Mexican, 1.3% Salvadoran, 0.6% Guatemalan, 0.6% Puerto Rican, 0.5% Nicaraguan, 0.3% Peruvian, 0.2% Cuban).
The Chinese population of the Bay Area is concentrated in San Francisco, where 30% of the Bay Area's Chinese American population resides, as well as in Oakland and in the Silicon Valley region of the South Bay, which is also home to a significant Indian American community. The San Francisco Bay Area is home to over 382,950 Filipino Americans, one of the largest communities of Filipino people outside of the Philippines with the largest proportion of Filipino Americans concentrating themselves within Daly City. There are more than one hundred thousand people of Vietnamese ancestry residing within San Jose city limits, the largest population of any city in the world outside of Vietnam. There is a sizable community of Korean Americans in Santa Clara County, and there is a large strip of Korean restaurants and businesses located in Santa Clara. East Bay cities such as Richmond and Oakland, and the North Bay city of Santa Rosa, have plentiful populations of Laotian and Cambodians in certain neighborhoods.
Pacific Islanders such as Samoans and Tongans have the largest presence in East Palo Alto, San Mateo, San Bruno, Redwood City and the Bayview-Hunters Point and Visitacion Valley neighborhoods of San Francisco.
The Latino population is widely spread out through the Bay Area, but have largest populations in San Jose, in The Peninsula the cities of Redwood City, East Palo Alto, South San Francisco, San Bruno, as well the cities of San Mateo, Daly City, and Menlo Park. East Bay cities namely Oakland, Richmond, Concord and Antioch, and in Sonoma County. San Francisco's Mission District is home to a thriving Mexican American community, as well as many residents of Salvadoran and Guatemalan descent.
The African American population of the San Francisco Bay Area has always been significant in Oakland and Richmond, but other East and North Bay cities such as Antioch, Vallejo and Fairfield have received an influx of black residents. While mainly concentrated in the East Bay and in San Francisco, there are well established black neighborhoods located in the North Bay and in the South Bay. The South Park neighborhood of Santa Rosa was home to once a primarily black community until the 1980s, when many Latino immigrants settled in the area. The Marin City community in Marin County has a significant black population. In the South Bay, East Palo Alto has the highest population of African Americans.
San Francisco's North Beach district is considered the Little Italy of the city, and was once home to a significant Italian American community. San Francisco and Marin County both have substantial Jewish communities.
In 2007 the population density was 1,057 people per square mile. There were 2,499,702 housing units with an average family size of 3.3. Of the 2,499,702 households, approximately one-third were renter occupied housing units, while two-thirds were owner occupied housing units. 12.7% had a female householder with no husband present, 11.6% of households had someone 65 years of age or older, and 27.4% of households were non-families.
The San Francisco Bay Area is one of the wealthiest regions in the United States, due, primarily, to the economic power engines of San Francisco, Oakland and San Jose. Pleasanton has the second highest household income in the country after New Canaan, CT. However, discretionary income is very comparable with the rest of the country, primarily because the higher cost of living offsets the increased income.
Forty-seven Bay Area residents made the Forbes magazine's 400 richest Americans list, published in 2007. Thirteen lived in San Francisco proper, placing it seventh among cities in the world. Among the forty-seven were several well-known names such as Steve Jobs, George Lucas, and Charles Schwab. The wealthiest resident was Larry Ellison of Oracle, worth $25 billion.
A study by Capgemini indicates that in 2009, 4.5% of all households within the San Francisco-Oakland and San Jose metropolitan areas held $1 million in investable assets, placing the region No. 1 in the United States (Metro New York City placed second at 4.3%).
As of 2007, there were approximately 80 public companies with annual revenues of over $1 billion a year, and 5–10 more private companies. Nearly 2/3 of these are in the Silicon Valley section of the Bay Area. According to the May 2010 Fortune Magazine analysis of the US "Fortune 500" companies, the combined San Jose-San Francisco-Oakland metropolitan region ranks second (after metro New York City and before Chicago) with 30 companies (May 2011, Fortune Magazine).
Politics.
The San Francisco Bay Area is widely regarded as one of the most liberal areas in the country. According to the Cook Partisan Voting Index (CPVI), congressional districts the Bay Area tends to favor Democratic candidates by roughly 40 to 50 percentage points, considerably above the mean for California and the nation overall. All congressional districts in the region voted for Democrat Barack Obama over Republican John McCain in the 2008 Presidential Election.
Over the last four and a half decades the 9-county Bay Area voted for Republican candidates only twice, once in 1972 for Richard Nixon and again in 1980 for Ronald Reagan, both Californians. The last county to vote for a Republican Presidential candidate was Napa county in 1988 for George H. W. Bush.
During the Base Realignment and Closures (BRACs) of the 1990s, almost all the military installations in the region were closed. The only remaining major active duty military installations are Travis Air Force Base and Coast Guard Island.
Climate.
Because the hills, mountains, and large bodies of water produce such vast geographic diversity within this region, the San Francisco Bay Area offers a significant variety of microclimates. The areas near the Pacific Ocean are generally characterized by relatively small temperature variations during the year, with cool foggy summers and mild rainy winters. Inland areas, especially those separated from the ocean by hills or mountains, have hotter summers and colder overnight temperatures during the winter. San Jose at the south end of the Bay averages fewer than of rain annually, while Napa at the north end of the Bay averages over 30 and parts of the Santa Cruz Mountains just a few miles west of San Jose get over 55. In the summer, inland regions can be over 40 degrees Fahrenheit (22 degrees Celsius) warmer than the coast. This large temperature contrast induces a strong pressure gradient, which results in brisk coastal winds which help keep the coastal climate cool and typically foggy during the summer. Additionally, strong winds are produced through gaps in the coastal ranges such as the Golden Gate, the Carquinez Strait, and the Altamont Pass, the latter the site of extensive wind farms. During the fall and winter seasons, when not stormy, a high pressure area is usually present inland, leading to an offshore flow. While negatively impacting air quality, this also clears fog away from the Pacific shore, and so the best weather in San Francisco can usually be found from mid September through mid October. Winter storms are typically wet and mild in temperature during this time of year, being caused by cold fronts sweeping the eastern Pacific and often originating in the Gulf of Alaska. During November into mid March, winter storms are usually several days in length, wet and cool, with severely damaging storms rare. There is also recorded snowfall on San Francisco Bay Area peaks, such as Mount St. Helena, Tamalpais, Diablo and Hamilton. Snow levels range every given year from 1000 feet in Sonoma County to 2,000 ft in Contra Costa, San Mateo, and Santa Clara counties the during the winter. Greater recorded snowfall amounts are generally recorded once every 5 to 10 years. In February 2001, 30 inches (76 cm) of snow fell on Mount Hamilton (4360 ft), 17 inches on Mount Tamalpais (2,574 ft) and 10 inches on Mount Diablo (3,864 ft). Occasionally during the late Summer or early Autumn, spells of warm humid weather will drift over the Bay Area from the Southwest Monsoon or from the residue of Western Pacific hurricanes near Mexico, usually bringing high variable clouds as well, and more rarely, high-based thunderstorms.
High and Low Average Temperatures in Various Cities in the San Francisco Bay Area expressed in Fahrenheit and (Celsius) degrees 
Ecology.
Despite its urban and industrial character, San Francisco Bay and the Sacramento-San Joaquin Delta remain perhaps California's most important ecological habitats. California's Dungeness crab, Pacific halibut, and Pacific salmon fisheries rely on the bay as a nursery. The few remaining salt marshes now represent most of California's remaining salt marsh, supporting a number of endangered species and providing key ecosystem services such as filtering pollutants and sediments from the rivers. Most famously, the bay is a key link in the Pacific Flyway. Millions of waterfowl annually use the bay shallows as a refuge. Two endangered species of birds are found here: the California least tern and the California clapper rail. Exposed bay muds provide important feeding areas for shorebirds, but underlying layers of bay mud pose geological hazards for structures near many parts of the bay perimeter. San Francisco Bay provided the nation's first wildlife refuge, Oakland's artificial Lake Merritt (constructed in the 1860s) and America's first urban National Wildlife Refuge, the Don Edwards San Francisco Bay National Wildlife Refuge (SFBNWR) (1972). The Bay is also invaded by non-native species.
Steelhead ("Oncorhynchus mykiss") populations in California have dramatically declined due to human and natural causes. The Central California Coast distinct population segment (DPS) was listed as threatened under the Federal Endangered Species Act on August 18, 1997; threatened status was reaffirmed on January 5, 2006. This DPS includes all naturally spawned anadromous steelhead populations below natural and manmade impassable barriers in California streams from the Russian River to Aptos Creek, and the drainages of San Francisco, San Pablo, and Suisun Bays. The U.S. National Marine Fisheries Service has a detailed description of threats.
The Central California Coast Coho salmon ("Oncorhynchus kisutch") Evolutionary Significant Unit (ESU) population is the most endangered of the many troubled salmon populations on the West Coast. It was listed as threatened on October 31, 1996 and later downgraded to endangered status on June 28, 2005. The ESU includes all naturally spawned populations of coho salmon ("Oncorhynchus kisutch") from Punta Gorda in northern California south to and including the San Lorenzo River in central California, as well as populations in tributaries to San Francisco Bay. The National Park Service has made major recent investments in restoring the tidal wetlands at the mouths of Lagunitas Creek and Redwood Creek including levee removal and placement of large woody debris in the creeks, which provide shelter to salmonids during heavy stream flows and flooding. Lagunitas Creek's coho population is especially important, as 80% of the ESU depends on this stream draining the north slope of Mount Tamalpais. This year's coho count dropped to 64 from an average of 600 in previous years.
Western Burrowing Owls ("Athene cunicularia") were listed as a Species of Special Concern (a pre-listing category under the Endangered Species Act) by the California Department of Fish and Game in 1979. California's population declined 60% from the 1980s to the early '90s, and continues to decline at roughly 8% per year. In 1994, the U.S. Fish and Wildlife Service nominated the Western Burrowing Owl as a Federal Category 2 candidate for listing as endangered or threatened, but loss of habitat continues due to development of the flat, grassy lands used by the owl. A 1992–93 survey reported no breeding burrowing owls in Napa, Marin, and San Francisco counties, and only a few in San Mateo and Sonoma. The Santa Clara County population is declining and restricted to a few breeding locations, leaving only Alameda, Contra Costa, and Solano counties as the remnant breeding range. Despite organized protests at Kiper Homes' Blue Ridge property in Antioch, California by "Friends of East Bay Owls", one-way doors were installed in the birds' burrows so that the owl families could not return to their nests in early 2010. In addition, in 2008, Mountain View, California evicted a pair of burrowing owls so that it could sell a parcel of land to Google to build a hotel at Shoreline Boulevard and Charleston Road. Eviction of the owls is controversial because the birds regularly reuse burrows for years, and there is no requirement that suitable new habitat be found for the owls.
Much of the SFBNWR consists of salt evaporation ponds acquired from the Leslie Salt Company and its successor, Cargill Corporation through a series of land sales and donations. Many of these salt ponds remain in operation and produce salt used throughout the Western United States in food, agriculture, industry and medicine. The refuge pond support dense populations of brine shrimp, and therefore serving as feeding areas for waterfowl. In 2003, the US Fish & Wildlife Service and California Department of Fish & Game entered one of the largest private land purchases in American history, with the state and federal governments paying $100 million for 15,100 acres (65 km²) of salt ponds (which government appraisers valued at $243 million prior to the acquisition) in the south bay and 1,400 acres near the Napa River. SFBNWR and state biologists hope to restore some of the recently purchased ponds as tidal wetlands.
Aquatic mammals recently re-colonizing the Bay Area include the California Golden Beaver ("Castor canadensis") which is now established on Alhambra Creek in Martinez, Napa River and Sonoma Creek; and North American River Otter ("Lontra canadensis") which was first reported in Redwood Creek at Muir Beach in 1996, and recently in Corte Madera Creek, and in the south Bay on Coyote Creek, as well as in 2010 in San Francisco Bay itself at the Richmond Marina. Sea otter ("Enhydra lutris") were hunted to extinction in San Francisco Bay by about 1817. Historical records reveal that the Russian-American Company snuck Aleuts into San Francisco Bay multiple times, despite the Spanish capturing or shooting them while hunting sea otters in the estuaries of San Jose, San Mateo, San Bruno and around Angel Island. The founder of Fort Ross, Ivan Kuskov, finding otter scarce on his second voyage to Bodega Bay in 1812, sent Russian ships and hired an American ship to hunt otter in the Bay, catching 1,160 sea otter in three months.
Humphrey the Whale, a humpback whale ("Megaptera novaeangliae"), entered San Francisco Bay twice on errant migrations, and was successfully rescued and redirected each time in the late 1980s and early 1990s. This occurred again with Dawn and Delta a mother and calf in 2007.
The seasonal range of water temperature in the Bay is from about 8 °C (46 °F) to about 23 °C (73 °F).
Industrial, mining, and other uses of mercury have resulted in a widespread distribution of that poisonous metal in the bay, with uptake in the bay's phytoplankton and contamination of its sportfish. In November 2007, a ship named "Cosco Busan" collided with the San Francisco – Oakland Bay Bridge and spilled over 58,000 gallons of bunker fuel, creating the largest oil spill in the region since 1996.
In March, 2012 a Bald eagle ("Haliaeetus leucocephalus") nest was reported on the northwest arm of Lower Crystal Springs Reservoir and upper San Mateo Creek. This is the first bald eagle breeding pair on the San Francisco Peninsula since 1915, when they nested in La Honda, almost one hundred years ago. The birds were once common in the Bay Area. While visiting Santa Clara County in 1855, physician naturalist James G. Cooper described "a nest of this bird large enough to fill a wagon, built in a large sycamore tree, standing alone in the prairie. Habitat destruction and thinning of eggs from (now banned) DDT poisoning reduced the California state population to 35 nesting pairs at their lowest point. In the 1980s re-introductions began with the Santa Cruz Predatory Bird Research Group and the San Francisco Zoo began importing birds and eggs from Vancouver Island and northeastern California in the late 1980s.
Geology and landforms.
Multiple terrains.
The area is well known worldwide for the complexity of its landforms, the region being composed of at least six terranes (continental, seabed, or island arc fragments with distinct characteristics) pushed together over many millions of years by the forces of plate tectonics. Nine out of eleven distinct "assemblages" have been identified in a single county (Alameda). Diverse assemblages adjoin in complex arrangements due to offsets along the many faults (both active and stable) in the area. As a consequence, many types of rock and soil are found in the region. Formations include the sedimentary rocks of sandstone, limestone, and shale in uplifted seabeds, metamorphic serpentine rock, coal deposits, and igneous forms such as basalt flows, rhyolite outcroppings, granite associated with the Salinian Block west of the San Andreas Fault, and ash deposits of extinct yet relatively recently active (10 million years) volcanos. Pleistocene-era fossils of mammals are abundantly present in some locations.
Vertical relief.
The region has considerable vertical relief in its landscapes that are not in the alluvial plains leading to the bay or in inland valleys. In combination with the extensive water regions this has forced the fragmented development of urban and suburban regions and has led to extensive building on poor soils in the limited flatland areas and considerable expense in connecting the various subregions with roads, tunnels, and bridges.
Several mountains are associated with some of the many ridge and hill structures created by compressive forces between the Pacific Plate and the North American plate. These provide spectacular views (in appropriate weather) of large portions of the Bay Area and include Marin County's Mount Tamalpais at 2,571 feet (784 m). Contra Costa County's Mount Diablo at 3,849 feet (1,173 m), Alameda County's Mission Peak at 2,517 to 2,604 feet (767 to 776 m), and Santa Clara County's Mount Hamilton at 4,213 ft (1,284 m), the latter with significant astronomical studies performed at its crowning Lick Observatory. Though Tamalpais and Mission Peak are quite lower than the others, Tamalpais has no other peaks and few hills nearby. Mission Peak is coast facing and is an interior mountain and therefore has excellent views of both sides.
The three major ridge structures (part of the Pacific Coast Range) which are all roughly parallel to the major faultlines:
Earthquake faults.
The region is also traversed by six major slip-strike fault systems with hundreds of related faults, many of which are "sister faults" of the infamous San Andreas Fault, all of which are stressed by the relative motion between the Pacific Plate and the North American Plate or by compressive stresses between these plates. The fault systems include the Hayward Fault Zone, Concord-Green Valley Fault, Calaveras Fault, Clayton-Marsh Creek-Greenville Fault, and the San Gregorio Fault. Significant blind thrust faults (faults with near vertical motion and no surface ruptures) are associated with portions of the Santa Cruz Mountains and the northern reaches of the Diablo Range and Mount Diablo.
Natural hazards.
Earthquakes.
The region is particularly exposed to hazards associated with large earthquakes, owing to a combination of factors:
Some of these hazards are being addressed by seismic retrofitting, education in household seismic safety, and even complete replacement of major structures such as the eastern span of the San Francisco – Oakland Bay Bridge.
For an article concerning a typical fault in the region and its associated hazards see Hayward Fault Zone.
For projected ground movement after selecting a locality and a generating fault see this ABAG web page
Flooding.
Some flooding occurs on local drainages under sustained wet conditions when the ground becomes saturated, more frequently in the North Bay area, which tends to receive substantially more rainfall than the South Bay. In one case, the Napa River drainage, floodplain developments are being purchased and removed and natural wetlands restored in the innovative Napa River Flood Project as the previous channelization of insufficient capacity around such developments was causing flooding problems upstream. Many of the local creeks have been channelized, although modern practice, and some restoration work includes returning the creeks to a natural state with dry stormwater bypasses constructed to handle flooding. While quite expensive, the restoration of a natural environment is of high priority in the intensively urbanized areas of the region.
Windstorms and wildfires.
Typically between late November and early March, a very strong Pacific storm can bring both substantial rainfall (saturating and weakening soil) and strong wind gusts that can cause trees to fall on power lines. Owing to the wide area involved (sometimes hundreds of miles of coast), electrical service can be interrupted for up to several days in some more remote localities, but service is usually restored quickly in urban areas. These storms occasionally bring lightning & thunder. More rarely they even spawn tornadoes. For example, during the abnormal hurricane-like storm in early 2010, a funnel cloud sparked an extremely rare Tornado Warning in Morgan Hill.
In the spring and fall, strong offshore winds periodically develop. These winds are an especially dangerous fire hazard in the fall when vegetation is at its driest, as exemplified historically by the 1923 Berkeley Fire and the 1991 Oakland Firestorm.
Mudslides and landslides.
Some geologically unstable areas have been extensively urbanized, and can become mobile due to changes in drainage patterns and grading created for development. These are usually confined to small areas, but there have been larger problems in the Santa Cruz Mountains.
Transportation.
The Bay Area is served by many public transportation systems, including three international airports (SFO, OAK, SJC), six major overlapping bus transit agencies (AC Transit, Muni, SamTrans, VTA, Golden Gate Transit, County Connection), in addition to dozens of smaller ones. There are four rapid transit and regional rail systems including BART and Caltrain and two light rail systems (San Francisco Muni Metro and VTA Light-rail). There are also several regional rail lines provided by Amtrak, notable the Capitol Corridor. In addition to rail lines, there are multiple public and private ferry services (notably Golden Gate Ferry and Blue and Gold Fleet), which are being expanded by the San Francisco Bay Water Transit Authority. The regional ferry hub is San Francisco Ferry Building. AC Transit and some other agencies provide an extensive network of express "transbay" commuter buses from the suburbs to San Francisco Transbay Terminal.
The freeway and highway system is very extensive; however, many freeways are heavily congested during rush hour, especially two of the trans-bay bridges (Golden Gate and Bay Bridge). Furthermore there are some large gaps in the highways which run onto city streets in San Francisco, partially due to the Freeway Revolt (SF Board of Supervisors decisions made in 1959, 1964 and 1966), which prevented completion of freeways connecting the San Francisco – Oakland Bay Bridge western terminus (Interstate 80) with the southern terminus of the Golden Gate Bridge, and U.S. 101 through San Francisco, and additionally due to the destruction of several of those very freeway structures that sparked the revolt, which were damaged in the 1989 Loma Prieta earthquake and subsequently removed rather than being reinforced or rebuilt.
Higher education.
The region is home to many colleges and seminaries, most notably the University of California, Berkeley, the University of California, San Francisco and Stanford University. In addition, the Bay Area is home to two of the twenty-eight Jesuit universities in the U.S.: Santa Clara University (founded in 1851), and University of San Francisco (1855); these are also the two oldest California colleges. San Jose State University is the founding campus of the California State University (CSU) system, and is the oldest public institution of higher education on the West Coast of the United States. Saint Mary's College of California was founded in 1863 by the Roman Catholic Archdiocese of San Francisco. In 2008, there were approximately 588,000 students enrolled in college or graduate school. The San Francisco Bay Area population is near the top in the Nation for overall education level with approximately 41 percent of residents aged 25 years and over having a bachelors degree or higher. The San Francisco and San Jose Primary Metropolitan Statistical Areas rank third and fourth in college graduates, ahead of Boston and behind only Boulder–Longmont, CO PMSA and Stamford–Norwalk, CT PMSA. The Oakland PMSA ranks eleventh.
Culture.
The Bay Area is host to numerous cultural events, including annual festivals and fairs. Many prominent writers make their homes there, and have developed a local literary culture, with a supportive network of booksellers, focused on the Northern California Independent Booksellers Association.
Music.
Classic rock.
San Francisco proper was headquarters for the hippie counterculture of the 1960s and the music scene that became associated with it. One of the area's most notable acts was The Grateful Dead, formed in 1965, who played regularly at the legendary venue "The Fillmore Auditorium". Other local artists in that movement included Jefferson Airplane and Janis Joplin; all three would be closely associated with the 1967 Summer of Love. Jimi Hendrix, although born in Seattle and later a resident of London, England, had strong connections to the movement and the metropolitan Bay area, as he lived in Berkeley for a brief time as a child and played many local venues in that decade. Creedence Clearwater Revival (of El Cerrito) would gain traction as an associated band of the . Rock and Roll Hall of Fame legend Neil Young has lived in the Bay Area in La Honda, CA for more than 40 years. Carlos Santana from San Francisco became famous in the late 1960s and early 1970s with his Santana band which pioneered a blend of rock, salsa, and jazz fusion. Journey formed in 1973 in San Francisco, by former members of Santana.
The Doobie Brothers, from San Jose, had a successful career with several albums earning RIAA gold certification. The early 1970s sounds of the Tower of Power from Oakland, Sly and the Family Stone and Pablo Cruise all came from the Bay Area.
Heavy metal.
During the 1980s and early 1990s, the Bay Area was home to one of the largest and most influential thrash metal scenes in the world, containing acts like Metallica (although Metallica had initially formed in Los Angeles, it wasn't until their relocation to El Cerrito in 1983 that Cliff Burton and Kirk Hammett joined as bassist and lead guitarist), Exodus, Laaz Rockit, Death Angel, Vio-lence, Forbidden, and Testament.
Many death metal bands had also formed in the area, including Autopsy, Possessed (considered one of the first in the genre), and in the '90s, Impaled, Exhumed and Vile.
Also, an avant-garde metal scene has emerged in the Bay Area with bands such as Giant Squid, Grayceon, and Ludicra.
Sludge band Neurosis and groove metal/post-thrash bands Machine Head and Skinlab formed in Oakland. In the alternative metal and nu-metal scenes worldwide, Faith No More (from San Francisco) and Primus (from El Sobrante, and featuring former Possessed guitarist Larry LaLonde) have been considered progenitors to both subgenres.
Heavy metal/hard rock icon Joe Satriani, also hails from the Bay Area (Berkeley).
Rising Deathcore band, Rings of Saturn, is also from Bay Area.
Alternative rock.
Many bands of the 1990s post-grunge era started and still reside in the Bay Area, including Third Eye Blind (of San Francisco), Counting Crows (of Berkeley) and Smash Mouth (of San Jose), all of whom have received extensive radio play across the world and released multi-platinum records during their career.
Punk.
The Bay Area saw a large punk movement from the 70s to the present. Bands such as the Dead Kennedys, The Avengers, Flipper, D.R.I., M.D.C. and Operation Ivy were popular in the '70s and '80s, with later bands such as Rancid, Green Day and AFI all coming out of Berkeley. The Dwarves are residents of San Francisco, and are considered to be pioneers of the punk and hardcore movement.
Rap and hip hop.
The Bay Area is the home of the hyphy movement, which started in the early to mid-'90s. The genre which was pioneered by rappers Andre "Mac Dre" Hicks, Too Short, Keak Da Sneak, Mistah Fab, E-40, and Yukmouth, is now becoming more popular throughout the world. Hyphy themes such as ghost riding, thizzin' and going dumb are now common in other parts of the country. The Bay Area was also home to rap legend Tupac Shakur who lived in Marin City, about north of San Francisco. The rap group Digital Underground originally hailed from Oakland. MC Hammer, and the Hieroglyphics hip hop crew, which is composed of local artists including the Souls of Mischief and Del tha Funkee Homosapien. Cindy Herron of En'Vogue attended Balboa High School in the late 1970s. Usher is commonly known to visit at times as well.
Media.
The Bay Area is one of the largest media markets in the United States.
According to Nielsen Media Research, the Bay Area ranks (as of the 2005–2006 television season) as the nation's sixth-largest "Designated Market Area (DMA)", with 2,355,740 "TV Homes", representing 2.137% of the United States Total.

</doc>
<doc id="19283814" url="http://en.wikipedia.org/wiki?curid=19283814" title="History of Belize">
History of Belize

The history of Belize dates back thousands of years. The Maya civilization spread into the area of Belize between 1500 BCE and CE 200 and flourished until about CE 1200. Several major archeological sites—notably Cahal Pech, Caracol, Lamanai, Lubaantun, Altun Ha, and Xunantunich—reflect the advanced civilization and much denser population of that period. The first recorded European settlement was established by shipwrecked English seamen in 1638. Over the next 150 years, more English settlements were established. This period also was marked by piracy, indiscriminate logging, and sporadic attacks by natives and neighboring Spanish settlements.
Great Britain first sent an official representative to the area in the late 18th century, but Belize was not formally termed the "Colony of British Honduras" until 1840. It became a crown colony in 1862. Subsequently, several constitutional changes were enacted to expand representative government. Full internal self-government under a ministerial system was granted in January 1964. The official name of the territory was changed from British Honduras to Belize in June 1973, and full independence was granted on September 21, 1981.
Ancient Maya civilization.
The Maya civilization emerged at least three millennia ago in the lowland area of the Yucatán Peninsula and the highlands to the south, in what is now southeastern Mexico, Guatemala, western Honduras, and Belize. Many aspects of this culture persist in the area despite nearly 500 years of European domination. Prior to about 2500 B.C., some hunting and foraging bands settled in small farming villages; they later domesticated crops such as corn, beans, squash, and chili peppers. A profusion of languages and subcultures developed within the Maya core culture. Between about 2500 B.C. and A.D. 250, the basic institutions of Maya civilization emerged. The peak of this civilization occurred during the classic period, which began about A.D. 250. The recorded history of the center and south is dominated by Caracol, where the inscriptions on their monuments was, as elsewhere, in the Lowland Maya aristocratic tongue Classic Ch'olti'an. North of the Maya Mountains, the inscriptional language at Lamanai was Yucatecan as of 625 CE. The last date recorded in Ch'olti'an within Belizean borders is 859 CE in Caracol, stele 10. Yucatec civilisation, in Lamanai, lasted longer.
Farmers engaged in various types of agriculture, including labor-intensive irrigated and ridged-field systems and shifting slash-and-burn agriculture. Their products fed the civilization's craft specialists, merchants, warriors, and priest-astronomers, who coordinated agricultural and other seasonal activities with a of rituals in ceremonial centers. These priests, who observed the movements of the sun, moon, planets, and stars, developed a complex mathematical and calendrical system to coordinate various cycles of time and to record specific events on carved stelae. The Maya were skilled at making pottery, carving jade, knapping flint, and making elaborate costumes of feathers. the architecture of Maya civilization included temples and palatial residences organized in groups around plazas. These structures were built of cut stone, covered with stucco, and elaborately decorated and painted. Stylized carvings and paintings, along with sculptured stelae and geometric patterns on buildings, constitute a highly developed style of art.
Belize boasts important sites of the earliest Maya settlements, majestic ruins of the classic period, and examples of late postclassic ceremonial construction. About five kilometers west of Orange Walk, is Cuello, a site from perhaps as early as 2,500 B.C. Jars, bowls, and other dishes found there are among the oldest pottery unearthed in present-day Mexico and Central America. Cerros, a site on Chetumal Bay, was a flourishing trade and ceremonial center between about 300 B.C. and A.D. 100. One of the finest carved jade objects of Maya civilization, the head of what is usually taken to be the sun god Kinich Ahau, was found in a tomb at the classic period site of Altún Ha, thirty kilometers northwest of present-day Belize City. Other Maya centers located in Belize include Xunantunich and Baking Pot in Cayo District, Lubaantún and Nimli Punit in Toledo District, and Lamanai on Hill Bank Lagoon in Orange Walk District.
In the late classic period, probably at least 400,000 people inhabited the Belize area. People settled almost every part of the country worth cultivating, as well as the cay and coastal swamp regions. But in the 10th century, Maya society suffered a severe breakdown. Construction of public buildings ceased, the administrative centers lost power, and the population declined as social and economic systems lost their coherence. Some people continued to occupy, or perhaps reoccupied, sites such as Altún Ha, Xunantunich, and Lamanai. Still, these sites ceased being splendid ceremonial and civic centers. The decline of Maya civilization is still not fully explained. Rather than identifying the collapse as the result of a single factor, many archaeologists now believe that the decline of the Maya was a result of many complex factors and that the decline occurred at different times in different regions.
Conquest and early colonial period.
Pre-Columbian Maya societies and the conquest.
Many Maya were still in Belize when the Europeans came in the 16th and 17th centuries. Archaeological and ethnohistorical research confirms that several groups of Maya peoples lived in the area now known as Belize in the 16th century. The political geography of that period does not coincide with present-day boundaries, so several Maya provinces lay across the frontiers of modern Belize, Mexico, and Guatemala.
Spain soon sent expeditions to Guatemala and Honduras, and the conquest of Yucatán began in 1527. Though the Maya offered stiff resistance to Spanish "pacification", diseases contracted from the Spanish devastated the indigenous population and weakened its ability to resist conquest. In the 17th century, Spanish missionaries established churches in Maya settlements with the intention of converting and controlling these people.
Piracy along the coast increased during this period. In 1642, and again in 1648, pirates sacked Salamanca de Bacalar, the seat of Spanish government in southern Yucatán. The abandonment of Bacalar ended Spanish control over the Maya provinces of Chetumal and Dzuluinicob.
Between 1638 and 1695, the Maya living in the area of Tipu enjoyed autonomy from Spanish rule. But in 1696, Spanish soldiers used Tipu as a base from which they pacified the area and supported missionary activities. In 1697 the Spanish conquered the Itzá, and in 1707, the Spanish forcibly resettled the inhabitants of Tipu to the area near Lago Petén Itzá. The political center of the Maya province of Dzuluinicob ceased to exist at the time that British colonists were becoming increasingly interested in settling the area.
Colonial rivalry between Spain and Britain.
In the 16th and 17th centuries, Spain tried to maintain a monopoly on trade and colonization in its New World colonies, but northern European powers were increasingly attracted to the region by the potential for trade and settlement. These powers resorted to smuggling, piracy, and war in their efforts to challenge and then destroy Spain's monopoly. In the 17th century, the Dutch, English, and French encroached on Spain's New World possessions.
Early in the 17th century, in southeastern Mexico and on the Yucatán Peninsula, English buccaneers began cutting logwood ("Haematoxylum campechianum"), which was used in the production of a textile dye. According to legend, one of these buccaneers, Peter Wallace, called "Ballis" by the Spanish, settled near and gave his name to the Belize River as early as 1638. English buccaneers began using the coastline as a base from which to attack Spanish ships. Buccaneers stopped plundering Spanish logwood ships and started cutting their own wood in the 1650s and 1660s. Logwood extraction then became the main reason for the English settlement for more than a century. A 1667 treaty, in which the European powers agreed to suppress piracy, encouraged the shift from buccaneering to cutting logwood and led to more permanent settlement.
Conflict continued between Britain and Spain over the right of the British to cut logwood and to settle in the region. In 1717 Spain expelled British logwood cutters from the Bay of Campeche west of the Yucatán. During the 18th century, the Spanish attacked the British settlers repeatedly. The Spanish never settled in the region, however, and the British always returned to expand their trade and settlement. The 1763 Treaty of Paris conceded to Britain the right to cut logwood but asserted Spanish sovereignty over the territory. When war broke out again in 1779, the British settlement was abandoned until the Treaty of Versailles in 1783 allowed the British to again cut logwood in the area. By that time, however, the logwood trade had declined and Honduras Mahogany ("Swietenia macrophylla") had become the chief export.
Beginnings of self-government and belize.
The British were reluctant to set up any formal government for the settlement for fear of provoking the Spanish. On their own initiative, settlers had begun electing magistrates to establish common law as early as 1738. In 1765 these regulations were codified and expanded into Burnaby's Code. When the settlers began returning to the area in 1784, Colonel Edward Marcus Despard was named superintendent to oversee the Settlement of Belize in the Bay of Honduras. The 1786 Convention of London allowed the British settlers to cut and export timber but not to build fortifications, establish any form of government, or develop plantation agriculture. Spain retained sovereignty over the area.
The last Spanish attack on the British settlement, the Battle of St. George's Caye, occurred two years after the outbreak of war in 1796. The British drove off the Spanish, thwarting Spain's last attempt to control the territory or dislodge other settlers.
Despite treaties banning local government and plantation agriculture, both activities flourished. In the late 18th century, an oligarchy of relatively wealthy settlers controlled the political economy of the British settlement. These settlers claimed about four-fifths of the available land; owned about half of all slaves; controlled imports, exports, and the wholesale and retail trades; and determined taxation. A group of magistrates, whom they elected from among themselves, had executive as well as judicial functions. The landowners resisted any challenge to their growing political power.
Slavery in the settlement, 1794-1838.
The earliest reference to African slaves in the British settlement appeared in a 1724 Spanish missionary's account, which stated that the British recently had been importing them from Jamaica, Bermuda, and other Central American British Colonies. A century later, the total slave population numbered about 2,300. Most slaves were born in Africa, and many slaves at first maintained African ethnic identifications and cultural practices. Gradually, however, slaves assimilated and a new, synthetic Kriol culture was formed.
Slavery in the settlement was associated with the extraction of timber, because treaties forbade the production of plantation crops. Settlers needed only one or two slaves to cut logwood, but as the trade shifted to mahogany in the last quarter of the 18th century, the settlers needed more money, land, and slaves for larger-scale operations. Other slaves worked as domestic helpers, sailors, blacksmiths, nurses, and bakers. The slaves' experience, though different from that on plantations in other colonies in the region, was nevertheless oppressive. They were frequently the objects of "extreme inhumanity," as a report published in 1820 stated. In the 18th century, many slaves escaped to Yucatán, and in the early 19th century a steady flow of runaways went to Guatemala and down the coast to Honduras.
One way the settler minority maintained its control was by dividing the slaves from the growing population of free Kriol people who were given limited privileges. Though some Kriols were legally free, their economic activities and voting rights were restricted. Privileges, however, led many free blacks to stress their loyalty and acculturation to British ways.
The act to abolish slavery throughout the British colonies, passed in 1833, was intended to avoid drastic social changes by effecting emancipation over a five-year transition period, by implementing a system of "apprenticeship" calculated to extend masters' control over the former slaves, and by compensating former slave owners for their loss of property. After 1838, the masters of the settlement continued to control the country for over a century by denying access to land and by limiting freedmen's economic freedom.
Emigration of the Garifuna.
At the same time that the settlement was grappling with the ramifications of the end of slavery, a new ethnic group—the Garifuna—appeared. In the early 19th century, the Garifuna, descendants of Carib peoples of the Lesser Antilles and of Africans who had escaped from slavery, arrived in the settlement. The Garifuna had resisted British and French colonialism in the Lesser Antilles until they were defeated by the British in 1796. After putting down a violent Garifuna rebellion on Saint Vincent, the British moved between 1,700 and 5,000 of the Garifuna across the Caribbean to the Bay Islands (present-day Islas de la Bahía) off the north coast of Honduras. From there they migrated to the Caribbean coasts of Nicaragua, Honduras, Guatemala, and the southern part of present-day Belize. By 1802 about 150 Garifuna had settled in the Stann Creek (present-day Dangriga) area and were engaged in fishing and farming.
Other Garifuna later came to the British settlement of Belize after finding themselves on the wrong side in a civil war in Honduras in 1832. Many Garifuna men soon found wage work alongside slaves as mahogany cutters. In 1841 Dangriga, the Garifuna's largest settlement, was a flourishing village. The American traveler John Stephens described the Garifuna village of Punta Gorda as having 500 inhabitants and producing a wide variety of fruits and vegetables.
The British treated Garifuna as squatters. In 1857 the British told the Garifuna that they must obtain leases from the crown or risk losing their lands, dwellings, and other buildings. The 1872 Crown Lands Ordinance established reservations for the Garifuna as well as the Maya. The British prevented both groups from owning land and treated them as a source of valuable labor.
Constitutional developments, 1850-62.
In the 1850s, the power struggle between the superintendent and the planters coincided with events in international diplomacy to produce major constitutional changes. In the Clayton-Bulwer Treaty of 1850, Britain and the United States agreed to promote the construction of a canal across Central America and to refrain from colonizing any part of Central America. The British government interpreted the colonization clause as applying only to any future occupation. But the United States government claimed that Britain was obliged to evacuate the area, particularly after 1853, when President Franklin Pierce's expansionist administration stressed the Monroe Doctrine. Britain yielded on the Bay Islands and the Mosquito Coast in eastern Nicaragua. But in 1854, Britain produced a formal constitution establishing a legislative for its possession of the settlement in present-day Belize.
The Legislative Assembly of 1854 was to have eighteen elected members, each of whom was to have at least £400 sterling worth of property. The assembly was also to have three official members appointed by the superintendent. The fact that voters had to have property yielding an income of £7 a year or a salary of a £100 a year reinforced the restrictive nature of this legislature. The superintendent could defer or dissolve the assembly at any time, originate legislation, and give or withhold consent to bills. This situation suggested that the legislature was more a chamber of debate than a place where decisions were made. The Colonial Office in London became, therefore, the real political-administrative power in the settlement. This shift in power was reinforced when in 1862, the Settlement of Belize in the Bay of Honduras was declared a British colony called British Honduras, and the crown's representative was elevated to a lieutenant governor, subordinate to the governor of Jamaica.
Under the Clayton-Bulwer Treaty of 1850 between the U.S. and Britain, neither country was to undertake any control, colonization or occupation of any part of Central America, but it was unclear if it applied to Belize. In 1853, a new American government attempted to have Britain leave Belize. In 1856, the Dallas-Clarendon Treaty between the two governments recognized Belize territory as British. The Sarstoon River was recognized as the southern border with Guatemala. The Anglo-Guatemalan Treaty of 1859 was signed, setting the present-day western boundary and temporarily settling the question of Guatemala's claim on the territory. Only the northern border with Mexico was undefined.
British Honduras (1862-1981).
Maya immigration and conflict.
As the British consolidated their settlement and pushed deeper into the interior in search of mahogany in the late 18th century, they encountered resistance from the Maya. In the second half of the 19th century, however, a combination of events outside and inside the colony redefined the position of the Maya. During the Caste War in Yucatán, a devastating struggle that halved the population of the area between 1847 and 1855, thousands of refugees fled to the British settlement. Though the Maya were not allowed to own land, most of the refugees were small farmers who were growing considerable quantities of crops by the mid-19th century. One group of Maya, led by Marcos Canul, attacked a mahogany camp on the Bravo River in 1866. A detachment of British troops sent to San Pedro was defeated by the Maya later that year. Early in 1867, British troops marched into areas in which the Maya had settled and destroyed villages in an attempt to drive them out. The Maya returned, however, and in April 1870, Canul and his men occupied Corozal. An unsuccessful 1872 attack by the Maya on Orange Walk was the last serious attack on the British colony.
In the 1880s and 1890s, Mopan and Kekchí Maya fled from forced labor in Guatemala and settled in several villages in southern British Honduras. Under the policy of indirect rule, a system of elected alcaldes (mayors) linked these Maya to the colonial administration. However, the remoteness of their settlements resulted in the Mopan and Kekchí Maya becoming less assimilated into the colony than the Maya of the north, where a Mestizo culture emerged. By the end of the 19th century, the ethnic pattern that remained largely intact throughout the 20th century was in place: Protestants largely of African descent, who spoke either English or Creole and lived in Belize Town; the Roman Catholic Maya and Mestizos who spoke Spanish and lived chiefly in the north and west; and the Roman Catholic Garifuna who spoke English, Spanish, or Garifuna and settled on the southern coast.
Formal establishment of the colony, 1862-71.
Largely as a result of the costly military expeditions against the Maya, the expenses of administering the new colony of British Honduras increased, at a time when the economy was severely depressed. Great landowners and merchants dominated the Legislative Assembly, which controlled the colony's revenues and expenditures. Some of the landowners were also involved in commerce but their interest differed from the other merchants of Belize Town. The former group resisted the taxation of land and favored an increase in import duties; the latter preferred the opposite. Moreover, the merchants in the town felt relatively secure from Maya attacks and were unwilling to contribute toward the protection of mahogany camps, whereas the landowners felt that they should not be required to pay taxes on lands given inadequate protection. These conflicting interests produced a stalemate in the Legislative Assembly, which failed to authorize the raising of sufficient revenue. Unable to agree among themselves, the members of the Legislative Assembly surrendered their political privileges and asked for establishment of direct British rule in return for the greater security of crown colony status. The new constitution was inaugurated in April 1871 and the new legislature became the Legislative Council.
Under the new constitution of 1871, the lieutenant governor and the Legislative Council, consisting of five ex officio or "official" and four appointed or "unofficial" members, governed British Honduras. This constitutional change confirmed and completed a change in the locus and form of power in the colony's political economy that had been evolving during the preceding half century. The change moved power from the old settler oligarchy to the boardrooms of British companies and to the Colonial Office in London.
The colonial order, 1871-1931.
The forestry industry's control of land and its influence in colonial decision-making slowed the development of agriculture and the diversification of the economy. Though British Honduras had vast areas of sparsely populated, unused land, landownership was controlled by a small European monopoly, thwarting the evolution of a Creole landowning class from the former slaves.
Landownership became even more consolidated during the economic depression of the mid-19th century. Major results of this depression included the decline of the old settler class, the increasing consolidation of capital, and the intensification of British landownership. The British Honduras Company (later the Belize Estate and Produce Company) emerged as the predominant landowner, with about half of all the privately held land in the colony. The new company was the chief force in British Honduras's political economy for over a century.
This concentration and centralization of capital meant that the direction of the colony's economy was henceforth determined largely in London. It also signaled the eclipse of the old settler elite. By about 1890, most commerce in British Honduras was in the hands of a clique of Scottish and German merchants, most of them newcomers. The European minority exercised great influence in the colony's politics, partly because it was guaranteed representation on the wholly appointed Legislative Council. In 1892, the governor appointed several Creole members, but whites remained the majority.
Despite the prevailing stagnation of the colony's economy and society during most of the century prior to the 1930s, seeds of change were being sown. The mahogany trade remained depressed, and efforts to develop plantation agriculture failed. A brief revival in the forestry industry took place early in the 20th century as new demands for forest products came from the United States. Exports of chicle, a gum taken from the sapodilla tree and used to make chewing gum, propped up the economy from the 1880s. A short-lived boom in the mahogany trade occurred around 1900 in response to growing demand for the wood in the United States, but the ruthless exploitation of the forests without any conservation or reforestation depleted resources.
Creoles, who were well-connected with businesses in the United States, challenged the traditional political-economic connection with Britain as trade with the United States intensified. In 1927, Creole merchants and professionals replaced the representatives of British landowners (except for the manager of the Belize Estate and Produce Company) on the Legislative Council. The participation of this Creole elite in the political process was evidence of emerging social changes that were largely concealed by economic stagnation.
An agreement between Mexico and Britain in 1893 set the boundary along the Rio Hondo, though the treaty was not finalized until 1897.
Genesis of modern politics, 1931-54.
The Great Depression shattered the colony's economy, and unemployment increased rapidly. On top of this economic disaster, the worst hurricane in the country's recent history demolished Belize Town on September 10, 1931, killing more than 1,000 people. The British relief response was tardy and inadequate. The British government seized the opportunity to impose tighter control on the colony and endowed the governor with the power to enact laws in emergency situations. The Belize Estate and Produce Company survived the depression years because of its special connections in British Honduras and London.
Meanwhile, workers in mahogany camps were treated almost like slaves. The law governing labor contracts, the Masters and Servants Act of 1883, made it a criminal offense for a laborer to breach a contract. In 1931 the governor, Sir John Burdon, rejected proposals to legalize trade unions and to introduce a minimum wage and sickness insurance. The poor responded in 1934 with a series of demonstrations, strikes, petitions, and riots that marked the beginning of modern politics and the independence movement. Riots, strikes, and rebellions had occurred before, but the events of the 1930s were modern labor disturbances in the sense that they gave rise to organizations with articulate industrial and political goals. Antonio Soberanis Gómez and his colleagues of the Labourers and Unemployed Association (LUA) attacked the governor and his officials, the rich merchants, and the Belize Estate and Produce Company, couching their demands in broad moral and political terms that began to define and develop a new nationalistic and democratic political culture. 
The labor agitation's most immediate success was the creation of relief work by a governor who saw it as a way to avoid civil disturbances. The movement's greatest achievements, however, were the labor reforms passed between 1941 and 1943. Trade unions were legalized in 1941, and a 1943 law removed breach-of-labor-contract from the criminal code. The General Workers' Union (GWU), registered in 1943, quickly expanded into a nationwide organization and provided crucial support for the nationalist movement that took off with the formation of the People's United Party (PUP) in 1950. The 1930s were therefore the crucible of modern Belizean politics. It was a decade during which the old phenomena of exploitative labor conditions and authoritarian colonial and industrial relations began to give way to new labor and political processes and institutions. The same period saw an expansion in voter eligibility. In 1945 only 822 voters were registered in a population of over 63,000, but by 1954 British Honduras achieved suffrage for all literate adults.
In December 1949, the governor devalued the British Honduras dollar in defiance of the Legislative Council, an act that precipitated Belize's independence movement. The governor's action angered the nationalists because it reflected the limits of the legislature and revealed the extent of the colonial administration's power. The devaluation enraged labor because it protected the interests of the big transnationals while subjecting the working class to higher prices for goods. Devaluation thus united labor, nationalists, and the Creole middle classes in opposition to the colonial administration. On the night that the governor declared the devaluation, the People's Committee was formed and the nascent independence movement suddenly matured.
Between 1950 and 1954, the PUP, formed upon the dissolution of the People's Committee on September 29, 1950, consolidated its organization, established its popular base, and articulated its primary demands. By January 1950, the GWU and the People's Committee were holding joint public meetings and discussing issues such as devaluation, labor legislation, the proposed West Indies Federation, and constitutional reform. As political leaders took control of the union in the 1950s to use its strength, however, the union movement declined.
The PUP concentrated on agitating for constitutional reforms, including universal adult suffrage without a literacy test, an all- elected Legislative Council, an Executive Council chosen by the leader of the majority party in the legislature, the introduction of a ministerial system, and the abolition of the governor's reserve powers. In short, PUP pushed for representative and responsible government. The colonial administration, alarmed by the growing support for the PUP, retaliated by attacking two of the party's chief public platforms, the Belize City Council and the PUP. In 1952 he comfortably topped the polls in Belize City Council elections. Within just two years, despite persecution and division, the PUP had become a powerful political force, and George Price had clearly become the party's leader.
The colonial administration and the National Party, which consisted of loyalist members of the Legislative Council, portrayed the PUP as pro-Guatemalan and even communist. The leaders of the PUP, however, perceived British Honduras as belonging to neither Britain nor Guatemala. The governor and the National Party failed in their attempts to discredit the PUP on the issue of its contacts with Guatemala, which was then ruled by the democratic, reformist government of President Jacobo Arbenz. When voters went to the polls on April 28, 1954, in the first election under universal literate adult suffrage, the main issue was clearly colonialism—a vote for the PUP was a vote in favor of self-government. Almost 70 percent of the electorate voted. The PUP gained 66.3 percent of the vote and won eight of the nine elected seats in the new Legislative Assembly. Further constitutional reform was unequivocally on the agenda.
Decolonization and the border dispute with Guatemala.
British Honduras faced two obstacles to independence: British reluctance until the early 1960s to allow citizens to govern themselves, and Guatemala's complete intransigence over its long-standing claim to the entire territory (Guatemala had repeatedly threatened to use force to take over British Honduras). By 1961, Britain was willing to let the colony become independent. Negotiations between Britain and Guatemala began again in 1961, but the elected representatives of British Honduras had no voice in these talks. George Price refused an invitation to make British Honduras an "associated state" of Guatemala, reiterating his goal of leading the colony to independence. In 1963 Guatemala broke off talks and ended diplomatic relations with Britain. Talks between Guatemala and British Honduras started and stopped abruptly during the late 1960s and early 1970s. From 1964 Britain controlled only British Honduran defense, foreign affairs, internal security, and the terms and conditions of the public service, and in 1973 the colony's name was changed to Belize in anticipation of independence.
By 1975, the Belizean and British governments, frustrated at dealing with the military-dominated regimes in Guatemala, agreed on a new strategy that would take the case for self-determination to various international forums. The Belize government felt that by gaining international support, it could strengthen its position, weaken Guatemala's claims, and make it harder for Britain to make any concessions. Belize argued that Guatemala frustrated the country's legitimate aspirations to independence and that Guatemala was pushing an irrelevant claim and disguising its own colonial ambitions by trying to present the dispute as an effort to recover territory lost to a colonial power. Between 1975 and 1981, Belizean leaders stated their case for self-determination at a meeting of the heads of Commonwealth of Nations governments, the conference of ministers of the Nonaligned Movement, and at meetings of the United Nations (UN). Latin American governments initially supported Guatemala. Between 1975 and 1979, however, Belize won the support of Cuba, Mexico, Panama, and Nicaragua. Finally, in November 1980, with Guatemala completely isolated, the UN passed a resolution that demanded the independence of Belize.
A last attempt was made to reach an agreement with Guatemala prior to the independence of Belize. The Belizean representatives to the talks made no concessions, and a proposal, called the Heads of Agreement, was initialed on March 11, 1981. However, when ultraright political forces in Guatemala labeled the proponents as sellouts, the Guatemalan government refused to ratify the agreement and withdrew from the negotiations. Meanwhile, the opposition in Belize engaged in violent demonstrations against the Heads of Agreement. A state of emergency was declared. However, the opposition could offer no real alternatives. With the prospect of independence celebrations in the offing, the opposition's morale fell. Independence came to Belize on September 21, 1981 after the Belize Act 1981, without reaching an agreement with Guatemala.
Independent Belize.
With Price at the helm, the PUP won all elections until 1984. In that election, first national election after independence, the PUP was defeated by the United Democratic Party (UDP), and UDP leader Manuel Esquivel replaced Price as prime minister. Price returned to power after elections in 1989. Guatemala’s president formally recognized Belize’s independence in 1992. The following year the United Kingdom announced that it would end its military involvement in Belize. All British soldiers were withdrawn in 1994, apart from a small contingent of troops who remained to train Belizean troops.
The UDP regained power in the 1993 national election, and Esquivel became prime minister for a second time. Soon afterward Esquivel announced the suspension of a pact reached with Guatemala during Price’s tenure, claiming Price had made too many concessions in order to gain Guatemalan recognition. The pact would have resolved a 130 year old border dispute between the two countries. Border tensions continued into the early 21st century, although the two countries cooperated in other areas.
The PUP won a landslide victory in the 1998 national elections, and PUP leader Said Musa was sworn in as prime minister. In the 2003 elections the PUP maintained its majority, and Musa continued as prime minister. He pledged to improve conditions in the underdeveloped and largely inaccessible southern part of Belize. 
In 2005, Belize was the site of unrest caused by discontent with the People's United Party government, including tax increases in the national budget. On February 8, 2008, Dean Barrow of the UDP was sworn in as Belize's first black prime minister.
Throughout Belize's history, Guatemala has claimed ownership of all or part of the territory. This claim is occasionally reflected in maps showing Belize as Guatemala's twenty-third province. As of March 2007, the border dispute with Guatemala remains unresolved and quite contentious; at various times the issue has required mediation by the United Kingdom, Caribbean Community heads of Government, the Organisation of American States, and the United States. In December 2008, Belize and Guatemala signed an agreement to submit the territorial differences to the International Court of Justice, after referenda in both countries (which have not taken place as of December 2013). Notably, both Guatemala and Belize are participating in the confidence-building measures approved by the OAS, including the Guatemala-Belize Language Exchange Project.
Since independence, a British garrison has been retained in Belize at the request of the Belizean government.

</doc>
<doc id="19283817" url="http://en.wikipedia.org/wiki?curid=19283817" title="Kos Manor">
Kos Manor

The Kos Manor () is a 16th-century manor house located in the Murova neighborhood of the town of Jesenice, Slovenia, at the street address of "Cesta maršala Tita 64". It is one of four so-called "ironworks castles" built in the area during the 16th and early 17th centuries by owners of local iron-mining and -processing facilities, in what were then the clustered settlements of Plavž, Sava, Murova and Javornik, amalgamated into the town of Jesenice in 1929. The Bucelleni-Ruard Manor in Sava is another survivor of the original four; the Plavž and Javornik manors have been torn down.
The Kos manor was built in 1521 by Sigismund (Žiga) of Dietrichstein, a leaseholder of the Bucelleni family, owners of the Sava ironworks. It is located in what was then the heart of the Murova settlement, at the foot of the path leading to the Church of St. Leonard atop a small hill a few hundred metres away. 
The manor is mentioned in period documents as the ""old belopeš castle"," in reference to the ancestral home of its builders the Bucelleni family, the village of Bela Peč (""White Furnace"," ), between Rateče and Tarvisio in present-day Italy. It was also described in Valvasor's 1689 survey The Glory of the Duchy of Carniola. 
The manor obtained its current name in 1821, when its then-owner, the local merchant Frančišek Pavel Kos, enlarged and renovated it in neoclassical style. At some point thereafter it was acquired by the Ruard family, from whom it passed into the hands of the KID company in 1872, by then the sole operator of the local ironworks. Ten years later it was purchased by the Jesenice municipal government, initially for use as a public school, in which capacity it served from 1883-1915. In the interwar period the manor, by then commonly known as the "old school" ("stara šola") was converted first into apartments and city offices and later into a courthouse and prison; it saw use as the latter during World War II as well, when the occupying Wehrmacht used the building as a transfer prison.
Currently the manor is administered by the Upper Sava Museum, Jesenice, and serves various cultural and public functions:
See also.
Bela Peč Castle

</doc>
<doc id="19283823" url="http://en.wikipedia.org/wiki?curid=19283823" title="Dickstein Shapiro">
Dickstein Shapiro

Dickstein Shapiro LLP (formerly Dickstein, Shapiro, Morin & Oshinsky) is a large U.S. law firm and lobbying group based in Washington, D.C., with six offices across the United States. According to the National Law Journal's 2012 rankings, it is the 128th largest law firm in the United States. The firm also ranked 75th in profit per attorney on the 2012 AmLaw 200 survey.
Practice areas.
The firm is divided into seven practice groups, each of which handle various aspects of their respective specialties: Complex Dispute resolution, Corporate & Finance, Energy, Government Law & Strategy (includes lobbying, political law, regulatory law and government contracts), Insurance coverage, Intellectual property, and Litigation.
Pro Bono
Dickstein Shapiro has been recognized by the DC Bar for its leadership in pro bono representation. It is one of the law firms representing the detainees at the Guantanamo Bay detention camp.
History.
Dickstein Shapiro was founded by Sidney Dickstein and David I. Shapiro in New York City in 1953. By 1956, the firm moved its headquarters to Washington, DC. The firm quickly established its reputation by winning several high-profile cases, including "Silver v. New York Stock Exchange" before the United States Supreme Court.
Over the following decades, the firm grew organically and through lateral hiring. In 2001, Dickstein Shapiro merged with Roberts, Sheridan & Kotel, a New York boutique firm that had spun off from Cravath, Swaine & Moore and which was primarily focused on corporate finance and tax law.
In 2012, the firm entered into merger discussions with San Francisco-based international firm Pillsbury Winthrop, but those talks ended by early 2013.

</doc>
<doc id="19283833" url="http://en.wikipedia.org/wiki?curid=19283833" title="Fair Labor Standards Act">
Fair Labor Standards Act

The Fair Labor Standards Act of 1938 (abbreviated as FLSA; also referred to as the Wages and Hours Bill) is a federal statute of the United States. The FLSA introduced a maximum 44-hour seven-day workweek, established a national minimum wage, guaranteed "time-and-a-half" for overtime in certain jobs, and prohibited most employment of minors in "oppressive child labor", a term that is defined in the statute. It applies to employees engaged in interstate commerce or employed by an enterprise engaged in commerce or in the production of goods for commerce, unless the employer can claim an exemption from coverage.
The FLSA was originally drafted in 1932 by Senator Hugo Black, who was later appointed to the Supreme Court in 1937. However, Black's proposal to require employers to adopt a thirty-hour workweek met stiff resistance. In 1938 a revised version of Black's proposal was passed that adopted an eight-hour day and a forty-hour workweek and allowed workers to earn wage for an extra four hours of overtime as well. According to the act, workers must be paid minimum wage and overtime pay must be one-and-a-half times regular pay. Children under eighteen cannot do certain dangerous jobs, and children under the age of sixteen cannot work during school hours. There were 700,000 workers affected by the FLSA, and Roosevelt called it the most important piece of New Deal legislation passed since the Social Security Act of 1935.
Amendments.
In 1946 the United States Supreme Court ruled in "Anderson v. Mt. Clemens Pottery Co." that preliminary work activities, where controlled by the employer and performed entirely for the employer's benefit, are properly included as working time under the Fair Labor Standards Act. In response, Congress passed an amendment to FLSA narrowing the Supreme Court's decision. The 1947 Portal-to-Portal Act specified exactly what type of time was considered compensable work time. In general, as long as an employee is engaging in activities that benefit the employer, regardless of when they are performed, the employer has an obligation to pay the employee for his or her time. It also specified that travel to and from the workplace was a normal incident of employment and shouldn't be considered paid working time.
The full effect of the FLSA of 1938 was postponed by the wartime inflation of the 1940s, which lowered wage values to below the level specified in the Act. The October 26, 1949 Fair Labor Standards Amendment (ch. 736, , , ) included changes to overtime compensation, defined a "regular rate," redefined the term "produced," raised the minimum wage from 40 cents to 75 cents per hour and extended child labor coverage. It also included a few new exemptions for special worker classes.
In 1955 the FLSA was amended once again to increase minimum wage, this time to one dollar per hour.
The 1961 FLSA Amendment added another method of determining a type of coverage called enterprise coverage. Enterprise coverage applies only when the business is involved in interstate commerce and its gross annual business volume is a minimum of $500,000. All employees working for these “enterprises” are then covered by the FLSA so long as the individual firms of the "enterprise have a revenue greater than $500,000 per year". Under the original 1938 Act, a worker whose work is in the channels of interstate commerce is covered as an individual. "Interstate commerce" is interpreted so broadly that a majority of work is included, such as ordering, loading, or using supplies from out of state, accepting payments from customers based on credit cards issued by out-of-state banks, and so on.
The 1961 Amendment also specified that coverage is automatic for schools, hospitals, nursing homes, or other residential care facilities. Coverage is also automatic for all governmental entities at whatever level of government, no matter how big or small. Coverage does not apply to certain entities that are not organized for a business purpose, such as churches and charitable institutions. The minimum wage level was again increased—this time to $1.25 per hour. What could be considered a wage was specifically defined, and entitlement to sue for back wages was granted.
The Contract Work Hours Standards Act, though not a direct amendment or modification to the FLSA, became law in 1962. It replaced the confusing and often ambiguous series of “Eight Hour Laws” (which date back to 1892) with a single, comprehensive law to govern hours of work for laborers.
The Equal Pay Act of 1963 was passed to amend the FLSA and make it illegal to pay workers lower wages strictly on the basis on their sex. It is often summed up with the phrase “equal pay for equal work”. This was a major step towards closing the wage gap in women's pay. In the past, it had been generally accepted that women did not deserve to earn as much money as men because they were not heads of households. However, in many homes, women were in fact the sole breadwinner for various reasons, ranging from death or disability of a spouse to divorce or single parenthood. Regardless of roles in the family, the Equal Pay Act established a single standard to apply to both sexes. The Equal Pay Act allows for unequal pay for equal work only when wages are set pursuant to a seniority system, a merit system, a system which measures earnings by quantity or quality of production, or other factors outside of sex.
The 1966 FLSA Amendment expanded coverage to some farm workers and increased the minimum wage to $1.60 per hour in stages. This was in large part due to the efforts of labor leaders like Cesar Chavez who brought farm worker rights to national attention during this period. The 1966 FLSA amendment also gave state and local government employees coverage for the first time.
The Age Discrimination in Employment Act (ADEA) of 1967 prohibited employment discrimination against persons forty years of age or older. Some older workers were being denied health benefits based on their age and denied training opportunities prior to the passage of the ADEA. This act applies only to businesses employing more than twenty workers.
The 1974 FLSA Amendment expanded coverage to include other state and local government employees that were not previously covered. Domestic workers also became covered and the minimum wage was increased to $2.30 per hour in stages.
The 1977 FLSA Amendment increased the minimum wage in yearly increments through 1981 to $3.35 an hour. Changes were made involving tipped employees and the tip credit. Partial overtime exemption was repealed in stages for certain hotel, motel, and restaurant employees.
The Migrant and Seasonal Agricultural Worker Protection Act (MSPA), passed in 1983, was designed to provide migrant and seasonal farm workers with protections concerning pay, working conditions, and work-related conditions, to require farm labor contractors to register with the U.S. Department of Labor, and to assure necessary protections for farm workers, agricultural associations, and agricultural employers.
The amendment to the FLSA enacted in 1985 permitted state and local government employers to compensate their employees' overtime hours with paid time away from work (compensatory time or “comp time”) in lieu of overtime pay. It also included modifications to ensure that true volunteer activities were not impeded or discouraged.
The Department of Defense Authorization Act of 1986 repealed the eight-hour daily overtime requirements on all federal contracts.
The 1989 FLSA amendments increased the minimum wage to $4.25 per hour in stages. The distinction between retail and non-retail was eliminated. Construction and laundry or dry cleaning were no longer named as enterprises. Changes were again made to the tip credit system. A “training wage” was established at 85% of minimum wage for workers less than 20 years of age. This “training wage”, also referred to as a "youth minimum wage" or "subminimum wage", could be paid for up to 90 days under certain conditions.
The 1996 FLSA amendment increased the minimum wage to $5.15 an hour. However, the Small Business Job Protection Act of 1996 (PL 104-188), which provided the minimum-wage increase, also detached tipped employees from future minimum-wage increases. Prior to 1996, tipped employees received 50% of the prevailing minimum wage. The tipped employee minimum wage was frozen, under federal law at least, at $2.13 per hour(). State laws that grant higher hourly wages remain in force.
On August 23, 2004, controversial changes to the FLSA's overtime regulations went into effect, making substantial modifications to the definition of an "exempt" employee. Low-level working supervisors throughout American industries were reclassified as “executives” and lost overtime rights. These changes were sought by business interests, which claimed that the laws needed clarification and that few workers would be affected. The Bush administration called the new regulations "FairPay". But other organizations, such as the AFL-CIO, claimed the changes would make millions of additional workers ineligible to obtain relief under the FLSA for overtime pay. Attempts in Congress to overturn the new regulations were unsuccessful.
Conversely, some low-level employees (particularly administrative-support staff) that had previously been classified as exempt were now reclassified as non-exempt. Although such employees work in positions bearing titles previously used to determine exempt status (such as "executive assistant"), the 2004 amendment to the FLSA now requires that an exemption must be predicated upon actual job function and not job title. Those employees with job titles that previously allowed exemption but whose job descriptions did not include managerial functions were now reclassified from exempt to non-exempt.
On May 25, 2007, President Bush signed into law a supplemental appropriation bill (H.R. 2206) which contains the Fair Minimum Wage Act of 2007. This provision amended the FLSA to provide for the increase of the federal minimum wage by an incremental plan, culminating in a minimum wage of $7.25 per hour by July 24, 2009.
Section 4207 of the Patient Protection and Affordable Care Act (H.R.3590) amends Section 7 to add that employers shall provide break time for nursing mothers to express milk and that "a place, other than a bathroom, that is shielded from view and free from intrusion from coworkers and the public" should be available for employees to express milk.
Practical application.
The Fair Labor Standards Act applies to "employees who are engaged in interstate commerce or in the production of goods for commerce, or who are employed by an enterprise engaged in commerce or in the production of goods for commerce", unless the employer can claim an exemption from coverage. Generally, an employer who does at least $500,000 of business or gross sales in a year satisfies the commerce requirements of the FLSA, and therefore that employer's workers will be subject to the FLSA's protections if none of the other exemptions apply. Several exemptions exist that relieve an employer from having to meet the statutory minimum wage, overtime, and record-keeping requirements. The largest exceptions apply to the so-called "white collar" exemptions that are applicable to professional, administrative and executive employees. Exemptions are narrowly construed; an employer must prove that the employees fit "plainly and unmistakeably" within the exemption's terms.
The FLSA applies to "any individual employed by an employer" but not to independent contractors or volunteers because they are not considered "employees" under the FLSA. Still, an employer cannot simply exempt workers from the FLSA by calling them independent contractors, and many employers have illegally misclassified their workers as independent contractors. Some employers similarly mislabel employees as volunteers. Courts will look at the "economic reality" of the relationship between the putative employer and the worker to determine whether the worker is, in fact, an independent contractor. Courts use a similar test to determine whether a worker was concurrently employed by more than one person or entity; commonly referred to as "joint employers". For example, a farm worker may be considered jointly employed by a labor contractor (who is in charge of recruitment, transportation, payroll, and keeping track of hours) and a grower (who generally monitors the quality of the work performed, determines where to place workers, controls the volume of work available, has quality control requirements, and has the power to fire, discipline, or provide work instructions to workers).
Presuming an employee is not exempt from overtime, there are many instances in which overtime is not paid properly, including when an employee is not paid for travel time between job sites, activities before their shift starts or after it ends, and activities to prepare for work that are central to work activities. If an employee is entitled to overtime they must be paid one and a half times the employee's "regular rate of pay" for all hours worked over 40 in the same work week.
Employees who are employed in a ministerial role by a religiously affiliated employer are not entitled to overtime under the act.
World War II.
During World War II, the Army-Navy "E" Award for excellence in war production required maintaining the fair labor standards established under the Act.
Tip credits and tip pools.
Under the FLSA, an employer must pay each employee the minimum wage, unless the employee is "engaged in an occupation in which he or she customarily and regularly receives more than $30 a month in tips". If the employees wage does not equal minimum wage including tips the employer must make up the difference. However, the employee must be allowed to keep all of their tips, either individually or through a tip pool. Also, a tip pool may contain only "employees who customarily and regularly receive tips". "The phrase 'customarily and regularly' signifies a frequency which must be greater than occasional, but which may be less than constant."
While the nomenclature of a job title is not dispositive, the job of "busboy" is explicitly validated for tip-pool inclusion by an authoritative source. "A busboy performs an integral part of customer service without much direct interaction, but he does so in a manner visible to customers...Thus, for a service bartender to be validly included in a tip pool, she must meet this minimal threshold in a manner sufficient to incentivize customers to 'customarily and regularly' tip in recognition' of her services (though she need not receive the tips directly).

</doc>
<doc id="19283852" url="http://en.wikipedia.org/wiki?curid=19283852" title="1938">
1938

__NOTOC__
Year 1938 (MCMXXXVIII) was a common year starting on Saturday (link will display the full calendar) of the Gregorian calendar.

</doc>
<doc id="19283869" url="http://en.wikipedia.org/wiki?curid=19283869" title="F59">
F59

F59 may refer to :
and also :

</doc>
<doc id="19283873" url="http://en.wikipedia.org/wiki?curid=19283873" title="1950">
1950

__NOTOC__

</doc>
<doc id="19283877" url="http://en.wikipedia.org/wiki?curid=19283877" title="Fabian Society">
Fabian Society

The Fabian Society is a British socialist organisation whose purpose is to advance the principles of socialism via gradualist and reformist means. The society laid many of the foundations of the Labour Party and subsequently affected the policies of states emerging from the decolonisation of the British Empire, most notably India and Singapore.
Originally, the Fabian society was committed to the establishment of a socialist economy, alongside a commitment to British imperialism as a progressive and modernizing force. Today its viewpoints are more social democratic.
Today, the society functions primarily as a think tank and is one of 15 socialist societies affiliated with the Labour Party. Similar societies exist in Australia (the Australian Fabian Society), Canada (the Douglas-Coldwell Foundation and the now disbanded League for Social Reconstruction) and in New Zealand.
Organisational history.
Establishment.
The Fabian Society was founded on 4 January 1884 in London as an offshoot of a society founded a year earlier called The Fellowship of the New Life. Fellowship members included poets Edward Carpenter and John Davidson, sexologist Havelock Ellis and the future Fabian secretary Edward R. Pease. They wanted to transform society by setting an example of clean simplified living for others to follow, but when some members also wanted to become politically involved to aid society's transformation, it was decided that a separate society, the Fabian Society, also be set up. All members were free to attend both societies. The Fabian Society additionally advocated renewal of Western European Renaissance ideas and their promulgation throughout the rest of the world.
The Fellowship of the New Life was dissolved in 1899, but the Fabian Society grew to become the pre-eminent academic society in the United Kingdom in the Edwardian era, typified by the members of its vanguard Coefficients club. Public meetings of the Society were for many years held at Essex Hall, a popular location just off the Strand in central London.
The Fabian Society, which favoured gradual change rather than revolutionary change, was namedat the suggestion of Frank Podmorein honour of the Roman general Fabius Maximus (nicknamed "Cunctator", meaning "the Delayer"). His Fabian strategy advocated tactics of harassment and attrition rather than head-on battles against the Carthaginian army under the renowned general Hannibal.
An explanatory note appearing on the title page of the group's first pamphlet declared:
"For the right moment you must wait, as Fabius did most patiently, when warring against Hannibal, though many censured his delays; but when the time comes you must strike hard, as Fabius did, or your waiting will be in vain, and fruitless."
Organisational growth.
Immediately upon its inception, the Fabian Society began attracting many prominent contemporary figures drawn to its socialist cause, including George Bernard Shaw, H. G. Wells, Annie Besant, Graham Wallas, Charles Marson, Hubert Bland, Edith Nesbit, Sydney Olivier, Oliver Lodge, Leonard Woolf and Virginia Woolf, Ramsay MacDonald and Emmeline Pankhurst. Even Bertrand Russell briefly became a member, but resigned after he expressed his belief that the Society's principle of entente (in this case, between countries allying themselves against Germany) could lead to war.
At the core of the Fabian Society were Sidney and Beatrice Webb. Together, they wrote numerous studies of industrial Britain, including alternative co-operative economics that applied to ownership of capital as well as land.
Many Fabians participated in the formation of the Labour Party in 1900 and the group's constitution, written by Sidney Webb, borrowed heavily from the founding documents of the Fabian Society. At the Labour Party Foundation Conference in 1900, the Fabian Society claimed 861 members and sent one delegate.
The years 1903 to 1908 saw a growth in popular interest in the socialist idea in Great Britain and the Fabian Society grew accordingly, tripling its membership to nearly 2500 by the end of the period, half of whom were located in London. In 1912 a student section was organised called the University Socialist Federation (USF) and by the outbreak of World War I this contingent counted its own membership of more than 500.
Early Fabian views.
The first Fabian Society pamphlets advocating tenets of social justice coincided with the zeitgeist of Liberal reforms during the early 1900s. The Fabian proposals however were considerably more progressive than those that were enacted in the Liberal reform legislation. The Fabians lobbied for the introduction of a minimum wage in 1906, for the creation of a universal health care system in 1911 and for the abolition of hereditary peerages in 1917.
Fabian socialists were in favour of reforming Britain's imperialist foreign policy as a conduit for internationalist reform, and were in favor of a capitalist welfare state modelled on the Bismarckian German model; they criticised Gladstonian liberalism both for its individualism at home and its internationalism abroad. They favoured a national minimum wage in order to stop British industries compensating for their inefficiency by lowering wages instead of investing in capital equipment; slum clearances and a health service in order for "the breeding of even a moderately Imperial race" which would be more productive and better militarily than the "stunted, anaemic, demoralised denizens...of our great cities"; and a national education system because "it is in the classrooms...that the future battles of the Empire for commercial prosperity are already being lost".
In 1900 the Society produced "Fabianism and the Empire", the first statement of its views on foreign affairs, drafted by Bernard Shaw and incorporating the suggestions of 150 Fabian members. It was directed against the liberal individualism of those such as John Morley and Sir William Harcourt. It claimed that the classical liberal political economy was outdated, and that imperialism was the new stage of the international polity. The question was whether Britain would be the centre of a world empire or whether it would lose its colonies and end up as just two islands in the North Atlantic. It expressed support for Britain in the Boer War because small nations, such as the Boers, were anachronisms in the age of empires. In order to hold onto the Empire, the British needed to fully exploit the trade opportunities secured by war; maintain the British armed forces in a high state of readiness to defend the Empire; the creation of a citizen army to replace the professional army; the Factory Acts would be amended to extend to 21 the age for half-time employment, so that the thirty hours gained would be used in "a combination of physical exercises, technical education, education in civil citizenship...and field training in the use of modern weapons".
The Fabians also favoured the nationalisation of land rent, believing that rents collected by landowners were unearned, an idea which drew heavily from the work of American economist Henry George.
Second generation.
In the period between the two World Wars, the "Second Generation" Fabians, including the writers R. H. Tawney, G. D. H. Cole and Harold Laski, continued to be a major influence on social-democratic thought.
It was at this time that many of the future leaders of the Third World were exposed to Fabian thought, most notably India's Jawaharlal Nehru, who subsequently framed economic policy for India on Fabian socialism lines. After independence from Britain, Nehru’s Fabian ideas committed India to an economy in which the state owned, operated and controlled means of production, in particular key heavy industrial sectors such as steel, telecommunications, transportation, electricity generation, mining and real estate development. Private activity, property rights and entrepreneurship were discouraged or regulated through permits, nationalization of economic activity and high taxes were encouraged, rationing, control of individual choices and Mahalanobis model considered by Nehru as a means to implement the Fabian Society version of socialism. In addition to Nehru, several pre-independence leaders in colonial India such as Annie Besant - Nehru's mentor and later a president of Indian National Congress - were members of the Fabian Society.
Obafemi Awolowo, who later became the premier of Nigeria's now defunct Western Region, was also a Fabian member in the late 1940s. It was the Fabian ideology that Awolowo used to run the Western Region during his premiership with great success, although he was prevented from using it in a similar fashion on the national level in Nigeria. It is less known that the founder of Pakistan, Muhammad Ali Jinnah, was an avid member of the Fabian Society in the early 1930s. Lee Kuan Yew, the first Prime Minister of Singapore, stated in his memoirs that his initial political philosophy was strongly influenced by the Fabian Society. However, he later altered his views, considering the Fabian ideal of socialism as impractical. In 1993, Lee said:
In the Middle East, the theories of Fabian Society intellectual movement of early-20th-century Britain inspired the Ba'athist vision. The Middle East adaptation of Fabian socialism led the state to control big industry, transport, banks, internal and external trade. The state would direct the course of economic development, with the ultimate aim to provide a guaranteed minimum standard of living for all. Michel Aflaq, widely considered as the founder of the Ba'athist movement, was a Fabian socialist. Aflaq's ideas, with those of Salah al-Din al-Bitar and Zaki al-Arsuzi, came to fruition in the Arab world in the form of dictatorial regimes in Iraq and Syria. Salāmah Mūsā of Egypt, another prominent champion of Arab Socialism, was a keen adherent of Fabian Society, and a member since 1909.
Among many current and former Fabian academics are the late political scientist Bernard Crick, the late economists Thomas Balogh and Nicholas Kaldor and the sociologist Peter Townsend.
Contemporary Fabianism.
Through the course of the 20th century the group has always been influential in Labour Party circles, with members including Ramsay MacDonald, Clement Attlee, Anthony Crosland, Richard Crossman, Tony Benn, Harold Wilson and more recently Shirley Williams,Tony Blair, Gordon Brown, Gordon Marsden and Ed Balls. The late Ben Pimlott served as its Chairman in the 1990s. (A Pimlott Prize for Political Writing was organised in his memory by the Fabian Society and "The Guardian" in 2005 and continues annually). The Society is affiliated to the Party as a socialist society. In recent years the Young Fabian group, founded in 1960, has become an important networking and discussion organisation for younger (under 31) Labour Party activists and played a role in the 1994 election of Tony Blair as Labour Leader. Today there is also an active Fabian Women's Network and Scottish and Welsh Fabian groups.
On 21 April 2009 the Society's website stated that it had 6,286 members: "Fabian national membership now stands at a 35 year high: it is over 20% higher than when the Labour Party came to office in May 1997. It is now double what it was when Clement Attlee left office in 1951."
The latest edition of the Dictionary of National Biography (a reference work listing details of famous or significant Britons throughout history) includes 174 Fabians. Four Fabians, Beatrice and Sidney Webb, Graham Wallas and George Bernard Shaw founded the London School of Economics with the money left to the Fabian Society by Henry Hutchinson. Supposedly the decision was made at a breakfast party on 4 August 1894. The founders are depicted in the Fabian Window designed by George Bernard Shaw. The window was stolen in 1978 and reappeared at Sotheby's in 2005. It was restored to display in the Shaw Library at the London School of Economics in 2006 at a ceremony over which Tony Blair presided.
Young Fabians.
Members aged under 31 years of age are also members of the Young Fabians. This group has its own elected Chair and executive and organises conferences and events. It also publishes the quarterly magazine "Anticipations". The Scottish Young Fabians, a Scottish branch of the group, reformed in 2005.
Influence on Labour government.
With the advent of a Labour Party government in 1997, the Fabian Society has been a forum for New Labour ideas and for critical approaches from across the party. The most significant Fabian contribution to Labour's policy agenda in government was Ed Balls' 1992 pamphlet, advocating Bank of England independence. Balls had been a "Financial Times" journalist when he wrote this Fabian pamphlet, before going to work for Gordon Brown. BBC Business Editor Robert Peston, in his book "Brown's Britain", calls this an "essential tract" and concludes that Balls "deserves as much credit – probably more – than anyone else for the creation of the modern Bank of England"; William Keegan offers a similar analysis of Balls' Fabian pamphlet in his book on Labour's economic policy, which traces in detail the path leading up to this dramatic policy change after Labour's first week in office.
The Fabian Society Tax Commission of 2000 was widely credited with influencing the Labour government's policy and political strategy for its one significant public tax increase: the National Insurance rise to raise £8 billion for National Health Service spending. (The Fabian Commission had in fact called for a directly hypothecated "NHS tax" to cover the full cost of NHS spending, arguing that linking taxation more directly to spending was essential to make tax rise publicly acceptable. The 2001 National Insurance rise was not formally hypothecated, but the government committed itself to using the additional funds for health spending.) Several other recommendations, including a new top rate of income tax, were to the left of government policy and not accepted, though this comprehensive review of UK taxation was influential in economic policy and political circles.
Criticism.
In the early 1900s Fabian Society members advocated the ideal of a scientifically planned society and supported eugenics by way of sterilization. This is said to have influenced the passage of the Half-Caste Act, and its subsequent implementation in Australia, where children were systematically and forcibly removed from their parents, so that the British colonial regime could "protect" the Aborigine children from their parents. In an article published in The Guardian on 14 February 2008 (following the apology offered by Australian Prime Minister Kevin Rudd to the "stolen generations"), Geoffrey Robertson criticised Fabian socialists for providing the intellectual justification for the eugenics policy that led to the stolen generations scandal. Such views on socialism, inequality and eugenics in early 20th century Fabians were not limited to one individual, but were widely shared in the Fabian Society and throughout a broad political spectrum.

</doc>
<doc id="19283891" url="http://en.wikipedia.org/wiki?curid=19283891" title="Executive compensation">
Executive compensation

Executive compensation or executive pay is composed of the financial compensation and other non-financial awards received by an executive of a firm. It is typically a mixture of salary, bonuses, shares of or call options on the company stock, benefits, and perquisites, ideally configured to take into account government regulations, tax law, the desires of the organization and the executive, and rewards for performance. 
The three decades starting with the 1980s, saw a dramatic rise in executive pay relative to that of an average worker's wage in the United States, and to a lesser extent in a number of other countries. Observers differ as to whether this rise is a natural and beneficial result of competition for scarce business talent that can add greatly to stockholder value in large companies, or a socially harmful phenomenon brought about by social and political changes that have given executives greater control over their own pay. Executive pay is an important part of corporate governance, and is often determined by a company's board of directors.
Types.
There are six basic tools of compensation or remuneration:
In a modern corporation, the CEO and other top executives are often paid salary plus short-term incentives or bonuses. This combination is referred to as Total Cash Compensation (TCC). Short-term incentives usually are formula-driven and have some performance criteria attached depending on the role of the executive. For example, the Sales Director's performance related bonus may be based on incremental revenue growth turnover; a CEO's could be based on incremental profitability and revenue growth. Bonuses are after-the-fact (not formula driven) and often discretionary. Executives may also be compensated with a mixture of cash and shares of the company which are almost always subject to vesting restrictions (a long-term incentive). To be considered a long-term incentive the measurement period must be in excess of one year (3–5 years is common). The vesting term refers to the period of time before the recipient has the right to transfer shares and realize value. Vesting can be based on time, performance or both. For example a CEO might get 1 million in cash, and 1 million in company shares (and share buy options used). Vesting can occur in two ways: "cliff vesting" (vesting occurring on one date), and "graded vesting" (which occurs over a period of time) and which maybe "uniform" (e.g., 20% of the options vest each year for 5 years) or "non-uniform" (e.g., 20%, 30% and 50% of the options vest each year for the next three years).
Other components of an executive compensation package may include such perks as generous retirement plans, health insurance, a chauffeured limousine, an executive jet, and interest-free loans for the purchase of housing.
Stock options.
Executive stock option pay rose dramatically in the United States after scholarly support from University of Chicago educated Professors Michael C. Jensen and Kevin J. Murphy. Due to their publications in the Harvard Business Review 1990 and support from Wall Street and institutional investors, Congress passed a law making it cost effective to pay executives in equity.
Supporters of stock options say they align the interests of CEOs to those of shareholders, since options are valuable only if the stock price remains above the option's strike price. Stock options are now counted as a corporate expense (non-cash), which impacts a company's income statement and makes the distribution of options more transparent to shareholders. Critics of stock options charge that they are granted without justification as there is little reason to align the interests of CEOs with those of shareholders. Empirical evidence shows since the wide use of stock options, executive pay relative to workers has dramatically risen. Moreover, executive stock options contributed to the accounting manipulation scandals of the late 1990s and abuses such as the options backdating of such grants. Finally, researchers have shown that relationships between executive stock options and stock buybacks, implying that executives use corporate resources to inflate stock prices before they exercise their options.
Stock options also incentivize executives to engage in risk-seeking behavior. This is because the value of a call option increases with increased volatility (see options pricing). Stock options also present a potential up-side gain (if the stock price goes up) for the executive, but no downside risk (if the stock price goes down, the option simply isn't exercised). Stock options therefore can incentivize excessive risk seeking behavior that can lead to catastrophic corporate failure.
Restricted stock.
Executives are also compensated with restricted stock, which is stock given to an executive that cannot be sold until certain conditions are met and has the same value as the market price of the stock at the time of grant. As the size of stock option grants have been reduced, the number of companies granting restricted stock either with stock options or instead of, has increased. Restricted stock has its detractors, too, as it has value even when the stock price falls. As an alternative to straight time vested restricted stock, companies have been adding performance type features to their grants. These grants, which could be called performance shares, do not vest or are not granted until these conditions are met. These performance conditions could be earnings per share or internal financial targets.
Levels.
The levels of compensation in all countries has been rising dramatically over the past decades. Not only is it rising in absolute terms, but also in relative terms. In 2007, the world's highest paid chief executive officers and chief financial officers were American. They made 400 times more than average workers—a gap 20 times bigger than it was in 1965. In 2010 the highest paid CEO was Viacom's Philippe P. Dauman at $84.5 million The U.S. has the world's highest CEO's compensation relative to manufacturing production workers. According to one 2005 estimate the U.S. ratio of CEO's to production worker pay is 39:1 compared to 31.8:1 in UK; 25.9:1 in Italy; 24.9:1 in New Zealand.
Controversy.
The explosion in executive pay has become controversial, criticized by not only leftists but conservative establishmentarians such as Ben Bernanke Peter Drucker, John Bogle, Warren Buffett. 
The idea that stock options and other alleged pay-for-performance are driven by economics has also been questioned. According to economist Paul Krugman, 
"Today the idea that huge paychecks are part of a beneficial system in which executives are given an incentive to perform well has become something of a sick joke. A 2001 article in "Fortune", "The Great CEO Pay Heist" encapsulated the cynicism: You might have expected it to go like this: The stock isn't moving, so the CEO shouldn't be rewarded. But it was actually the opposite: The stock isn't moving, so we've got to find some other basis for rewarding the CEO.` And the article quoted a somewhat repentant Michael Jensen theorist for stock option compensation: `I've generally worried these guys weren't getting paid enough. But now even I'm troubled.'" 
Defenders of high executive pay say that the global war for talent and the rise of private equity firms can explain much of the increase in executive pay. For example, while in conservative Japan a senior executive has few alternatives to his current employer, in the United States it is acceptable and even admirable for a senior executive to jump to a competitor, to a private equity firm, or to a private equity portfolio company. Portfolio company executives take a pay cut but are routinely granted stock options for ownership of ten percent of the portfolio company, contingent on a successful tenure. Rather than signaling a conspiracy, defenders argue, the increase in executive pay is a mere byproduct of supply and demand for executive talent. However, U.S. executives make substantially more than their European and Asian counterparts.
United States.
The U.S. Securities and Exchange Commission (SEC) has asked publicly traded companies to disclose more information explaining how their executives' compensation amounts are determined. The SEC has also posted compensation amounts on its website to make it easier for investors to compare compensation amounts paid by different companies. It is interesting to juxtapose SEC regulations related to executive compensation with Congressional efforts to address such compensation.
Since the 1990s, CEO compensation in the US has outpaced corporate profits, economic growth and the average compensation of all workers. Between 1980 and 2004, Mutual Fund founder John Bogle estimates total CEO compensation grew 8.5%/year compared, compared to corporate profit growth of 2.9%/year and per capita income growth of 3.1%. By 2006 CEOs made 400 times more than average workers—a gap 20 times bigger than it was in 1965. As a general rule, the larger the corporation the larger the CEO compensation package.
The share of corporate income devoted to compensating the five highest paid executives of (each) public firms more than doubled from 4.8% in 1993-1995 to 10.3% in 2001-2003.
The pay for the five top-earning executives at each of the largest 1500 American companies for the ten years from 1994 to 2004 is estimated at approximately $500 billion in 2005 dollars. 
As of late March 2012 USA Today's tally showed the median CEO pay of the S&P 500 for 2011 was $9.6 million.
Lower level executives also have fared well. About 40% of the top 0.1% income earners in the United States are executives, managers, or supervisors (and this doesn't include the finance industry) — far out of proportion to less than 5% of the working population that management occupations make up.
A study by University of Florida researchers found that highly paid CEOs improve company profitability as opposed to executives making less for similar jobs. However, a review of the experimental and quasi-experimental research relevant to executive compensation, by Philippe Jacquart and J. Scott Armstrong, found opposing results. In particular, the authors conclude that "the notion that higher pay leads to the selection of better executives is undermined by the prevalence of poor recruiting methods. Moreover, higher pay fails to promote better performance. Instead, it undermines the intrinsic motivation of executives, inhibits their learning, leads them to ignore other stakeholders, and discourages them from considering the long-term effects of their decisions on stakeholders" 
Another study by Professors Lynne M. Andersson and Thomas S. Batemann published in the "Journal of Organizational Behavior" found that highly paid executives are more likely to behave cynically and therefore show tendencies of unethical performance.
Australia.
In Australia, shareholders can vote against the pay rises of board members, but the vote is non-binding. Instead the shareholders can sack some or all of the board members.
Canada.
A 2012 report by the Canadian Centre for Policy Alternatives complained that the top 100 Canadian CEOs were paid an average of C$8.4 million in 2010, a 27% increase over 2009, this compared to C$44,366 earned by the average Canadian that year, 1.1% more than in 2009. The top three earners were automotive supplier Magna International Inc. founder Frank Stronach at C$61.8 million, co-CEO Donald Walker at C$16.7 million and former co-CEO Siegfried Wolf at C$16.5 million.
Europe.
In 2008, Jean-Claude Juncker, president of the European Commission's “Eurogroup” of finance ministers, called excessive pay a “social scourge” and demanded action.
United Kingdom.
Although executive compensation in the UK is said to be "dwarfed" by that of corporate America, it has caused public upset. In response to criticism of high levels of executive pay, the Compass organisation set up the High Pay Commission. Its 2011 report described the pay of executives as "corrosive". 
In December 2011/January 2012 two of the country’s biggest investors, Fidelity Worldwide Investment, and the Association of British Insurers, called for greater shareholder control over executive pay packages. Dominic Rossi of Fidelity Worldwide Investment stated, “Inappropriate levels of executive reward have destroyed public trust and led to a situation where all directors are perceived to be overpaid. The simple truth is that remuneration schemes have become too complex and, in some cases, too generous and out of line with the interests of investors.” Two sources of public anger were Barclays, where senior executives were promised million-pound pay packages despite a 30% drop in share price; and Royal Bank of Scotland where the head of investment banking was set to earn a "large sum" after thousands of employees were made redundant.
Regulation.
There are a number of strategies that could be employed as a response to the growth of executive compensation.

</doc>
<doc id="19283898" url="http://en.wikipedia.org/wiki?curid=19283898" title="Security guard">
Security guard

A security officer (or security guard) is a person who is paid to protect property, assets, or people. Security guards are usually privately and formally employed civilian personnel. Security officers are generally uniformed and act to protect property by maintaining a high visibility presence to deter illegal and inappropriate actions, observing (either directly, through patrols, or by watching alarm systems or video cameras) for signs of crime, fire or disorder; then taking action and reporting any incidents to their client and emergency services as appropriate.
Until the 1980s, the term watchman was more commonly applied to this function, a usage dating back to at least the Middle Ages in Europe. This term was carried over to North America where it was interchangeable with night-watchman until both terms were replaced with the modern security-based titles. Security guards are sometimes regarded as fulfilling a private policing function.
Functions and duties.
Many security firms and proprietary security departments practice the "detect, deter, observe and report" methodology. Security officers are not required to make arrests, but have the authority to make a citizen's arrest, or otherwise act as an agent of law enforcement, for example, at the request of a police officer or sheriff.
A private security officer's primary duty is the prevention and deterrence of crime. Security personnel enforce company rules and can act to protect lives and property, and they often have a contractual obligation to provide these actions. In addition to basic deterrence, security officers are often trained to perform specialized tasks such as arrest and control (including handcuffing and restraints), operate emergency equipment, perform first aid, CPR, take accurate notes, write detailed reports, and perform other tasks as required by the client they are serving.
All security officers are also required to go through additional training mandated by the state for the carrying of weapons such as batons, firearms, and pepper spray (e.g. the Bureau of Security and Investigative Services in California has requirements that a license for "each" item listed must be carried while on duty). Some officers are required to complete police certification for special duties. Virginia training standards for security are identical to police training with regards to firearms (shotgun and handgun) but do not place licensing requirements for other items carried, only that training be provided that is documented. Several security companies have also become certified in RADAR and trained their sworn special police officers to use it on protected properties in conjunction with lights/sirens, allowing them to legally enforce traffic laws on private property.
The number of jobs is expected to grow in the U.S., with 175,000 new security jobs expected before 2016. In recent years, due to elevated threats of terrorism, most security officers are required to have bomb-threat training and/or emergency crisis training, especially those located in soft target areas such as shopping malls, schools, and any other area where the general public congregate.
One major economic justification for security personnel is that insurance companies (particularly fire insurance carriers) will give substantial rate discounts to sites which have a 24-hour presence. For a high risk or high value property, the discount can often exceed the money being spent on its security program. Discounts are offered because having security on site increases the odds that any fire will be noticed and reported to the local fire department before a total loss occurs. Also, the presence of security personnel (particularly in combination with effective security procedures) tends to diminish "shrinkage", theft, employee misconduct and safety rule violations, property damage, or even sabotage. Many casinos hire security guards to protect money when transferring it from the casino to the casino's bank.
Security personnel may also perform access control at building entrances and vehicle gates; meaning, they ensure that employees and visitors display proper passes or identification before entering the facility. Security officers are often called upon to respond to minor emergencies (lost persons, lockouts, dead vehicle batteries, etc.) and to assist in serious emergencies by guiding emergency responders to the scene of the incident, helping to redirect foot traffic to safe locations, and by documenting what happened on an incident report.
Armed security officers are frequently contracted to respond as law enforcement until a given situation at a client location is under control and/or public authorities arrive on the scene.
Patrolling is usually a large part of a security officer's duties. Often these patrols are logged by use of a guard tour patrol system, which require regular patrols. Until recently the most commonly used form used to be mechanical clock systems that required a key for manual punching of a number to a strip of paper inside with the time pre-printed on it. But recently, electronic systems have risen in popularity due to their light weight, ease of use, and downloadable logging capabilities. Regular patrols are, however, becoming less accepted as an industry standard, as it provides predictability for the would-be criminal, as well as monotony for the security officer on duty. Random patrols are easily programmed into electronic systems, allowing greater freedom of movement and unpredictability. Global positioning systems are beginning to be used because they are a more effective means of tracking officers' movements and behavior.
Personnel.
Although security officers differ greatly from police officers, military personnel, federal agents/officers, and the like, Australia and the United States have a growing proportion of security personnel that have former police or military experience, including senior management personnel. On the other hand, some security officers, young people in particular, use the job as practical experience to use in applying to law enforcement agencies.
Types of security personnel and companies.
Security personnel are classified as either of the following:
Industry terms for security personnel include: security guard, security officer, security agent, safety patrol, private police, company police, security enforcement officer, and public safety. Terms for specialized jobs include bouncer, bodyguards, executive protection agent, loss prevention, alarm responder, hospital security officer, mall security officer, crime prevention officer, patrolman, private patrol officer, and private patrol operator.
State and local governments sometimes regulate the use of these terms by law—for example, certain words and phrases that "give an impression that he or she is connected in any way with the federal government, a state government, or any political subdivision of a state government" are forbidden for use by California security licensees by Business and Professions Code Section 7582.26. So the terms "private homicide police" or "special agent" would be unlawful for a security licensee to use in California. Similarly, in Canada, various acts specifically prohibits private security personnel from using the terms "Probation Officer", "law enforcement", "police", or "police officer".
Alberta and Ontario prohibit the use of the term "Security Officer", which has been in widespread use in the United States for many decades. Recent changes to the act have also introduced restrictions on uniform and vehicle colours and markings to make private security personnel clearly distinctive from police personnel. Some sources feel that some of these restrictions are put in place to satisfy the Canadian Police Association.
There is a marked difference between persons performing the duties historically associated with watchmen and persons who take a more active role in protecting persons and property. The former, often called "guards", are taught the mantra "observe and report", are minimally trained, and not expected to deal with the public or confront criminals.
The latter are often highly trained, sometimes armed depending on contracts agreed upon with clientele, and are more likely to interact with the general public and to confront the criminal element. These employees tend to take pride in the title "Security Officer" or "Protection Officer" and disdain the label of "guard".
Security jobs vary in pay and duties. There is sometimes little relationship between duties performed and compensation, for example some mall "security officers" who are exposed to serious risks earn less per hour than "industrial security guards" who have less training and responsibility. However, there are now more positions in the security role that separate not just the titles, but the job itself. The roles have progressed and so have the areas for which security people are needed.
The term "agent" can be confusing in the security industry because it can describe a civil legal relationship between an employee and their employer or contractor ("agent of the owner" in California PC 602), and also can describe a person in government service ("Special Agent Jones of the Federal Bureau of Investigation".) The title "agent" can be confused with bail enforcement agents, also known as "bounty hunters", who are sometimes regulated by the same agencies which regulate private security. The term "agent" is also used in other industries, such as banking agents, loan agents and real estate agents.
Security agents are often employed in loss prevention and personal or executive protection (bodyguards) roles. They typically work in plainclothes (without a uniform), and are usually highly trained to act lawfully in direct defense of life or property.
Security personnel are essentially private citizens, and therefore are bound by the same laws and regulations as the citizenry they are contracted to serve, and therefore are not allowed to represent themselves as law enforcement under penalty of law.
Training.
Just as with the police profession, training requirements for the private security industry have evolved over time. For many years security guards were poorly chosen and poorly trained (if at all), partly because security guard companies who contracted with clients in private industry were paid very little for their security guard services. For the most part, contracts were awarded to security guard companies through a competition process and the final selection was often made based on cost rather than the experience or professionalism of the security guard company. That changed drastically on September 11, 2001 when radical Islamic terrorists attacked the United States. The event moved corporate threat concerns to the top of the priority list for most security guard contracts started being awarded based on professionalism. More money was invested in security so more money became available for training of security guards. The term 'security professional' began to surface and large private security companies like Blackwater, USA began offering training services for the private security industry that approached the level of training provided by the military. Security guard companies began paying enough to attract people with significant backgrounds in law enforcement and the military, often in special operations.
Australia.
Any person who conducts a business or is employed in a security-related field within Australia is required to be licensed. Each of the six states and two territories of Australia have separate legislation that covers all security activities. Licensing management in each state/territory is varied and is carried out by either Police, Attorney General's Department, Justice Department or the Department of Consumer Affairs.
All of this legislation was intended to enhance the integrity of the private security industry.
All persons licensed to perform security activities are required to undertake a course of professional development in associated streams that are recognised nationally. This has not always been the case and the introduction of this requirement is expected to regulate the educational standards and knowledge base so that the particular job can be competently performed.
Strict requirements are laid down as to the type of uniform and badge used by security companies. Uniforms or badges that may be confused with a police officer are prohibited. Also, the use of the titles 'Security Police' or 'Private Detective' are unacceptable. While the term security guard is used by companies, government bodies and individuals, the term security officer is deemed more suitable. Bouncers use the title Crowd Controllers, and Store Detectives use the title Loss Prevention or Asset Protection Officers.
Security Officers may carry firearms, handcuffs or batons where their role requires them to do so and then only when working and have the appropriate sub-class accreditation to their license.
Canada.
In Canada, private security falls under the jurisdiction of Canada's ten provinces and three territories. All ten of Canada's provinces and one of its territories (the Yukon) have legislation that regulates the contract security industry. These eleven jurisdictions require that companies that provide security guard services and their employees be licensed.
Most provinces in Canada regulate the use of handcuffs and weapons (such as firearms and batons) by contract security companies and their employees, either banning such use completely or permitting it only under certain circumstances. Additionally, in some provinces, some terms, or variations of them, are prohibited either on a uniform or in self-reference.
Canada's federal laws also restrict the ability of security guards to be armed. For example, section 17 of the Firearms Act makes it an offense for any person, including a security guard, to possess prohibited or restricted firearms (i.e. handguns) anywhere outside of his or her home.
There are two exceptions to this prohibition found in sections 18 and 19 of the Act. Section 18 deals with transportation of firearms while Section 19 deals with allowing persons to carry such firearms on their persons to protect their lives or the lives of other persons, or for the performance of their occupation (Armour Car Guards, Licensed Trappers), provided an Authorization to Carry (ATC) is first obtained.
British Columbia.
Private security in the province of British Columbia is governed by two pieces of legislation: the Security Services Act and the "Security Services Regulation". These laws are administered and enforced by the Security Programs and Police Technology Division of the Ministry of Public Safety and Solicitor General.
The legislation requires that guards must be at least 19 years old, undergo a criminal background check, and successfully complete a training course. As far as weapons, British Columbia law severely restricts their use by security guards. Section 11(1)(c) of the Security Services Regulation prohibits security personnel from carrying or using any "item designed for debilitating or controlling a person or animal", which the government interprets to include all weapons. As well, section 11 forbids private security from using or carrying restraints, such as handcuffs, unless authorized by the government. However, as in other parts of Canada, armoured car guards are permitted to carry firearms.
In the past, only personnel that worked for contract security, that is, security companies, were regulated in British Columbia. However, as of September 1, 2009, in-house security guards and private investigators came under the jurisdiction of the Security Services Act and Security Services Regulation. Bodyguards and bouncers, effective November 1, 2009, are also subject to these regulations.
Europe.
Armed private security are much rarer in Europe, and illegal in many countries, such as the United Kingdom, the Netherlands and Switzerland. In developing countries (with host country permission), an armed security force composed mostly of ex-military personnel is often used to protect corporate assets, particularly in war-torn regions.
As a requirement of the Private Security Industry Act 2001, the UK now requires all contract security guards to have a valid Security Industry Authority license. The licence must be displayed when on duty, although a dispensation may be granted for store detectives, bodyguards and others who need to operate without being identified as a security guard. This dispensation is not available to Vehicle Immobilisers. Licenses are valid for three years and require the holders to undergo formal training, and are also to pass mandatory Criminal Records Bureau checks. Licences for Vehicle Immobilisers are valid for one year. Armed guarding and guarding with a weapon are illegal.
In Finland, all contract security guards are required to have a valid license granted by police. Temporary license is valid for four months and normal license for five years. License requires a minimum 40-hour course for temporary license and 60 hours more for a normal license. Additionally a narrow security vetting is required. The 40-hour course allows the carrying of a fixed-length baton and handcuffs, separate training and license is required for the security guard to carry pepper spray, extendable baton or a firearm. Rehearse of weapons usage is mandatory every year and is regulated by the Ministry of The Interior, to ensure the safe handling of pepper spray and such. In Finland, a security guard has the right to detain a person "red-handed", or seen committing a crime and the right to search the detained individual for harmful items and weapons. An individual who has been forcefully detained can only be released by the police. All companies providing security guarding services are also required to have a valid license from Ministry of the Interior.
In The Netherlands, security guards Beveiligingsbeambte must undergo a criminal background check by the local police department in the area where the private security company is located. To become a security guard in The Netherlands, a person must complete the basic training level 2 Beveiliger2. To complete the training a trainee must undergo a three-month internship with a private security company that is licensed by the svpb, the board that controls security exams. A trainee guard must pass for his diploma within one year. If the trainee does not pass he is not allowed to work anymore until he completes his training with a positive result. After a positive result a new ID can be issued and is valid for three years, after which the guard must undergo a background check by the local police again. Security guards in The Netherlands are not allowed to carry any kind of weapon or handcuffs. Every uniformed security guard in The Netherlands must have the V symbol on his or her uniform to advise the public they are dealing with a private guard; this rule is mandated by the Ministry of Justice. Security uniforms may not look like similar to police uniforms, and may not contain any kind of rank designation. The colors yellow and gold are not allowed to be used because the Dutch police uses gold accents in their uniforms; also, wearing a uniform cap is not longer allowed. Every new uniform design or addition must be approved by the Ministry of Justice before use. A patrol vehicle may not look like a police striped vehicle. The only private security guards who are allowed to carry firearms are those who work for the military or Dutch National bank (De Nederlandsche Bank); this is where the national gold reserve can be found.
Norway.
In Norway security officers are called "Vektere". There are two different types of vekterethe normal uniformed or civil-clothing officers who watch over private and semi-public properties, and government-hired vektere who work in public places, such as the Parliament. The law provides more enforcement powers to security officers in the Parliament than to private security officers.
Security officers must undergo three weeks of training and internship. They are allowed to work for six months after one week of the introduction course. It is also possible to choose Security as a high school major, which requires two years of school and two years of trainee positions at private companies, resulting in a certificate from the government. This certificate makes it easier to get a job, with slightly higher pay. It also makes it easier to get a job elsewhere in the security industry. The certificate can also be obtained by private security officers who have had a minimum of 5 years working experience.
In addition to normal "vektere" there also is a special branch for "Ordensvakter" who normally work as bouncers or security at concerts and similar types of events. Ordensvakter have to undergo an extra week of training to learn techniques on how to handle drunk people and people on various drugs. They also learn about the alcohol laws of Norway (which are rather strict). The police in the local police district must approve each Ordensvakt. These special regulations arose after events in the 1990s when bouncers had a bad reputation, especially in Oslo, for being too brutal and rough with people. At that time, the police had no control over who worked as bouncers. After the government implemented training and mandatory ID cards for bouncers the problems have been reduced. The police of Oslo report that Ordensvakter are now helping the police identify crimes that otherwise would not be reported.
In 2007 several guards from the Securitas AB company were arrested for brutality against a robber they apprehended on the main street of Oslo. The crime was captured with a mobile camera by pedestrians and created a public outcry, with many objecting to the way the security guards took the law into their own hands. Later, it came to light that the thief first attacked the security guards when they approached him, so the brutality charges were dropped. As a result of this episode, the police said that they would be more careful when conducting criminal background checks for security guards. Before 2007 security guards were checked when they applied for a job, but not while they were working. Security companies were also criticized for not checking criminal records sufficiently, in some cases not at all. Now guards working in private security must be checked annually. The police have the authority to withdraw a company's licence if the company does not submit lists of employees to the police. The police in Norway were widely criticized for not checking guards properly, and even when they encounter an issue with a guard, the guard can still work for months before anything is done. The security company G4S, after being criticized by police for hiring criminals, stated that they cannot do anything about the problem, because only the police have the ability to check the guard's criminal records.
In 2012 Norwegian media reported that police officers and Home Guard soldiers had contracts of employment on civilian ships, and leaders of police were planning sanctions against the use of police officers. 
Today there are around 15,000 people working within private security in Norway. The police have around 10,000 employees in total.
Notable companies operating in Norway:
Hong Kong.
In Hong Kong, the term "Security Officer" refers to a senior staff member who supervises a team of security personnel. The staff who work under security officers' supervision are called
"Security Guards".
Legislation.
Before 1 October 1996, private security personnel were regulated by the "Watchmen Ordinance" (Chapter 299). However, there were many problems with that system of regulation—for example, there were no restrictions as to whom may establish private security service companies to provide security services to a client. Also, there was no regulation of people whom may perform installation of security systems. Some employers hired "caretakers" instead of security guards to avoid their responsibilities under the ordinance (in formal definition, "caretakers" are supposed to provide facilities management service, although security service, which provided to residential properties, takes some parts of facilities management service). As a result, the Hong Kong Government enacted a wholly new law, the "Security and Guarding Services Ordinance" (Chapter 460), to replace the "Watchmen Ordinance".
According to the "Security and Guarding Services Ordinance":
No individual shall do, agree to do, or hold himself/herself out as doing, or as available to do, security work for another person unless he/she does so-
"Security work" means any of the following activities-
"Security device" means a device designed or adapted to be
installed in any premises or place, except on or in a vehicle, for the purpose
of detecting or recording- (Amended 25 of 2000 s. 2)
Qualification.
Qualification for security guards vary from country to country. Different requirements have to be completed before applying for this job.
Hong Kong.
Any applicant who wishes to apply for a Security Personnel Permit (SPP) must:
Permit.
Security Personnel Permit was separated to four types: A, B, C, and D.
The permit is valid for five years. All holders must renew their permit before it expires, or they will lose their qualification to work, as such, until their permit is renewed.
The type A and Type B security service are gradually combined with property management service, though the boundary between these two industries is unclear.
Power of Arrest.
Security Guards in Hong Kong do not have special powers of arrest above that of the ordinary citizen, i.e. citizen's arrest, also known locally as the "101 arrest power". The Section 101 in the Criminal Procedure Ordinance addresses that arrest of an offender by a private citizen is allowed in certain circumstances if the offender is attempting an arrestable offense. Once arrested, the suspect must be delivered to a police office as soon as possible.
An arrestable offence is defined as any crime carrying a sentence of more than 12 months imprisonment. No security personnel are allowed to search other person, nor are they allowed to get personal information from other people, with the exception of some specific circumstances.
Israel.
In Israel, almost all security guards carry a firearm, primarily to prevent revenge attacks. Security guards are common: they perform entrance checks at shopping malls, transportation terminals, government and other office buildings, and many stores. Many locations with a high number of visitors, such as the Jerusalem Central Bus Station, employ X-ray machines to check passenger's bags; in other places, they are opened and visually inspected. Since 2009, private security guards companies as Mikud have also replaced official security forces at some checkpoints inside and on the border of the West Bank, as well as the crossings to Gaza.
Malaysia.
Peninsular Malaysia allows for the use of Nepalese security guards whereby East Malaysian immigration policy does not allow the use of foreign workers to be in employed in the security industry.
Security guard companies need to apply to the Ministry of Home Affairs (Kementerian Dalam Negeri).
South Africa.
Security guards along with the rest of the private security industry are regulated under Act 56 of 2001, Private Security Industry Regulation Act.
United States.
Private security guards have outnumbered police officers since the 1980s, predating the heightened concern about security brought on by the September 11, 2001, attacks. The more than 1 million contract security officers, and an equal number of guards estimated to work directly for U.S. corporations, is much greater than the nearly 700,000 sworn law enforcement officers in the United States.
Most states require a license to work as a security officer. This license may include a criminal background check or mandated training requirements.
Security guards have the same powers of arrest as a private citizen, called a "private person" arrest, "any person" arrest, or "citizen's arrest". Most security officers do not carry weapons. If weapons are carried, additional permits and training are usually required. Armed security personnel are generally employed to protect sensitive sites such as government and military installations, armored money transports, casinos, banks and other financial institutions, and nuclear power plants. However, armed security is quickly becoming a standard for vehicle patrol officers and on many other non-government sites.
In some states, companies are developing technology to enhance private security. Using behavior analysis, computers can detect threats more quickly with fewer errors in judgement. Using specific algorithms, a computer can now detect aggressive and defensive body language, which triggers an alert to security or proper authorities depending on the event. These systems can also track slips and falls, theft and other events commonly experienced in corporate America.
The responsibilities of security guards in the United States are expanding in scope. For example, a trend is the increasing use of private security to support services previously provided by police departments. James F. Pastor addresses substantive legal and public policy issues which directly or indirectly relate to the provision of security services. These can be demonstrated by the logic of alternative or supplemental service providers. The use of private police has particular appeal because property or business owners can directly contract for public safety services, thereby providing welcome relief for municipal budgets. Finally, private police functions can be flexible, depending upon the financial, organizational, political, and circumstances of the client.
Arizona—Licensed security companies are required to provide eight hours of pre-assignment training to all persons employed as security guards before the employee acts in the capacity of a security guard. There is a state-mandated curriculum that must be taught, and subjects covered must include criminal law and laws of arrest, uniforms and grooming, communications, use of force, general security procedures, crime scene preservation, ethics, and first response.
California—Security Guards are required to obtain a license from the Bureau of Security and Investigative Services (BSIS), of the California Department of Consumer Affairs. Applicants must be at least 18 years old, undergo a criminal history background check through the California Department of Justice (DOJ) and the Federal Bureau of Investigation (FBI), and complete a 40-hour course of required training. This required training is broken down into smaller training sections and time-lines. The first is 8 hours of BSIS-designed instruction on powers to arrest and weapons. Then, within 30 days of getting the individual officers license, they must receive 16 hours of training on various mandatory and elective courses. Finally, within 6 months of getting their license, they must receive an additional 16 hours of training on various mandatory and elective courses.
California security officers are also required to complete 8 hours of annual training on security-related topics, in addition to the initial 40 hours of training.
The training and exam may be administered by any private patrol operator or by any of a large number of certified training facilities. This training can be in the classroom or online.
New Jersey—As of 2006 all security personnel must undergo a state mandated certified training program. This law, commonly referred to as SORA, is the state's effort to increase the quality of security personnel.
New Mexico—As of 2008 all security guards must undergo FBI background checks and a certified training program. Guards who carry firearms must also undergo additional training with a firearm through an approved firearms instructor and pass a psychological exam. The security industry is regulated through the New Mexico Regulation and Licensing Division.
North Carolina—Security Officers in North Carolina are required to register and become certified with the Private Protective Services Board (PPSB), the private security authority body under the North Carolina Department of Justice. The purpose of the Private Protective Services Board is to administer the licensing, education and training requirements for persons, firms, associations and corporations engaged in private protective services within North Carolina. The board is totally fee funded and is staffed by departmental employees directed on a daily basis by the Director, who is appointed by the Attorney General. There are two classifications for an officer: armed and unarmed. While an unarmed officer is required to take a 16 hour class of training and instruction to become certified, an armed officer must take additional hours of classroom training as well as qualify on a gun range with the firearm which will be carried on duty.
Oklahoma—Security officers in Oklahoma are licensed by CLEET (Council on Law Enforcement Education and Training). To be licensed as an unarmed officer an individual must be at least 18 years of age and undergo 40 hours of classroom training and pass criminal history checks. Armed guards must be 21 years of age, have another 40 hours of classroom training, qualify with their firearm and pass a psychological evaluation.
Oregon—Department of Public Safety, Standards and Training
Pennsylvania—No licensing requirements to be an unarmed security guard. However, anyone who carried a firearm or other "lethal weapon" in the course and scope of their employment must be trained as a "Certified Agent" and successfully complete a 40 hour training course (including shooting range time) in order to be certified to carry weapons while on duty under the Lethal Weapons Training Act (commonly referred to as Act 235 certification). Certification involves completing a medical physical exam, a psychological examination, classroom training and qualifying on a pistol range, with firing of 50 rounds of ammo larger than a .380acp. Agents are also required to qualify on a shotgun. The certification is good for five years at which time an eight hour refresher course must be taken or the certification is revoked. PA State Police—Lethal Weapons Training Program
South Carolina—All Security Officers have the same authority and power of arrest as Sheriff's Deputies, while on the property they are paid to protect, and according to Attorney General Alan Wilson, are considered Law Enforcement for the purpose of making arrests and swearing out a warrant before the magistrate once an arrest has been made. Private Officers may respond to calls for service, make arrests and use blue lights and traffic radar. They may also be specially authorized by the State Law Enforcement Division (SLED) to issue Uniform Traffic Tickets to violators. Security Officers are licensed or registered (as appropriate) by SLED for one year at a time. Training for unarmed officers is 8 hours, an additional 8 hours is required for a security weapons permit or a concealed security weapons permit. Additional hours are required to be documented for officers issuing public or private tickets as well as officers who will be using batons, pepper spray or tasers.
Virginia—Since the 1980s, Security Officers in Virginia are required to be certified by DCJS (Department of Criminal Justice Services, the same agency that certifies law enforcement officers). To be certified as an unarmed security officer one must go through 18 hours of classroom training from a certified instructor in order to obtain this card and it must be done by the end of their 90 days after hire with a Security company. Every two years the card must be renewed, by completing an in-service with a certified instructor. To be certified as an armed security officer one must complete an additional 16 hours of firearms training, 6 hours of training in conducting a lawful arrest, and qualification with the type and caliber of weapon they intend to carry. Firearms endorsements must be renewed annually by completing an in-service and passing a firearms qualification. Certified armed security officers are authorized under state code to arrest for any offense committed in their presence while they are on duty at the location they are hired to protect. Unarmed officers have no arrest powers. They also are granted the authority by the by state law to issue summons to appear in court for felonies and misdemeanors. Virginia also allows security officers to attend additional 40 hours of training to become certified as Conservators of the Peace (Special Police) for the company employing them. This appointment is performed by a Circuit Court Judge, wherein the officer is actually sworn in and has the powers of a police officer on property they are working, as well as the lawful duty to act upon witnessing any felony and the ability to pursue fleeing felons. Such sworn officers are also permitted the use of sirens and red lights. Those who handle K-9s, work as dispatchers, alarm responders, private investigators, instructors, bounty hunters, armored car couriers and Executive Protection Specialists are other categories of training regulated by DCJS with additional training requirements. All positions require State Police and FBI background checks.
St. Louis, Missouri—Security officers are required to be licensed by the St. Louis County Police Department or St. Louis Police Department. St. Louis County security officer training is a two-day class and yearly renewal class. Armed officers must shoot bi-annually to keep their armed status. County license is called a Metropolitan License, meaning it is good for St. Louis City and County. The St. Louis City web site has all the information regarding licensing requirements, as they are the same in the city and county.
Security officers and the police.
Security personnel are not police officers, unless they are security police, but are often identified as such due to similar uniforms and behaviors, especially on private property. Security personnel in the U.S. derive their powers from state laws, which allow them a contractual arrangement with clients that give them Agent of the Owner powers.
This includes a nearly unlimited power to question with the absence of probable cause requirements that frequently dog public law enforcement officers.
Some jurisdictions do commission or deputize security officers and give them limited additional powers, particularly when employed in protecting public property such as mass transit stations. This is a special case that is often unique to a particular jurisdiction or locale. Additionally, security officers may also be called upon to act as an agent of law enforcement if a police officer, sheriff's deputy, etc. is in immediate need of help and has no available backup.
Some security officers do have reserve police powers and are typically employed directly by governmental agencies. Typically, these are sworn law enforcement personnel whose duties primarily involve the security of a government installation, and are also a special case.
Other local and state governments occasionally enter into special contracts with security agencies to provide patrol services in public areas. These personnel are sometimes referred to as "private police officers".
Sometimes, police officers work as security personnel while not on duty. This is usually done for extra income, and work is particularly done in hazardous jobs such as bodyguard work and bouncers outside nightclubs.
Police are called in when a situation warrants a higher degree of authority to act upon reported observations that security does not have the authority to act upon. However, some states allow Licensed Security Officers full arrest powers equal to those of a Sheriff's Deputy.
In 1976, the Law Enforcement Assistance Administration's National Advisory Commission on Criminal Justice Standards and Goals reported:
'One massive resource, filled with significant numbers of personnel, armed with a wide array of technology, and directed by professionals who have spent their entire adult lifetimes learning how to prevent and reduce crime, has not been tapped by governments in the fight against criminality. The private security industry, with over one million workers, sophisticated alarm systems and perimeter safeguards, armored trucks, sophisticated mini-computers, and thousands of highly skilled crime prevention experts, offers a potential for coping with crime that can not be equalled by any other remedy or approach... Underutilized by police, all but ignored by prosecutors and the judiciary, and unknown to corrections officials, the private security professional may be the only person in this society who has the knowledge to effectively prevent crime.'
In New York City, the Area Police/Private Security Liaison program was organized in 1986 by the NYPD commissioner and four former police chiefs working in the private security industry to promote mutual respect, cross-training, and sharing of crime-related information between public police and private security.
Trends.
Australia.
Private Security personnel initially outnumbered police. From the Australian Bureau of Statistics Report in 2006 there were 52,768 full-time security officers in the security industry compared to 44,898 police officers. But since Security Industry Regulation Act 2007 it has dropped to less than half that.
UK.
The trend in the UK at the time of writing (March 2008) is one of polarisation. The market in Manned Guarding (the security industry term for the security guards most people are familiar with) is diverging toward two opposite extremes; one typified by a highly trained and well paid security officer; the other with security officers on or about minimum wage with only the minimum training required by law.
Within the "in-house" sector, where security personnel are not subject to licensing under the Private Security Industry Act 2001, the same divergence can be seen, with some companies opting for in-house security to maintain control of their standards, while others use it as a route to cheaper, non-regulated, security.
In a very few cases, such as the Northern Ireland Security Guard Service, security guards may be attested as Special Constables.
United States.
Economist Robert B. Reich, in his 1991 book "The Work of Nations", stated that in the United States, the number of private security guards and officers was comparable to the number of publicly paid police officers. He used this phenomenon as an example of the general withdrawal of the affluent from existing communities where governments provide public services. Instead, the wealthy pay to provide their own premium services, through voluntary, exclusive associations.
As taxpayer resistance has limited government budgets, and as the demand for secure homes in gated communities has grown, these trends have continued in the 1990s and 2000s (decade).
In the aftermath of the September 11, 2001 attacks, the trend in the US is one of a quiet transformation of the role of security guards into first responders in case of a terrorist attack or major disaster. This has resulted in longer guard instruction hours, extra training in terrorism tactics and increased laws governing private security companies in some states.
History.
The "vigiles" were soldiers assigned to guard the city of Rome, often credited as the origin of both security personnel and police, although their principal duty was as a fire brigade. There have been night watchmen since at least the Middle Ages in Europe; walled cities of ancient times also had watchmen. A special chair appeared in Europe sometime in the late Middle Ages, called the watchman's chair; this unupholstered wooden chair had a forward slanting seat to prevent the watchman from dozing off during duty. 
Famous Grenadier Guard infantryman, Kipling I. Peel, was said to have influenced the formation of the phrase "keeping (one's) eyes peeled". Peel was well known throughout the community as a man never to have been thought to blink his eyes, according to legend associated with the British Victorian Era. Debate on the topic is widespread. Peel had on numerous occasions claimed that he "could not, with a clear conscience take credit for such a thing".
Unionization.
Canada.
Many security guards in Canada are unionized. The primary unions which represent security guards in Canada are the United Food and Commercial Workers (UFCW), Local 333, and the Canadian branch of the United Steelworkers (USW). In contrast to the legal restrictions in the United States, Canadian labour relations boards will certify bargaining units of security guards for a Canadian Labour Congress (CLC)-affiliated union or in the same union with other classifications of employees.
United States.
In June 1947, the United States Congress passed the Taft-Hartley Act placing many restrictions on labor unions. Section 9 (B) (3) of the act prevents the National Labor Relations Board (NLRB) from certifying for collective bargaining any unit which mixes security employees with non-security employees. This restricts the ability of security employees to join any union that also represents other types of employees.
They may be part of an independent, "security-only" union, not affiliated with any coalition of other types of labor unions such as the American Federation of Labor and Congress of Industrial Organizations (AFL-CIO). A union which also represents non-security employees may also represent and bargain on behalf of security employees with the employer's consent.
Two of the largest security unions are the Security, Police, and Fire Professionals of America (SPFPA) and the United Government Security Officers of America (UGSOA).
Security, Police, and Fire Professionals of America.
In 1948 with the Taft-Hartley restrictions well into effect, the Detroit, Michigan area security guards of United Auto Workers (UAW) Amalgamated Local 114 were forced to break away and start a separate "Plant Guards Organizing Committee". The NLRB ruled that as an affiliate of the CIO, the committee was indirectly affiliated with production unions and therefore ineligible for certification under the new restrictions.
The committee was then forced to completely withdraw from the CIO and start the independent United Plant Guard Workers of America. By the 1990s, this union had evolved to include many other types of security officers and changed its name to the SPFPA.
United Government Security Officers of America.
In 1992, the UGSOA was formed. It specializes in organizing federal, state, and local government security officers, but since May, 2000 has been open to representing other types of security personnel as well.
Others.
The Service Employees International Union (SEIU) has also sought to represent security employees, although its efforts have been complicated by the Taft-Harley Act because the SEIU also represents janitors, trash collectors, and other building service employees.
Hazards in the Industry.
Security personnel often are exposed to physical and physiological trauma that can have lasting effects. This has always been an issue, but the 21st century's more violent and angry culture combined with drug- and alcohol-related violence results in more instances of security personnel being physically or verbally abused.
Other contributing factors are high workload, long hours, low pay, boredom and disregard of industry standards by employers and clients: e.g. break times, access to bathrooms and facilities, etc.

</doc>
<doc id="19283907" url="http://en.wikipedia.org/wiki?curid=19283907" title="Yu Chao'en">
Yu Chao'en

Yu Chao'en (魚朝恩) (722 – April 10, 770), formally the Duke of Han (韓公), was a eunuch official of the Chinese dynasty Tang Dynasty. He was powerful early during the reign of Emperor Daizong and was feared by others, including chancellors. At the urging of the chancellor Yuan Zai, Emperor Daizong secretly executed him at a meeting in 770, although Emperor Daizong publicly claimed that he committed suicide.
Background.
Yu Chao'en was born in 722, during the reign of Emperor Xuanzong. His family was from Lu Prefecture (瀘州, in modern Luzhou, Sichuan). Late in Emperor Xuanzong's "Tianbao" (742–756) era, Yu was an eunuch attached to the examination bureau of government (門下省, "Menxia Sheng"). It was said that he was intelligent and was capable both in publicly announcing imperial edicts and in accounting.
During Emperor Suzong's reign.
Early in the "Zhide" (756–758) era of Emperor Xuanzong's son and successor Emperor Suzong, during which Emperor Suzong was occupied with trying to suppress the rebel state Yan, Yu Chao'en was often commissioned to serve as a monitor of the armies, including serving as monitor of the army of Li Guangjin (李光進) during the recapturing of the capital Chang'an from Yan forces in 757. For his contributions to the campaign, he was put in charge of the eunuch bureau (內侍省, "Neishi Sheng") and given a general title. Subsequently, after Tang forces recaptured the eastern capital Luoyang (which served as Yan's capital), forcing the Yan emperor An Qingxu to flee to Yecheng, nine Tang military governors ("Jiedushi") put Yecheng under siege. The two most prominent generals of the nine were Guo Ziyi and Li Guangbi (Li Guangjin's brother), and as Emperor Suzong did not want to force one to submit to the command of the other, he did not commission a supreme commander; rather, he made Yu the monitor of the armies. It was said that Yu was jealous of Guo and often submitted reports criticizing Guo, but that Guo defused the tension by being humble with Yu.
In 759, the Yan general Shi Siming, who had briefly submitted to Tang but then rose again against Tang, attacked Tang forces at Yecheng and, while not achieving a victory, caused the Tang forces to collapse by themselves. He subsequently killed An Qingxu and took over the Yan throne. Meanwhile, Yu blamed the collapse on Guo, and as a result, Li Guangbi was put in command of the armies. Shi Siming subsequently attacked Luoyang and captured it. After a failed attempt by Tang forces to capture Luoyang, instigated by Yu and opposed by Li Guangbi, Shi tried to attack west toward Chang'an, but was repelled by the general Wei Boyu (衛伯玉), who was under Yu's command, at Shan Prefecture (陝州, in modern Sanmenxia, Henan). After a joint Tang and Huige army recaptured Luoyang in 762, Yu stationed his elite Shence Army to Bian Prefecture (汴州, in modern Kaifeng, Henan). For his contributions in this battle, he was created the Duke of Fengyi. Later in 762, he moved back to Shan Prefecture.
During Emperor Daizong's reign.
Also in 762, Emperor Suzong died and was succeeded by his son Emperor Daizong. In 763, when Tufan launched a sudden attack against Chang'an, Emperor Daizong was forced to flee to Shan Prefecture. When he fled, very few imperial guard soldiers accompanied him, and it was not until Yu Chao'en met him at Huayin (華陰, in modern Weinan, Shaanxi) that he was protected by an army. Emperor Daizong gave Yu the title of monitor of troops over the entire realm (天下觀軍容宣慰處置使, "Tianxia Guanjunrong Xuanwei Chuzhishi"). After Emperor Xuanzong's return to Chang'an later in the year, Yu continued to be in command of the Shence Army and was greatly favored by Emperor Daizong, receiving much wealth. He was also permitted to enter and leave the palace as he wished. As the generals under his command continued to achieve important victories, particularly in the subsequent conflict against the rebellious general Pugu Huai'en, he considered himself capable in military command. As he considered himself learned in the Confucian classics as well and was capable of writing. In 765, during an attack by Pugu's forces, aligned with Huige and Tufan, Yu tried to use his soldiers to coerce the imperial officials into concurring with moving the capital to Hezhong (河中, in modern Yuncheng, Shanxi), but when an official named Liu publicly denounced the plan even with Yu's soldiers surrounding him, Yu abandoned the plan.
Also in 765, Yu, because he believed himself capable in literary matters, was made the acting principal of the imperial university (國子監, "Guozijian"). He was also created the Duke of Zheng. Under him, the imperial university, which had been destroyed during the Anshi Rebellion, was rebuilt. In 766, when the university's construction was completed, Yu personally lectured about the "I Ching", tried to satirize the chancellors by talking about how a "ding" (a large cooking vessel often used to symbolize chancellorship) would overturn if imbalanced. The chancellor Wang Jin, was visibly incensed, but the more powerful Yuan Zai remained calm and pleasant, leading Yu to comment, "It is common for the target to get angry, but one who remains smiling needs to be paid attention to even more carefully." Yuan, however, was secretly resentful. Yu continued to be the principal of the university until 768, despite opposition by the official Chang Gun that a eunuch should not head the university.
In 767, Yu donated his mansion outside Chang'an to be rebuilt into a Buddhist temple dedicated to Emperor Daizong's deceased mother Consort Wu. As she was posthumously honored Empress Zhangjing, the temple was named Zhangjing Temple. The temple was said to be so luxuriously built that the wood in Chang'an was not enough, and several imperial pavilions had to be torn down so that the wood could be reused, and many officials and generals were required to donate their own houses for wood. In 768, he was created the Duke of Han. That year, at the anniversary of Consort Wu's death, Yu held a feast in her honor — at which he openly talked about how the chancellors were incompetent and should yield their seats. The chancellors did not dare to respond, but the junior officials Xiangli Zao (相里造) and Li Kan (李衎) responded and rebuked Yu, causing him to be displeased and to adjourn the feast early. Late in the year, Guo Ziyi's father's tomb was opened by grave robbers, but it was commonly believed that, because Yu disliked Guo immensely, that he was responsible for instigating it, and thus, when Guo subsequently arrived in the capital, there was anticipation that Guo would react violently. Guo defused the tension by stating that his soldiers have themselves robbed many graves, and that this must have been divine retribution. In 769, when Emperor Daizong had Yu escort Guo on a tour of Zhangjing Temple, Yuan tried to exploit the tension between the two by having Guo's subordinates falsely warning Guo that Yu was set to kill him during the tour. Guo refused to take precautions and told Yu about the rumors, defusing the tension between the two.
Meanwhile, several things caused Emperor Daizong to begin to be pleased with Yu. Yu was beginning to expect Emperor Daizong to accept every suggestion of his, and on one occasion, when Emperor Daizong did not, Yu stated, "Is there anything in this realm that I cannot decide?" Yu's young adoptive son Yu Linghui (魚令徽) was then serving as a eunuch inside the palace, and he wore the green robe for sixth and seventh rank officials. On an occasion, he had an argument with his colleagues, and he told Yu Chao'en about the argument. Yu Chao'en met Emperor Daizong the next day and stated, "My son's rank is too low, and his colleagues look down on him. Please let him wear a purple robe." Even before Emperor Daizong could respond, the officials nearby, following Yu Chao'en's cue, already brought out a purple robe and put it on Yu Linghui. Yu Linghui bowed to thank Emperor Daizong, who smiled and responded, "This child now has a purple robe. He should be happy." However, he was internally displeased about how the incident went. Yuan saw that Emperor Daizong was becoming displeased with Yu, and therefore suggested to Emperor Daizong to eliminate Yu. They began to plot together. Yuan began to bribe two close associates of Yu's; Zhou Hao (周皓) the commander of the imperial guard archery corps, and Huangfu Wen (皇甫溫) the military governor of Shan Circuit (headquartered in modern Sanmenxia). Zhou and Huangfu became associates of Yuan's, and from this point on, Yuan and Emperor Daizong were able to anticipate Yu's moves.
In spring 770, at Yuan's suggestion, Emperor Daizong carried out several moves that were intending to be preludes to eliminating Yu — moving the general Li Baoyu from being the military governor ("Jiedushi") of Fengxiang Circuit (鳳翔, headquartered in modern Baoji) to Shannan West Circuit (山南西道, headquartered in modern Xi'an, Shaanxi, to the southwest of Chang'an), while moving Huangfu, then the military governor of Shan Circuit (headquartered in modern Sanmenxia) to Fengxiang — while allaying Yu's suspicions by transferring control of four counties near Chang'an to the imperial guards, under Yu's command. (Yuan's intent was that, as Huangfu arrived in Chang'an, to use his soldiers against Yu.) Soon, when Huangfu arrived in Chang'an, Yuan laid a trap for Yu with Huangfu's and Zhou's soldiers, and at a secret meeting between Emperor Daizong and Yu, Yuan and Emperor Daizong acted and killed Yu. Emperor Daizong then issued a public rebuke of Yu and then claimed that, when Yu received the rebuke, he committed suicide. Emperor Daizong still had him buried with honors, at imperial expense.

</doc>
